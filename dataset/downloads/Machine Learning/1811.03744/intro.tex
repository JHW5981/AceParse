% !TEX root =  main.tex

\section{Introduction}

In multidimensional density estimation, an algorithm has access to independent draws from an unknown 
target probability distribution over $\mathbb{R}^d$, which is typically assumed to belong to or be close to some class of
``nice'' distributions.  The goal is to output a hypothesis distribution which with high probability is
close to the target distribution.  A number of different distance measures can be used to capture the notion of
closeness; in this work we use the total variation distance (also known as the ``statistical distance'' 
% this is the same as $L_1$ distance up to a factor of two
and equivalent to the $L_1$ distance).  This is a well studied framework which has been investigated in detail, see e.g. the books~\citep{DG85,devroye2012combinatorial}.

Multidimensional density estimation 
is typically attacked in one of
two ways.  In the first general approach a parameterized hypothesis class is chosen, and a setting of parameters
is chosen based on the observed data points.  This approach is justified given the belief that the parameterized class contains
a good approximation to the distribution generating the
data, or even that the parameterized class actually contains the target distribution.  See \citep{Dasgupta:99,KMV:10,MoitraValiant:10} for some well-known multidimensional distribution learning results in this line.  

In the second general approach a hypothesis distribution is constructed by ``smoothing'' the empirical distribution with a kernel function.  This approach is justified by the belief that the target distribution satisfies some smoothness assumptions, and is more appropriate when studying distributions that do not have a parametric representation.  The current paper falls within this second strand.

The most popular smoothness assumption is that the distribution has
a density that belongs to a Sobolev space 
\citep{sobolev1963theorem,barron1991minimum,holmstrom1992asymptotic,devroye2012combinatorial}.
The simplest Sobolev space used in this context corresponds to having a bound on the average
of the partial first ``weak derivatives'' of the density; other Sobolev spaces 
correspond to bounding additional derivatives.  A drawback of this approach
is that it does not apply to distributions whose densities have jump
discontinuities. Such jump discontinuities can arise in various applications, for example, when
objects under analysis must satisfy hard constraints.

\ignore{\rnote{Do we want to discuss Besov spaces at all?}  }
To address this, some authors have used the weaker assumption that the
density belongs to a Besov space
\citep{besov1959family,devore1993besov,masry1997multivariate,willett2007multiscale,acharya2017sample}.
In the simplest case, this allows jump discontinuities as long as the
function does not change very fast on average.  The precise definition,
which is quite technical (see \cite{devore1993besov}),
makes reference to the effect on a distribution of shifting the domain
by a small amount.

\medskip
\noindent {\bf The densities we consider.}
In this paper we analyze 
% an alternative, simpler, smoothness
a clean and simple smoothness
assumption, which is a continuous analog of the notion of
shift-invariance that has recently been used for analyzing the
learnability of various types of discrete distributions
\citep{barbour1999poisson,daskalakis2013learning,DLS18asums}.
The assumption is based on the {\em shift-invariance of $f$ in direction $v$ at scale $\kappa$},
which, for a density $f$ over $\mathbb{R}^d$, a unit vector $v \in \mathbb{R}^d$, and a positive real value $\kappa$, we 
% essentially 
\ignore{\pnote{The discussion about full-rank covariance matrices seems to 
no longer be needed.}}
define to be
% \footnote{This is actually a slight simplification of the true definition, which aligns with the true definition when the covariance matrix of $f$ has full rank. See Section~\ref{sec:prelims} for the precise definition. For simplicity, in the rest of this introduction the density $f$ is assumed to have a full-rank covariance matrix, and the subsequent definitions of $\si(f,\kappa)$, etc. reflect this assumption; again, see Section~\ref{sec:prelims} for  precise definitions.}
\[
\si(f,v,\kappa) \eqdef 
{\frac 1 \kappa} \cdot \sup_{\kappa' \in [0,\kappa]} \int_{\R^d} \left|f(x+ \kappa'  v) - f(x)\right| dx.\]
 We define the quantity $\si(f,\kappa)$ to be the worst case
of $\si(f,v,\kappa)$ over all directions $v$, i.e.\ 
\[
\si(f,\kappa) \eqdef \sup_{v: \Vert v \Vert_2 =1} \si(f,v,\kappa). 
\]
For any  
constant $c$,
we define the class of densities $\CSI(c, d)$  to consist of all $d$-dimensional densities $f$ with the property that $\si(f,\kappa) \le c$ for all $\kappa > 0.$ 

Our notion of shift-invariance provides a quantitative way of capturing the intuition that the density $f$ 
% is ``Lipschitz on average in every direction''.  
changes gradually on average in every direction.
Several natural classes fit nicely into this framework; for example, we note that $d$-dimensional standard normal distributions are easily shown to belong to $\CSI(1, d)$.  As another example, we will show later that any $d$-dimensional isotropic log-concave distribution belongs to $\CSI(O_d(1),d)$.





%For\rnote{We are not interested in the $c=o(1)$ regime, right? Assuming $c \geq 1$ is nice since it lets us simplify some later expressions} $c \geq 1,\kappa > 0$, we define the class of densities $\CSI(c,d,\kappa)$ over $\R^d$ to consist of those densities $f$ with the following property:
%letting $f_v$ denote the univariate distribution $f_v$ obtained by
%projecting $f$ onto direction $v$, it holds that
%\[
%\sup_{\|v\|=1} \si\left(f,v,\frac{\kappa}{\sqrt{\Var[f_v]}} \right) \leq c\sqrt{\Var[f_v]} \quad \text{for every direction $v$.}
%\]
%(The various factors of $\sqrt{\Var[f_v]}$ are to make the definition scale-invariant: with this definition, rescaling the axes does not affect
%membership in $\CSI(c,d,\kappa)$.  See Lemma~\ref{l:rescale}.)
%\ignore{\rnote{Add some intuition/interpretation of this class.  We had previously said ``(The multiplication by $\sqrt{\Var[f_v]}$ is to make the definition scale-invariant: with this definition, stretching and squeezing the axes does not affect
%membership in $\CSI(c,d)$.)'' but this is no longer true because of the $h$ factor, right?}
%\pnote{I think that the new definition is scale-invariant.}}

Many distributions arising in practice have light tails, and distributions with light
tails can in general be learned more efficiently.  To analyze learning shift-invariant distributions
in a manner that takes advantage of light tails when they are available, while accommodating heavier tails
when necessary, we define classes with different combinations of shift-invariant and tail behavior.
%  
% Our analysis refines the above notion of shift-invariance by taking into account the rate of tail decay.  
Given a nonincreasing function $g: \R^+ \to [0,1]$ which satisfies $\lim_{t \to +\infty} g(t) = 0$, we define the class of densities $\CSI(c,d,g)$ to consist of those $f \in \CSI(c,d)$ which have the additional property that for all $t > 0$, it holds that 
\[
\Prx_{\bx \leftarrow f}\left[|| \bx - \mu || > t \right] \leq g(t),
\]
where $\mu \in \R^d$ is the mean of the distribution $f.$  
% (Note that by Chebychev's inequality, it suffices to consider only tail decay functions $g$ that satisfy $g(t) \leq 1/t^2$ for $t > 1.$)

As motivation for its study, we feel that  $\CSI(c,d,g)$ is a simple and easily understood class that exhibits an attractive tradeoff between expressiveness and tractability.  As we show, it is broad enough to include distributions of central interest such as multidimensional isotropic log-concave distributions, but it is also limited enough to admit efficient noise-tolerant density estimation algorithms.

\medskip

\noindent {\bf Our density estimation framework.}  We recall the standard notion of density estimation with respect to total variation distance.  Given a class $\calC$ of densities over $\mathbb{R}^d$, a density estimation algorithm for $\calC$ is given access to i.i.d. draws from $f$, where $f \in \calC$ is the unknown \emph{target density} to be learned.  For any $f \in \calC$, given any parameter $\eps > 0$, after making some number of draws depending on $d$ and $\eps$ the density estimation algorithm must output a description of a hypothesis density $h$ over $\mathbb{R}^d$ which, with high probability over the draws from $f$, satisfies $\dtv(f,h) \leq \eps$.  It is of interest both to bound the \emph{sample complexity} of such an algorithm (the number of draws from $f$ that it makes) and its running time.

Our learning results will hold even in a challenging model of \emph{noise-tolerant} density estimation for a class $\calC$.  In this framework, the density estimation algorithm is given access to i.i.d. draws from $f'$, which is a mixture $f' = (1-\eps)f + \eps f_{\mathrm{noise}}$ where $f \in \calC$ and $f_{\mathrm{noise}}$ may be any density. (We will sometimes say that such an $f'$ is an \emph{$\eps$-corrupted} version of $f$. 
This model of noise is sometimes referred to as \emph{Huber's contamination model}~\citep{huber1967behavior}.)  Now the goal of the density estimation algorithm is to output a description of a hypothesis density $h$ over $\mathbb{R}^d$ which, with probability at least (say) 9/10 over the draws from $f'$, satisfies $\dtv(f',h) \leq O(\eps)$. This is a challenging variant of the usual density estimation framework, especially for multidimensional density estimation.  In particular, there are simple distribution learning problems (such as learning a single Gaussian or product distribution over $\{0,1\}^n$) which are essentially trivial in the noise-free setting, but for which computationally efficient noise-tolerant learning algorithms have proved to be a significant challenge \cite{DKKLMNS16,DKKLMS18, Steinhardt18}.

\subsection{Results}

Our main positive result is a general algorithm which efficiently learns any class $\CSI(c,d,g)$ in the noise-tolerant model described above.  Given 
a constant $c$ and a tail bound 
% $g(\cdot)$, 
$g$, 
we show that any distribution in the class $\CSI(c,d,g)$ can be noise-tolerantly learned to any error $O(\eps)$ with a sample complexity that depends on $c, g, \eps$ and $d$.   The running time of our algorithm is roughly 
% essentially 
quadratic in the sample complexity, and the sample complexity is 
% roughly 
$O_{c,d,g}(1) \cdot {\left(\frac {1}{\eps}\right)^{d+2}}$
 (see Theorem~\ref{thm:main-semiagnostic} in Section~\ref{sec:semi-agnostic} for a precise statement of the exact bound).
% , which is somewhat involved).  
These bounds on the number of examples and running time do not depend on which member of $\CSI(c,d,g)$ is being learned.


% Let us make some remarks about this sample complexity.  First, observe that since (as noted above) the only interesting tail bound functions $g(\cdot)$ satisfy $g(t) \leq 1/t^2$, we will always have $g^{-1}(t) < 1/t^{1/2}$, and hence the sample complexity of our algorithm will never be worse than (essentially) $O_d(1) \cdot (1/\eps^{3d/2 + 2})$.  In many interesting cases the tail bound function $g(\cdot)$ satisfies $g(t) = 1/t^{\omega(1)}$ (corresponding to sub-polynomial tails), in which case our sample complexity is essentially $1/\eps^{d(1+o(1))}.$  
% 


%\ignore{Our upper bound also applies in the
%semi-agnostic learning model, in which the data need not be 
%generated by a distribution in $\CSI(c,d)$, but rather from
%a distribution whose total variation distance from a member of 
%$\CSI(c,d)$ is $c_0 \epsilon$, for a constant $c_0$.  Since
%$\CSI(c,d)$ is scale-free, the range $[-1,1]$ of values taken by
%each variable can be replaced by any rescaling.  Also, since the algorithm
%is semi-agnostic, the hard constraint on the support can be replaced by
%a requirement that the distribution has light tails (see Definition~\ref{def:compact-support} below), \red{at the cost of only a small increase in running time and sample complexity.}
%}


\ignore{
\rnote{We need to work this out}To illustrate the power of our analysis, we note
that our positive results straightfowardly imply that the class of $d$-dimensional log-concave
distributions can be learned 
% in the above stated time and sample complexity,
% time, 
with the time and sample complexity described above,
solving an open problem posed by Diakonikolas \emph{et al.} \citep{diakonikolas2016learning}.
}


\medskip
\noindent {\bf Application:  Learning multivariate log-concave densities.} A multivariate density function $f$ over $\mathbb{R}^d$ is said to be \emph{log-concave} if there is an upper semi-continuous concave function $\phi: \mathbb{R}^d \to [-\infty,\infty)$ such that $f(x) = e^{\phi(x)}$ for all $x$.  Log-concave distributions arise in a range of contexts and have been well studied; see \citep{CDSS13,CDSS14,acharya2017sample,AcharyaDK15,CanonneDGR16,DKS16a} for work on density estimation of univariate (discrete and continuous) log-concave distributions.  In the multivariate case, 
% Kim and Samworth \cite{KimSamworth14} 
\citep{KimSamworth14} 
gave a sample complexity lower bound (for squared Hellinger distance) which implies that $\Omega(1/\eps^{(d+1)/2})$ samples are needed to learn $d$-dimensional log-concave densities to error $\eps$.  More recently, \citep{diakonikolas2016learning} established the first finite sample complexity upper bound for multivariate log-concave densities, by giving an algorithm that \emph{semi-agnostically} (i.e. noise-tolerantly in a very strong sense) learns any $d$-dimensional log-concave density using $\tilde{O}_d(1/\eps^{(d+5)/2})$ samples.  The 
% \cite{diakonikolas2016learning} algorithm 
algorithm of \citep{diakonikolas2016learning} 
is not computationally efficient, and indeed, Diakonikolas et al.\ ask if there is an algorithm with running time polynomial in the sample complexity, referring to this as ``a challenging and important open question.'' A subsequent (and recent) work of 
Carpenter et al.~\citep{carpenter2018} showed that the maximum likelihood estimator (MLE) is statistically efficient (i.e., achieves near optimal sample complexity). However, we note that the MLE is computationally inefficient and thus has no bearing on the question of finding an efficient algorithm for learning log-concave densities.



We show that multivariate log-concave densities can be learned in
polynomial time as a special case of our main algorithmic result.  We
establish that any $d$-dimensional log-concave density is
$O_d(1)$-shift-invariant.  Together with well-known tail bounds on
$d$-dimensional log-concave densities, this easily yields that any
$d$-dimensional log-concave density belongs to $\CSI(c,d,g)$ where the
tail bound function $g$ is inverse exponential.
Theorem~\ref{thm:main-semiagnostic} then immediately implies the
following, answering the open question of
\citep{diakonikolas2016learning}:

\begin{theorem} \label{thm:log-concave-intro}
There is an algorithm with the following property:  Let $f$ be a unknown log-concave density over $\mathbb{R}^d$ and let $f'$ be an $\eps$-corruption of $f$.\ignore{ whose covariance matrix $\Sigma$ has full rank} Given any error parameter $\eps>0$ and confidence parameter $\delta > 0$ and access to independent draws from $f'$, the algorithm with probability $1-\delta$ outputs a hypothesis density ${h}:\mathbb{R}^d \rightarrow \mathbb{R}^{\geq 0}$ such that $\int_{x \in \mathbb{R}^d} |f'(x) - {h}(x)| \le O(\epsilon)$.  The algorithm runs in time $\tilde{O}_d(1/\eps^{2d+2}) \cdot \log^2(1/\delta)$ and uses $\tilde{O}_d(1/\eps^{d+2}) \cdot \log^2(1/\delta)$ many samples.  
\end{theorem}

While our sample complexity is quadratically larger than the optimal sample complexity for learning log-concave distributions (from \citep{diakonikolas2016learning}), such \emph{computational-statistical} tradeoffs are in fact quite common (see, for example, the work of~\cite{bhaskara2015sparse} which gives a faster algorithm for learning Gaussian mixture models by using more samples).

\medskip
\noindent {\bf A lower bound.}  We also prove a simple lower bound,
showing that any algorithm that learns shift-invariant $d$-dimensional
densities with bounded support to error $\eps$ must use
$\Omega\left(1/\epsilon^d\right)$ examples.  These densities may be
thought of as satisfying the strongest possible rate of tail decay as
they have zero tail mass outside of a bounded region (corresponding to
$g(t)=0$ for $t$ larger than some absolute constant).  This lower
bound shows that a sample complexity of at least $1/\eps^d$ is
necessary even for very structured special cases of our multivariate
density estimation problem.



\subsection{Our approach} \label{sec:approach}

For simplicity, and because it is a key component of our general algorithm, we first describe how our algorithm learns an $\eps$-error hypothesis when the target distribution belongs to $\CSI(c,d)$ and 
% further satisfies two key conditions.  The first of these is that $f$ 
also has \emph{bounded support}:  all its mass is on points in the origin-centered ball of radius $1/2$.  

In this special case, analyzed in Section~\ref{sec:restricted}, our algorithm has two conceptual stages.  First, we smooth the density that we are to learn through convolution -- this is done in a simple way by randomly perturbing each draw.  This convolution
uses a kernel that damps the contributions to the density coming from 
high-frequency functions in its Fourier decomposition; intuitively, the shift-invariance of the target density ensures
that the convolved density (which is an average over small shifts of the original density) is close to the original density.
In the second conceptual stage,
the algorithm approximates relatively few Fourier coefficients of the smoothed density.  We show that an
inverse Fourier transformation using this approximation still provides an accurate approximation to
the target density.\footnote{We note that a simpler version of this approach, which only uses a smoothing kernel and does not employ Fourier analysis, can be shown to give a 
similar, but quantitatively worse,
% quantatively 
results, such as a sample complexity of essentially $1/\eps^{2d}$ when $g(t)$ is zero outside of a bounded region.  However, this is worse than the lower bound of $\Omega(1/\eps^{d})$ by a quadratic factor, whereas our algorithm essentially achieves this optimal sample complexity.}

Next, in Section~\ref{sec:no-noise}, we consider the more general case in which the target distribution belongs to the class $\CSI(c,d,g)$ (so at this point we are not yet in the noise-tolerant framework).  Here the high-level idea of our approach is very straightforward:  it is essentially to reduce to the simpler special case (of bounded support and good shift-invariance in every direction) described above.  
% Building on a result of 
% % Rudelson 
% \citep{Rudelson:2003}, we give a simple algorithm which can be used to affinely transform (in an invertible way) an arbitrary shift-invariant distribution into a shift-invariant distribution which satisfies the boundedness and variance conditions required for the above special case, while incurring only a small error.  
(A crucial aspect of this transformation algorithm is that it uses only a small number of draws from the original shift-invariant distribution; we return to this point below.)  We can then use the algorithm for the special case to obtain a high-accuracy hypothesis, and perform the inverse transformation to obtain a high-accuracy hypothesis for the original general distribution.  We remark that while the conceptual idea is thus very straightforward, there are a number of technical challenges that must be met to implement this approach.  One of these is that it is necessary to truncate the tails of the original distribution so that an affine transformation of it will have bounded support, and doing this changes the shift-invariance of the original distribution.  Another is that the transformation procedure only succeeds with non-negligible probability, so we must run this overall approach multiple times and perform hypothesis selection to actually end up with a single high-accuracy hypothesis.  
% Because of space constraints, full details of the algorithm and analysis are given in Appendix~\ref{ap:no-noise}.

In Section~\ref{sec:semi-agnostic}, we consider the most general case of noise-tolerant density estimation for $\CSI(c,d,g).$  Recall that in this setting the target density $f'$ is some distribution which need not actually belong to $\CSI(c,d,g)$ but satisfies $\dtv(f',f) \leq \eps$ for some density $f \in \CSI(c,d,g)$.  It turns out that this case can be handled using essentially the same algorithm as the previous paragraph.  We show that even in the noise-tolerant setting, our transformation algorithm will still successfully find a transformation as above that would succeed if the target density were $f \in \CSI(c,d,g)$ rather than $f'$.  (This robustness of the transformation algorithm crucially relies on the fact that it only uses a small number of draws from the given distribution to be learned.)  We then show that after transforming $f'$ in this way, the original algorithm for the special case can in fact learn the transformed version of $f'$ to high accuracy; then, as in the previous paragraph, performing the inverse transformation gives a high-accuracy hypothesis for $f'$.

In Section~\ref{sec:logconcave} we apply the above results to establish efficient noise-tolerant learnability of log-concave densities over $\R^d$. To apply our results, we need to have (i) bounds on the rate of tail decay, and (ii) shift-invariance bounds.  As noted earlier, exponential tail bounds on $d$-dimensional log-concave densities are well known, so it  remains to establish shift-invariance.  Using basic properties of log-concave densities, in Section~\ref{sec:logconcave} we show that any $d$-dimensional isotropic
log-concave density is $O_d(1)$-shift-invariant.  Armed with this bound, by applying our noise-tolerant learning result (Theorem~\ref{thm:main-semiagnostic}) we get that any $d$-dimensional isotropic log-concave density can be noise-tolerantly learned in time $\tilde{O}_d(1/\eps^{2d+2})$, using $\tilde{O}_d(1/\eps^{d+2})$ samples.
Log-concave distributions are shift-invariant even if they are only approximately isotropic.
We show that general log-concave distributions may be learned by bringing them into approximately
isotropic position with a preprocessing step, borrowing techniques from \cite{LovaszVempala07}.

\medskip

\noindent
{\bf The lower bound.}  As is standard, our lower bound (proved in Section~\ref{sec:lowerbound})  is obtained
via Fano's inequality.  We identify a large set $\cF$ of
bounded-support shift-invariant $d$-dimensional densities with the
following two properties: all pairs of densities from $\cF$ have
KL-divergence that is not too big (so that they are hard to tell
apart), but also have total variation distance that is not too small
(so that a successful learning algorithm is required to tell them
apart). The members of $\cF$ are obtained by choosing functions that
take one of two values in each cell of a $d$-dimensional checkerboard.
The two possible values are within a small constant factor of each
other, which keeps the KL divergence small.  To make the total
variation distance large, we choose the values using an
error-correcting code -- this means that distinct members of $\cF$
have different values on a constant fraction of the cells, which leads
to large variation distance.\ignore{ we also choose values are
  different enough to achieve the desired lower bound on the
  total-variation distance.}

\subsection{Related work} The most closely related work that we are aware
of was mentioned above: 
% Holmstr{\"o}m \emph{et al.} \cite{holmstrom1992asymptotic}
\citep{holmstrom1992asymptotic}
obtained bounds similar to ours for using kernel methods to learn densities that belong to various 
Sobolev spaces.  As mentioned above, these results do not directly apply for learning densities in
$\CSI(c,d,g)$ because of the possibility of jump discontinuities.\ignore{  \rnote{Removed the following parenthetical:  \green{(On
the other hand, it may be possible to obtain results similar to ours by arguing that, 
via a suitable convolution, a member of $\CSI(c,d,g)$ may be
approximated by a member of a Sobolev space.  The Fourier analysis
used in our approach appears to be a simpler and more illuminating path to our main result.)}}}
\citep{holmstrom1992asymptotic} also
proved a lower bound on the sample complexity of  algorithms that compute kernel
density estimates.  In contrast our lower bound holds for any density estimation algorithm,
kernel-based or otherwise.

The assumption that the target density belongs to a Besov
space 
(see \citep{kle2009smoothing})
makes reference to the effect of shifts on the distribution, as does shift-invariance.
We do not see any obvious containments between classes of functions defined through
shift-invariance and Besov spaces, but this is a potential topic for further research.  
% The intro of https://arxiv.org/pdf/1605.00162.pdf claims a result, but
% the body of the paper adds the assumption that the log-concave
% distribution is a Radon measure.  Still it may be safer not to
% make the below claim.
%
% \blue{To the best of our knowledge, log-concave distributions
% have not been proved to be members of any Besov space; the ease with
% which they are shown to be shift-invariant illustrates the utility of
% this notion.}

Another difference with prior work is the ability of our approach to succeed in the challenging noise-tolerant learning model.  We are not aware of analyses for density estimation of densities belonging to Sobolev or Besov spaces that extend to the noise-tolerant setting in which the target density is only assumed to be close to some density in the relevant class.

As mentioned above, shift-invariance was used in the analysis of algorithms for
learning discrete probability distributions in \citep{barbour1999poisson,daskalakis2013learning}.
Likewise, both the discrete and continuous Fourier transforms have been used in the past to learn discrete probability distributions~\citep{diakonikolas2016optimal,diakonikolas2016fourier, DDKT16}.
%Concentration of the discrete Fourier transform was used for learning discrete probability
%distributions in \citep{diakonikolas2016optimal,diakonikolas2016fourier}.  

