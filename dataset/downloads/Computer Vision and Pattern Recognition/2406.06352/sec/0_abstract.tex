\begin{abstract}
Mitigating biases in generative AI and, particularly in text-to-image models, is of high importance given their growing implications in society. The biased datasets used for training pose challenges in ensuring the responsible development of these models, and mitigation through hard prompting or embedding alteration, are the most common present solutions. Our work introduces a novel approach to achieve diverse and inclusive synthetic images by learning a direction in the latent space and solely modifying the initial Gaussian noise provided for the diffusion process. Maintaining a neutral prompt and untouched embeddings, this approach successfully adapts to diverse debiasing scenarios, such as geographical biases. Moreover, our work proves it is possible to linearly combine these learned latent directions to introduce new mitigations, and if desired, integrate it with text embedding adjustments. Furthermore, text-to-image models lack transparency for assessing bias in outputs, unless visually inspected. Thus, we provide a tool to empower developers to select their desired concepts to mitigate. The project page with code is available online\footnote{\url{https://latent-debiasing-directions.compute.dtu.dk/}}.
% \footnote{Project page: \url{https://latent-diffusion-directions.compute.dtu.dk/}

%Additionally, there is an existing need to provide explainability in text-to-image models to evaluate the bias present in generations. Therefore, we provide an understanding tool to empower developers in selecting their desired concepts to mitigate.
%In this paper, we present four comprehensive understanding pipelines to empower developers in selecting and addressing specific concepts for mitigation purposes.


%As a result, We present the potential of using the learned latent directions, and linearly combine them, contributing to responsible generative AI development.

%This paper addresses these challenges by proposing two lines of exploration in diffusion models: understanding and mitigating biases. The understanding section introduces four automated pipelines (TERU, VERU, SDIG, ODIG) to analyze the model's text and vision embeddings and its understanding of prompts in terms of the generated images' components. The mitigation work focuses on achieving debiased generations by a novel approach, uniquely altering the random Gaussian noise provided for the diffusion process. 
\end{abstract}