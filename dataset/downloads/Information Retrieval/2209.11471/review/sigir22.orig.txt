# Feedbacks

Ben Carterette, Shane Culpepper, Gabriella Kazai

SIGIR 2022 PC Chairs

[https://sigir.org/sigir2022](https://sigir.org/sigir2022)

[https://twitter.com/SIGIRConf](https://twitter.com/SIGIRConf)

SUBMISSION: 697

TITLE: Modeling and Leveraging Prerequisite Knowledge in Recommendation

- ------------------------ METAREVIEW ------------------------

This is the meta-review for submission 697. The reviewers identified different strengths (+) and weaknesses (-):

+ Promising research direction.

- Empirical work is relatively weak due to the ground truth data not being a great fit for the problem domain and potential data leakage issues.
- Technology-enhanced learning is an important related work area that is not considered.
- Important details are missing.

Overall, the considerable weaknesses do not make the paper a good fit for SIGIR in its current form.

- ---------------------- REVIEW 1 ---------------------
- ---------- Relevance to SIGIR -----------

SCORE: 4 (good)

- ---------- Technical soundness -----------

SCORE: 2 (poor)

- ---- TEXT:

The paper is interesting and the topic relevant to SIGIR. My main concerns are about the framing and its consequences for the actual experiments, baselines, ground truths and contributions. The authors position their work in a learning-context where learning paths (curricula), i.e. relations between acquired competences/knowledge concepts are crucial to determine suitable items to users. However, with the exception of the Course dataset, the experiments address scenarios where learning may not be within scope but users may select items based on taste and mood for mere entertainment purposes. Table 6 underlines that point: I don't have to have a deeper understanding about scientists to ENJOY Alien as a movie. The ground truth data in both the Amazon and MovieLens data is not geared towards learning (not even informal learning) and does not in any way reflect the stated optimisation goal, and reflect user preferences independent of learning objectives. Potential performance gains in those cases may simply be due to the fact that the approach encodes implicit item similarities and user preferences rather than actual knowledge prerequisites.

To this end, my suggestion is to reconsider the framing (learning vs non-learning) and then pick an experimental setup able to support the contribution in that very area.

In that regard, the contributions and novelty also seems overstated, specifically the claim that this is the first recommender system work that considers prerequisites. When it comes to eductional and learning settings, encoding prior knowledge through some representation of knowledge concepts has been one of the general aims in technology-enhanced learning throughout the last decade and there are long-standing workshop series in that area. While techniques in the past often differ, this work should position itself in this context.

If we consider the contribution not in the context of actual learning/knowledge acquisition but in recommendation in general, assuming the presented PDRS approach encodes item relationships and user preferences rather than actual *curricula*, there is also a wealth of further baselines with which to compare.

- ---------- Quality of presentation -----------

SCORE: 4 (good)

- ---- TEXT:

The paper is well presented and easy to follow with minor exceptions.

- ---------- Adequacy of citations -----------

SCORE: 2 (poor)

- ---- TEXT:

See overall comments.

- ---------- Reproducibility of methods -----------

SCORE: 3 (fair)

- ---- TEXT:

Data is available, code seems not shared yet and there are some questions re annotation (see overall comments).

- ---------- Strengths -----------
- relevant and interesting topic
- easy to read and follow paper
- experiments on three datasets
- ---------- Weaknesses -----------
- poor match of data and approach: two of the datasets do not match the knowledge acquisition/learning-scenario addressed in the paper
- related work: the approach should position itself better in the context of learning-related recommender systems (an active research area since many years that is often lacking a grounding the actual RecSys state-of-the-art)
- ---------- Overall recommendation -----------

SCORE: -1 (weak reject)

- ---------- Detailed comments to authors -----------

This paper introduces a neural architecture for prerequisite-based item recommendation, that uses two encoders that create embedded representations of users, items and prerequisites which are utilised for final recommendations. For learning prerequisites, the authors use two different approaches, namely one that learns potential prerequsites from distributional patterns of user/concept interactions, assuming that concepts frequently consumed prior to other concepts may be perceived as prerequisites. In addition, concept relations are derived from Wikipedia. The approach is applied to three datasets, Course (an actual dataset of course-taking histories in a learning scenario) and the well-known MovieLens and Amazon Books dataset frequently used for recommendation.

The paper is interesting and the topic relevant to SIGIR. My main concerns are about the framing and its consequences for the actual experiments, baselines, ground truths and contributions. The authors position their work in a learning-context where learning paths (curricula), i.e. relations between acquired competences/knowledge concepts are crucial to determine suitable items to users. However, with the exception of the Course dataset, the experiments address scenarios where learning may not be within scope but users may select items based on taste and mood for mere entertainment purposes. Table 6 underlines that point: I don't have to have a deeper understanding about scientists to ENJOY Alien as a movie. The ground truth data in both the Amazon and MovieLens data is not geared towards learning (not even informal learning) and does not in any way reflect the stated optimisation goal, and reflect user preferences independent of learning objectives. Potential performance gains in those cases may simply be due to the fact that the approach encodes implicit item similarities and user preferences rather than actual knowledge prerequisites.

To this end, my suggestion is to reconsider the framing (learning vs non-learning) and then pick an experimental setup able to support the contribution in that very area.

In that regard, the contributions and novelty also seems overstated, specifically the claim that this is the first recommender system work that considers prerequisites. When it comes to eductional and learning settings, encoding prior knowledge through some representation of knowledge concepts has been one of the general aims in technology-enhanced learning throughout the last decade and there are long-standing workshop series in that area. While techniques in the past often differ, this work should position itself in this context.

If we consider the contribution not in the context of actual learning/knowledge acquisition but in recommendation in general, assuming the presented PDRS approach encodes item relationships and user preferences rather than actual *curricula*, there is also a wealth of further baselines with which to compare.

Minor:

- Section 3: "For constructing user’s prior knowledge and target knowledge, we take knowledge concepts from the first 30% and the last 20% of items interacted with as prior and target knowledge". How did you arrive at these thresholds? Wouldn't these be necessarily very dataset-dependent?
- Section 3: "we train a logistic regression by semi-supervision over a few annotated samples (300 per dataset)." Who annotated what and with what inter-rater agreement?
- Figure 3: It wasn't transparent how the two prerequisite metrics (Asyd, Wikipedia-based) are considered in the model.
- Figure 4 is not explained in the paper
- Table 7: what exactly is the shown NDCG score actually computed on?
- ---------- Nominate for Best Paper -----------

SELECTION: no

---

- ---------------------- REVIEW 2 ---------------------
- ---------- Relevance to SIGIR -----------

SCORE: 4 (good)

- ---------- Technical soundness -----------

SCORE: 2 (poor)

- ---------- Quality of presentation -----------

SCORE: 4 (good)

- ---------- Adequacy of citations -----------

SCORE: 4 (good)

- ---------- Reproducibility of methods -----------

SCORE: 3 (fair)

- ---------- Strengths -----------

1. This work clearly presents an interesting problem formulation in recommender systems, which tackles the long-term learning goal of a user, instead of accurate one-shot information consumption.

2. The ideas of mining prerequisite knowledge and incorporating it into the recommendation algorithm is novel.

3. extensive analyses are conducted on different components of the model (e.g. prerequisite relation mining) and ablation analysis of different combinations of the types of knowledge (prior, target, current content).

- ---------- Weaknesses -----------

1. Rigor of experiments: based on the paper's description of data split strategy, it is unclear if the in prerequisite knowledge mining stage included test data in the recommendation algorithm training/test stage. If it's true, then the advantage of the proposed method can be explained by data leakage, which would critically undermine its merit.

- ---------- Overall recommendation -----------

SCORE: -1 (weak reject)

- ---------- Detailed comments to authors -----------

This paper proposes a new problem setup for recommender systems: a user is exposed to a trajectory of content, and previous consumption of information may inform later consumption. The method represent an item’s content by mining concepts within the item’s description, and then represent the prerequisite relations between concepts by mining (1) the semantic distance between concepts based on BERT embedding; (2) the trajectory of user’s exposure to concepts in the user-item rating data, and (3) concept distance in Wikipedia. The prerequisite concepts and relations give prior knowledge and target knowledge of a user, which enhances the feature representation of the proposed PDR recommendation algorithm.

This work clearly presents an interesting problem formulation in recommender systems, which tackles the long-term learning goal of a user, instead of accurate one-shot information consumption. The ideas of mining prerequisite knowledge and incorporating it into the recommendation algorithm is novel. The authors did several analyses on different components of the model (e.g. prerequisite relation mining) and ablation analysis of different combinations of the types of knowledge (prior, target, current content).

My largest concern is the rigor of the experimental evaluation, which affects the interpretation of the claimed advantage of the proposed PDR system. It is clear that the prerequisite relations are the key contributor of the performance advantage shown in Table 3, compared to other baseline algorithms without prerequisite knowledge. It is therefore critical to ensure that the prerequisite relations are mined from the same training data subset as all other baseline algorithms. In other words, PDR should not have mined the prerequisite relations from the test data, which is a data leakage from test to training. However, according to the data description in Section 3:

- “For constructing user’s prior knowledge and target knowledge, we take knowledge concepts from the first 30% and the last 20% of items interacted with as prior and target knowledge, respectively; such as to leave the remaining 50% of knowledge concepts for training and testing a PDR system.”

And in Section 5.2, the author said:

- “we decide to split our interaction data into (80%, 5%, 15%) to serve as training, validation and testing, respectively. ”

It is unclear whether the prerequisite relation mining was conducted on the 80% training data, or it is conducted using the entire data (including test data). It is also unclear how PDR addresses the out-of-vocabulary problem – when an item in the test set does not have a prerequisite relation with any known concept, can PDR handle such cases robustly? If we cannot clearly rule out the potential of data leakage from test to training, then all performance improvements shown in Tables 3 and 4 will be put to doubt – is the performance gain due to the prerequisite knowledge, or due to an (unintentional) exposure of test data to the training stage?

Therefore I recommend the authors to clarify their data split strategy in prerequisite knowledge mining stage and RS algorithm training stage, and ensure the two stages uses the same training data with the test data untouched in either stage. That will allow a better assessment of the merit of the proposed method.

- ---------- Nominate for Best Paper -----------

SELECTION: no

---

- ---------------------- REVIEW 3 ---------------------
- ---------- Relevance to SIGIR -----------

SCORE: 4 (good)

- ---- TEXT:

Recommendation is a hot topic of recent SIGIR conferences.

- ---------- Technical soundness -----------

SCORE: 3 (fair)

- ---- TEXT:

While the idea of leveraging prerequisite knowledge is interesting, there are many issues in both methodology and evaluation.

- ---------- Quality of presentation -----------

SCORE: 3 (fair)

- ---- TEXT:

Overall, the presentation is acceptable but could be further improved. Some important details are not clear in the paper.

- ---------- Adequacy of citations -----------

SCORE: 2 (poor)

- ---- TEXT:

The paper overlooks several lines of relevant research.

- ---------- Reproducibility of methods -----------

SCORE: 3 (fair)

- ---- TEXT:

The reproducibility could be improved by polishing the presentation and releasing code.

- ---------- Strengths -----------
- S1: Leveraging prerequisite knowledge is a promising direction for not only improving recommendation performance but also making recommenders more accountable and transparent.
- S2: The authors carried out various experiments and analyses, offering many interesting insights.
- ---------- Weaknesses -----------
- W1: The paper overlooks several lines of relevant research.
- W2: There are many issues in both methodology and evaluation.
- W3: Some important details are not clear in the paper.
- W4: The proposed method is less sophisticated (although I do not put too much emphasis on technical contributions).
- ---------- Overall recommendation -----------

SCORE: -1 (weak reject)

- ---------- Detailed comments to authors -----------

This paper proposes to extract prerequisite relationships among concepts and leverage them for improving recommendation performance. The idea is interesting and promising (S1). I enjoyed reading this paper, especially in the latter part presenting several interesting results (S2). However, I have too many concerns as detailed below, due to which I cannot recommend accepting it in its current form.

- *W1**

Although the paper states `we are the first to explore the use of prerequisite knowledge for recommendation`, several lines of existing research addresses the same or similar problems.

- Sequential recommendation: This task considers the order dependency among items in users' interactions to predict what items they will select next. While typical sequential recommenders do not explicitly consider prerequisite relationships among concepts, the prediction target looks the same as this work to me, especially in non-educational domains where the meaning of the prerequisite is unclear. (I do not pick up any particular papers because many recommenders are proposed for this task every year.)
- Progression modeling: Some papers [Ref-1, Ref-2, Ref-3] apply progression modeling techniques to the recommendation problem to capture the transitions of user interests/expertise/skills/... over time in an interpretable manner. Those papers only require user interaction data to learn both recommenders and transitions.
- Knowledge tracing: This is not a recommendation task but is similar to the present work in that it models learners' knowledge state. Some work (e.g., [Ref-4]) can estimate the dependency among concepts during the modeling process.

Almost no papers from these categories are cited in this submission, due to which the novelty of the present work is unclear. The paper should discuss whether the techniques proposed in such existing work can be used for this work and if so, add them to the baselines.

- *W2**
- Section 3, `no existing dataset is specifically designed for prerequisite modeling`: As described earlier, the task addressed in this paper is similar to knowledge tracing. Thus, the authors could use datasets for that task, which are counterexamples of this claim, I think.
- Section 3, `For this reason, we assume that users have mastered the knowledge contained in items previously interacted with`: I do not think this assumption always holds. Indeed, several papers [Ref-5, Ref-6, Ref-7] consider learners' forgetting phenomenon in the knowledge tracing task.
- Section 3, `we take knowledge concepts from the first 30% and the last 20% of items interacted with as prior and target knowledge, respectively; such as to leave the remaining 50% of knowledge concepts for training and testing a PDR system`: There are at least two issues. First, this regards the prior and target knowledge states do not change during the remaining interaction sequence, which is not realistic. Second, after the proposed recommender is deployed, interactions corresponding to the target knowledge cannot be obtained in this manner. This is because we only have interaction data before the target time point in that situation.
- Section 5.2: Negative sampling is reported to be a bad practice making evaluation results unreliable [Ref-8, Ref-9].
- Section 5.3: As described earlier, sequential modeling seems important for this problem. However, this paper selects baselines from non-sequential recommenders, which regard each user's past interactions as a set instead of a sequence. What is the reason for not adding sequential recommenders as baselines? If added, can the proposed recommender still achieve the best performance?
- Section 5.4: The paper does not report on statistical tests.
- Figure 4: RMSE and R-squared are metrics for the regression task, not for recommendation formulated as the ranking task.
- *W3**
- Section 3, Concept Extraction: I do not understand the detail of this process. For example, what is a fully-connected graph? How is it constructed? Is it different from the graph used for applying TextRank in the previous step? These may be explained in the literature [24], but I think the description should be self-contained as this part is a core process of the proposed solution.
- Section 3, Wikipedia Reference Distance: It is unclear to me how the authors find Wikipedia concepts (equivalent to articles?) related to a given concept. The symbols $t^{k_{i}}$ and $t^{k_{j}}$ are not defined. What do you mean by "refers to" (e.g., the existence of the corresponding phrase/link/...)?
- Section 3, Prerequisite Learning: How did you select 300 samples? Who annotated those samples? What are annotation criteria? How consistent were the annotations provided by different people?
- Section 5.2: How did you split each dataset into three splits, randomly or based on interaction timestamps? Given the sequential nature of the problem, the former splitting approach may result in data leakage.
- Section 6.1, `We also find that prerequisite graphs have different structural properties ...`: No corresponding result is presented for this paragraph. Thus, I cannot judge whether the statement here is valid.
- Section 6.3: My understanding is that the proposed recommender uses embedding matrices to encode users and items. If so, it is unclear to me how the authors obtained the embeddings for users and items unseen in the training phase.
- Section 7.1, `the prerequisite relations among concepts which we see as a type of subjective knowledge`: I disagree. Two features out of three for calculating KPL scores are obtained independent of interaction data. The remaining one is obtained from interaction data, but its scores are not personalized.
- *W4**
- The proposed recommender is based on MLPs, which is rather naive compared to recent recommenders. For example, is simply averaging knowledge embeddings sufficient for this task? Different concepts may be important for different inputs. If so, attention mechanisms are worth considering.
- *Other comments**
- Section 2: The function $f$ takes the union of three knowledge sets as an argument. To me, this looks incorrect because the proposed recommender in Figure 3 encodes the three sets separately. Otherwise, the recommender cannot tell apart which is the prior/target/item knowledge set. (By the way, I think the set notation in this function can be simplified.)
- Section 3: Until reading Section 5.1, I did not understand what item documents are. I think the current presentation flow confuses the reader.
- Figure 2: should KLP be KPL?
- Equation 1: Is $\sigma$ the sigmoid function?
- Section 5.1, `500 labeled samples`: According to Section 3, there are 300 annotated samples. Which is correct?
- Section 6.1: There is a reference error.
- Table 6: This table lists the concepts for which "machine learning" is estimated as a prerequisite. I also want to know the opposite: estimated prerequisites for "machine learning", which are not the same as the concepts with low KPL scores, I think.
- *References**

1.

[https://dl.acm.org/doi/10.1145/2488388.2488466](https://dl.acm.org/doi/10.1145/2488388.2488466)

2.

[https://dl.acm.org/doi/10.1145/2566486.2568044](https://dl.acm.org/doi/10.1145/2566486.2568044)

3.

[https://doi.org/10.1109/ICDE48307.2020.00022](https://doi.org/10.1109/ICDE48307.2020.00022)

4.

[https://dl.acm.org/doi/10.1145/3437963.3441802](https://dl.acm.org/doi/10.1145/3437963.3441802)

5.

[https://dl.acm.org/doi/10.1145/3308558.3313565](https://dl.acm.org/doi/10.1145/3308558.3313565)

6.

[https://dl.acm.org/doi/abs/10.1145/3340531.3411994](https://dl.acm.org/doi/abs/10.1145/3340531.3411994)

7.

[https://dl.acm.org/doi/10.1145/3132847.3132929](https://dl.acm.org/doi/10.1145/3132847.3132929)

8.

[https://dl.acm.org/doi/10.1145/3394486.3403226](https://dl.acm.org/doi/10.1145/3394486.3403226)

9.

[https://dl.acm.org/doi/abs/10.1145/3460231.3475943](https://dl.acm.org/doi/abs/10.1145/3460231.3475943)

- ---------- Nominate for Best Paper -----------

SELECTION: no