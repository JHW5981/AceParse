Reviewer #1
Questions
1. Overall Rating
Reject
2. Interest to Audience. Will this paper attract the interest of WSDM 2021 attendees? Will it be intriguing and inspiring? Might it be highlighted afterwards in the press as particularly innovative?
Some interest to a large fraction of the attendees or a lot of interest to some attendees
3. Significance: Does the paper contribute a major breakthrough or an incremental advance? Are other people (practitioners or researchers) likely to use these ideas or build on them? Is it clear how this work differs from previous contributions, and is related work adequately referenced?
Minor or incremental advance over known techniques or insights
4. Experiments: Do the experiments support the claims made in the paper and justify the proposed techniques?
OK, but certain claims are not covered by the experiments
5. Paper Clarity. Is the paper clearly written? Is it well-organized? Is the result section (and or are the proofs, if applicable) clear enough so that expert readers will understand them and can reproduce the results?
Good: most of the paper was understandable, with minor typos or details that could be improved
6. Summary of the paper (what is being proposed and in what context) and a brief justification of your overall recommendation. (One solid paragraph)
This paper proposes the concept of prerequisite knowledge at user level, which tries to capture the knowledge of the users that the models need to capture to improve the recommendation effectiveness. Concretely, the authors model the prior knowledge of the users and try to capture the target knowledge that will drive the recommendation process. The evaluation was performed on three datasets, considering classic evaluation metrics and several baselines.

While the paper has positive aspects, such as its clarity in presentation, the soundness of the technical solution, other aspects such as the distance from the prerequisite knowledge at item level are presented at conceptual level but not at a practical level. Indeed, the paper seems to lack of comparison with the state of the art in that sense.
7. Three (or more) strong points about the paper. Please be precise and explicit; clearly explain the value and nature of the contribution.
- It is interesting to consider a novel and user-centered perspective to prerequisite knowledge in recommender systems;
- The technical solution is sound and interesting;
- The paper presentation is clear and related work is well covered.

Please, refer to my detailed evaluation for more comments on these points.
8. Three (or more) weak points about the paper. Please clearly indicate whether the paper has any mistakes, missing related work, or results that cannot be considered a contribution. Please be polite, specific, and constructive.
- The practical difference between prerequisite knowledge at item and user level is not clear;
- There seems to be a lack of comparison with methods to deal with prerequisite knowledge at item level, to assess if they can be a solution to the problem at user level;
- Reproducibility can be limited due to the lack of shared source code.

Please, refer to my detailed evaluation for more comments on these points.
9. Detailed Evaluation (Contribution, Pros/Cons, Errors); please number each point and please provide as constructive feedback as possible.
This paper deals with a timely and relevant topic (prerequisite knowledge in recommender systems) and does so from a novel user-centered perspective. This comes in a nicely presented paper, which presents a novel interesting model. In short, this paper has a lot of positive aspects, which made it an interesting reading.

Unfortunately, there are also shortcomings, which might limiting for a paper published at WSDM. I will now highlight them, with suggestions on how to possibly deal with them.

At a and practical level, it is not clear why modeling prerequisite knowledge at user level is different from doing that at item level. Given that user prerequisites are extracted from items, it all seems to return to the same problem, just tackled at a different granularity. I invite the authors to consider the interplay between these two forms of modeling in more depth in future revisions of this work.

As a consequence, this paper seems to miss an evaluation on why prerequisite knowledge at the item level is not a solution also to at user level. Unfortunately, these baselines seem to be missing in the evaluation of this work. I recommend the authors to include them in the future.

Finally, reproducibility seems limited because of the lack of source code. I invite the authors to consider linking the source code in future revisions of the paper; this would facilitate future advances on this area.
10. Reviewer's confidence.
Knowledgeable in this sub-area
Reviewer #3
Questions
1. Overall Rating
Weak Accept
2. Interest to Audience. Will this paper attract the interest of WSDM 2021 attendees? Will it be intriguing and inspiring? Might it be highlighted afterwards in the press as particularly innovative?
Interesting to many attendees
3. Significance: Does the paper contribute a major breakthrough or an incremental advance? Are other people (practitioners or researchers) likely to use these ideas or build on them? Is it clear how this work differs from previous contributions, and is related work adequately referenced?
Moderate advance in methodology or understanding of phenomena, likely to be useful to others
4. Experiments: Do the experiments support the claims made in the paper and justify the proposed techniques?
OK, but certain claims are not covered by the experiments
5. Paper Clarity. Is the paper clearly written? Is it well-organized? Is the result section (and or are the proofs, if applicable) clear enough so that expert readers will understand them and can reproduce the results?
Average: the main points of the paper was understandable, but some parts were not clear
6. Summary of the paper (what is being proposed and in what context) and a brief justification of your overall recommendation. (One solid paragraph)
The authors introduce a new approach titled, Prerequisite Driven Recommendation (PDR), that leverages prerequisite relations between concepts to improve recommendation systems. Their new approach presents a method to model and use prerequisite knowledge for commuting recommendations. The PDR framework consists of three different components: 1) PLM – logistic regression model to learn knowledge prerequisite linkage scores using three different types of features, 2) Encoder – learns a representation of user and items and concepts using two different modules. 3) KDRM – uses the learned representation of user and items and concepts to jointly train the prerequisite learning and recommendation models. They evaluate their framework in two settings: a warm start and a cold start scenario with three different datasets from different areas: Books, Movies, and Courses. Their method incorporates both domain and general prerequisites (derived from Wikipedia). The authors use Hits@N and NGDC@N. Additionally, the authors also conduct an ablation study to understand the importance of the different components of their model to the downstream task. They find the PDR approach to be useful for the cold-start problem and still see improvements for the warm start problem.
7. Three (or more) strong points about the paper. Please be precise and explicit; clearly explain the value and nature of the contribution.
-The authors consider the role of prerequisites for recommendation systems which seems like a natural requirement for certain problems and is intuitive.
-The authors conduct several experiments to validate their findings: the ablation study, the pre-training vs non-pretrained PDRS model with different hyper-parameters.
-The authors consider three datasets from different areas: courses, books, and movies.
-The PDRS approach can be used for cold-start cases
8. Three (or more) weak points about the paper. Please clearly indicate whether the paper has any mistakes, missing related work, or results that cannot be considered a contribution. Please be polite, specific, and constructive.
-Figure 1 is complex and hard to follow. A legend in the figure would increase the readability of the figure. The image can be simplified. Also, the explanation for the figure does not really match with what is done in the study. Thus, the figure 1 creates more of a confusion.
-It is unclear if the logistic regression model is sufficient to estimate the prerequisite linkage scores. There are no results evaluating this model.
-The authors do not provide any insights to runtime or space complexity
-It is unclear for what happens at {Hits, NDHC}@N for examples where there as less than 10 items to evaluate.
-Using Movies as the third evaluation dataset is not convincing enough. It would be more appropriate to use a scenario like task completion
-It is hard to quantify if the methods authors use to determine prerequisites is sufficient enough or how much noise and error it introduces to the PDRS framework.
9. Detailed Evaluation (Contribution, Pros/Cons, Errors); please number each point and please provide as constructive feedback as possible.
-The authors provide an approach to represent and extract prerequisites for recommendations with minimal human annotation
-The PDRS method shows promising results for cold-start problems.
-The PDRS method also appears to be useful for cases where modeling prerequisites is crucial (i.e., Courses) and where prerequisites could be beneficial (Books and Movies).
10. Reviewer's confidence.
Knowledgeable in this sub-area
Reviewer #4
Questions
1. Overall Rating
Weak Accept
2. Interest to Audience. Will this paper attract the interest of WSDM 2021 attendees? Will it be intriguing and inspiring? Might it be highlighted afterwards in the press as particularly innovative?
Some interest to a large fraction of the attendees or a lot of interest to some attendees
3. Significance: Does the paper contribute a major breakthrough or an incremental advance? Are other people (practitioners or researchers) likely to use these ideas or build on them? Is it clear how this work differs from previous contributions, and is related work adequately referenced?
Moderate advance in methodology or understanding of phenomena, likely to be useful to others
4. Experiments: Do the experiments support the claims made in the paper and justify the proposed techniques?
OK, but certain claims are not covered by the experiments
5. Paper Clarity. Is the paper clearly written? Is it well-organized? Is the result section (and or are the proofs, if applicable) clear enough so that expert readers will understand them and can reproduce the results?
Good: most of the paper was understandable, with minor typos or details that could be improved
6. Summary of the paper (what is being proposed and in what context) and a brief justification of your overall recommendation. (One solid paragraph)
This paper studied the problem of better capturing and leveraging prerequisite knowledge in recommendation systems, which is interesting and practical. The authors proposed a two-stage framework to address the problem of inferring prerequisite knowledge and then encoding the prerequisite knowledge into recommendation models. The effectiveness of the proposed method is demonstrated with detailed ablation studies over different types of real datasets. The motivation of the paper is very interesting and the paper is mostly well-written and easy to follow. And the analysis and detailed ablation studies are really helpful. The paper could be further enhanced if the experimental comparison could be more comprehensive.
7. Three (or more) strong points about the paper. Please be precise and explicit; clearly explain the value and nature of the contribution.
(1) Studied an interesting and practical problem.
(2) Proposed an end-to-end two-stage framework.
(3) Detailed ablation studies and experimental analysis.
8. Three (or more) weak points about the paper. Please clearly indicate whether the paper has any mistakes, missing related work, or results that cannot be considered a contribution. Please be polite, specific, and constructive.
(1) An intuitive and simple way to leverage the prerequisite knowledge for recommendation is to filter the recommendation results w.r.t. prerequisite knowledge dependencies. Namely, it should be straightforward to derive the item dependencies via the concept dependencies, and then filter the recommendation results to remove items whose dependent items are not already occupied by the users. It would be great if the authors could compare the proposed approach with this simple baseline to better justify the modeling complexity of jointly modeling the prerequisite knowledge together with user preferences.
(2) Experimental study could be more comprehensive. For example, one important baseline method, namely the two-tower approach to encode more metadata info on both user and item sides, is not included in experimental comparison. After all, the prerequisite knowledge is part of the user and item metadata.
(3) It could be interesting to see some case studies on both the prerequisite knowledge inference part and the overall recommendation part. It would help demonstrate whether the modeling ideas are working as intended.
(4) Some technical details were not well explained. For example, how the 30%/20% prior/target knowledge slicing is determined; how the hyperparameter tuning is performed for the proposed approach and the baseline approaches (e.s.p for the +KEM variates).
(5) Some typos. For instance, in Sec 5.1., "Large, positive KPL scores indicate that knowledge concept is a prerequisite of 'machine learning'" shall be "Large, positive KPL scores indicate that 'machine learning' is a prerequisite of the knowledge concept".
9. Detailed Evaluation (Contribution, Pros/Cons, Errors); please number each point and please provide as constructive feedback as possible.
The motivation of the paper is very interesting and the paper is mostly well-written and easy to follow. And the analysis and detailed ablation studies are really helpful. The paper could be further improved in the following aspects:
(1) An intuitive and simple way to leverage the prerequisite knowledge for recommendation is to filter the recommendation results w.r.t. prerequisite knowledge dependencies. Namely, it should be straightforward to derive the item dependencies via the concept dependencies, and then filter the recommendation results to remove items whose dependent items are not already occupied by the users. It would be great if the authors could compare the proposed approach with this simple baseline to better justify the modeling complexity of jointly modeling the prerequisite knowledge together with user preferences.
(2) Experimental study could be more comprehensive. For example, one important baseline method, namely the two-tower approach to encode more metadata info on both user and item sides, is not included in experimental comparison. After all, the prerequisite knowledge is part of the user and item metadata.
(3) It could be interesting to see more case studies on both the prerequisite knowledge inference part and the overall recommendation part. It would help demonstrate whether the modeling ideas are working as intended.
(4) Some technical details were not well explained. For example, how the 30%/20% prior/target knowledge slicing is determined; how the hyper-parameter tuning is performed for the proposed approach and the baseline approaches (e.s.p for the +KEM variates).
(5) Some typos. For instance, in Sec 5.1., "Large, positive KPL scores indicate that knowledge concept is a prerequisite of 'machine learning'" shall be "Large, positive KPL scores indicate that 'machine learning' is a prerequisite of the knowledge concept".
10. Reviewer's confidence.
Knowledgeable in this sub-area
Reviewer #5
Questions
1. Overall Rating
Weak Reject
2. Interest to Audience. Will this paper attract the interest of WSDM 2021 attendees? Will it be intriguing and inspiring? Might it be highlighted afterwards in the press as particularly innovative?
Interesting to a small group or sub-specialty
3. Significance: Does the paper contribute a major breakthrough or an incremental advance? Are other people (practitioners or researchers) likely to use these ideas or build on them? Is it clear how this work differs from previous contributions, and is related work adequately referenced?
Moderate advance in methodology or understanding of phenomena, likely to be useful to others
4. Experiments: Do the experiments support the claims made in the paper and justify the proposed techniques?
OK, but certain claims are not covered by the experiments
5. Paper Clarity. Is the paper clearly written? Is it well-organized? Is the result section (and or are the proofs, if applicable) clear enough so that expert readers will understand them and can reproduce the results?
Good: most of the paper was understandable, with minor typos or details that could be improved
6. Summary of the paper (what is being proposed and in what context) and a brief justification of your overall recommendation. (One solid paragraph)
This paper presents a recommendation framework by leveraging prerequisite knowledge. The authors emphasize three main contribution: 1) formalise the prerequisite knowledge via users’ prior knowledge, users’ target knowledge and the prerequisite graph, 2) develop a knowledge linking prediction method to extract prerequisite knowledge and 3) propose an joint training strategy to optimize knowledge linking prediction and recommendation tasks. Experiment are done on three datasets to demonstrate the proposed method.
7. Three (or more) strong points about the paper. Please be precise and explicit; clearly explain the value and nature of the contribution.
How to accurately model the user-item interactions with auxiliary side information is at the core of modern recommendation methods. One good point of the paper is the proposed prerequisite learning module. In particular, the author extract concepts and inferring prerequisite relation by several distance metric. Another good point is the joint training strategy to avoid weakening the prerequisite information.
8. Three (or more) weak points about the paper. Please clearly indicate whether the paper has any mistakes, missing related work, or results that cannot be considered a contribution. Please be polite, specific, and constructive.
The motivation of so-called “prerequisite knowledge” needs to be stated clearly.
These baselines are outdated.
Some important baselines are missing, which means the comparison might be unfair.
9. Detailed Evaluation (Contribution, Pros/Cons, Errors); please number each point and please provide as constructive feedback as possible.
1. In terms of motivation, the benefits of “prerequisite graph” are overstate.
a. Taking “prerequisite graph” as side information for recommendation is similar to KG-based recommendation, e.g. Section 3.1, the PLM module, where Concept Extraction is like NER, and (k_i, r_{ij}, k_j) is like KG triple. And the work PGPR in sigir19 construct their KG by user-item reviews without any external KG. The authors are suggested to clarify their differences and where the prerequisite graph advantages are.
b. In Section 6.1, paragraph 2, “……that prerequisites can be formalised as causal graphs”. While Section 3.1 shows that prerequisites are constructed by Sequential dependency(AsyD) and domain dependency(RefD). However, neither of them can represent causal relationship.
2. In terms of experiments, there are several limitations.
a. As for the dataset, SSG Courses dataset is not publicly available yet. However, no detail information of this dataset (say, the dataset’s scenarios and task). Without clear descriptions, the readers would be confused.
b. Those compared baselines are outdated, and the latest method among them is done in 2017. For example, there are many graph-based recommendation methods beat GCMC nowadays. What’s more, as related works, the KG-based recommendations are ignored. The authors are strongly suggested to select some of them as baseline.
10. Reviewer's confidence.
Knowledgeable in this sub-area