**Overall Concerns**

From Education perspective,

- Lack of the technology-enhanced learning context
> HC: ignore
- poor match of data and approach: two of the datasets do not match the knowledge acquisition/learning-scenario addressed in the paper
> HC: ignore
- related work: the approach should position itself better in the context of learning-related recommender systems (an active research area since many years that is often lacking a grounding the actual RecSys state-of-the-art)

From RS perspective,

- there is also a wealth of further baselines with which to compare
HC: accept, but ignore
- Lack of some import details
    - it is unclear if the in prerequisite knowledge mining stage included test data (data leakage)
    > HC: accept. TODO: distinguish the two stage more clearly
    - There are 2 splitting: prior, target, normally-used data; train, test, validation data. It’s not clear of the usage of them.
    > HC: re-stated them in section 3 and section 5.
- Assumption:
    - Re-think about the assumption that “users have mastered the knowledge contained in items previously interacted with”.
    - The prior and target knowledge states do not change during the remaining interaction sequence, which is not realistic.
    - After the proposed recommender is deployed, interactions corresponding to the target knowledge cannot be obtained in this manner. This is because we only have interaction data before the target time point in that situation.
- RW: The paper overlooks several lines of relevant research.
    - Sequential Recommendation. (the meaning of the prerequisite is unclear)
    - Progression modeling to capture the transitions of user interests
    - Knowledge tracing
- Approach
    - The details are not clear enough.
        - E.g., what is a fully-connected graph? How is it constructed? Is it different from the graph used for applying TextRank in the previous step? These may be explained in the literature [24], but I think the description should be self-contained as this part is a core process of the proposed solution.
        HC: accepted, but nore sure how to state it in a short space.
        - Wikipedia Reference Distance: It is unclear to me how the authors find Wikipedia concepts (equivalent to articles?) related to a given concept.
    - Annotation: How did you select 300 samples? Who annotated those samples? What are annotation criteria? How consistent were the annotations provided by different people?
    - The proposed method is less sophisticated
        - MLPs is rather naive compared to recent recommenders.
        - Is simply averaging knowledge embeddings sufficient for this task?
- Dataset
    - Consider using Knowledge Tracing Datasets?
- Evaluation
    - Negative sampling is reported to be a bad practice making evaluation results unreliable.
    - Consider adding sequential recommenders as baselines.
- Result
    - It’s not able to judge the accuracy of the statement “We also find that prerequisite graphs have different structural properties”.
    - Cold-start: It is unclear to me how the authors obtained the embeddings for users and items unseen in the training phase.
    > HC: it's randomly initialized. I think it is not necessary to state.

**Minor Concerns**

- How did we arrive at the 30% and 20% splitting ratios?
- Who annotated what and with what inter-rater agreement?
- It’s not transparent that how the two prerequisite metrics are considered in the model.
- Until reading Section 5.1, I did not understand what item documents are. (Re-structure this description.)
> HC: accepted, mentioned what documents is in section 3.
- KLP and KPL make confusion.
> HC: accepted.
- Equation 1: Is $\sigma$ the sigmoid function?
> HC: solved.
- Section 5.1, `500 labeled samples`: According to Section 3, there are 300 annotated samples.
> HC: solved. 300
- Section 6.1: There is a reference error.
> HC: solved
- Figure 4 is not referred in text.
> HC: solved