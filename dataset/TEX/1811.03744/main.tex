\documentclass[11pt]{article} 


\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{cancel}
\usepackage{fullpage}
\usepackage{liyang}
\usepackage{framed}
\usepackage{natbib}
\usepackage{verbatim}
\usepackage{enumitem}
\usepackage{array}
\usepackage{multirow}%
\usepackage{afterpage}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{setspace}
\usepackage{soul}

% \usepackage{subfigure}


\newcommand{\anote}[1]{\footnote{{\bf \color{green}Anindya:} {#1}}}
\newcommand{\pnote}[1]{\footnote{{\bf \color{purple}Phil}: {#1}}}

%\usepackage{times}
\usepackage[normalem]{ulem}


\usepackage{caption} 
%\usepackage{times}

% clashes (?) with COLT style
% \usepackage[usenames,dvipsnames]{xcolor}

\usepackage{todonotes}

\makeatletter
\newtheorem*{rep@theorem}{\rep@title}
\newcommand{\newreptheorem}[2]{
\newenvironment{rep#1}[1]{
 \def\rep@title{#2 \ref{##1}}
 \begin{rep@theorem}\itshape}
 {\end{rep@theorem}}}
\makeatother
% Latex was barfing, so I provisionally commented this out.
%
% \theoremstyle{plain}
% 
%   \newtheorem{thm}{Theorem}


\newcommand{\marginnote}[2][]{\todo[size=\scriptsize]{#2}}

\newcommand{\mnote}[1]{ \marginpar{\tiny\bf
            \begin{minipage}[t]{0.5in}
              \raggedright #1
           \end{minipage}}}



% for use with amsthm
% same as proof environment, but with definition-style proof head
% and named theorem.
%
% Depends on amsthm, which is disallowed by jmlr class.  Added a proofof
% to the jmlr class.
\makeatletter
\newenvironment{proofof}[1]{\par
  \pushQED{\qed}%
  \normalfont \topsep6\p@\@plus6\p@\relax
  \trivlist
  \item[\hskip\labelsep
\emph{    Proof of #1\@addpunct{.}}]\ignorespaces
}{%
  \popQED\endtrivlist\@endpefalse
}
\makeatother


\newcommand{\reda}[1]{{\color{red} {#1}}}


\def\verbose{0}


% \def\colorful{0}
\def\colorful{1}
\ifnum\colorful=1
\newcommand{\violet}[1]{{\color{violet}{#1}}}
\newcommand{\orange}[1]{{\color{orange}{#1}}}
\newcommand{\blue}[1]{{{#1}}}
\newcommand{\red}[1]{{\color{red} {#1}}}
\newcommand{\green}[1]{{\color{green} {#1}}}
\newcommand{\pink}[1]{{\color{pink}{#1}}}
\newcommand{\gray}[1]{{\color{gray}{#1}}}

\fi
\ifnum\colorful=0
\newcommand{\violet}[1]{{{#1}}}
\newcommand{\orange}[1]{{{#1}}}
\newcommand{\blue}[1]{{{#1}}}
\newcommand{\red}[1]{{{#1}}}
\newcommand{\green}[1]{{{#1}}}
\newcommand{\gray}[1]{{{#1}}}

\fi


\usepackage{boxedminipage}

%\newcommand{\ignore}[1]{}

\newtheorem{theorem}{Theorem}
\newtheorem*{theorem*}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}[theorem]{Definition}
\newtheorem*{noclaim*}{Claim}


\newtheorem{claim}[theorem]{Claim}
\newtheorem{fact}[theorem]{Fact}
\newtheorem{observation}[theorem]{Observation}


\newcommand{\ds}{\displaystyle}

\newcommand{\CSI}{{\mathcal C}_{\mathrm{SI}}}
\newcommand{\si}{{\mathrm{SI}}}
\renewcommand{\ss}{\subseteq}
\newcommand{\cube}{{\mathrm{cube}}}
\newcommand{\cF}{{\cal F}}
\newcommand{\cC}{{\cal C}}
\newcommand{\diag}{\mathrm{diag}}
\renewcommand{\bSigma}{\overline{\Sigma}}
\newcommand{\tSigma}{\tilde{\Sigma}}
\newcommand{\tmu}{\tilde{\mu}}
\newcommand{\tpsi}{\tilde{\psi}}
\newcommand{\bVar}{\overline{\Var}}

\newcommand{\uth}{{\huge{\red{UP TO HERE}}}}



% \title[Short Title]{Density estimation
%                    for shift-invariant multidimensional 
%                    distributions}
\title{Density estimation
                   for shift-invariant multidimensional 
                   distributions}
\usepackage{times}
 % Use \Name{Author Name} to specify the name.
 % If the surname contains spaces, enclose the surname
 % in braces, e.g. \Name{John {Smith Jones}} similarly
 % if the name has a "von" part, e.g \Name{Jane {de Winter}}.
 % If the first letter in the forenames is a diacritic
 % enclose the diacritic in braces, e.g. \Name{{\'E}louise Smith}

 % Two authors with the same address
  % \coltauthor{\Name{Author Name1} \Email{abc@sample.com}\and
  %  \Name{Author Name2} \Email{xyz@sample.com}\\
  %  \addr Address}

 % Three or more authors with the same address:
 % \coltauthor{\Name{Author Name1} \Email{an1@sample.com}\\
 %  \Name{Author Name2} \Email{an2@sample.com}\\
 %  \Name{Author Name3} \Email{an3@sample.com}\\
 %  \addr Address}


 % Authors with different addresses:

\author{
Anindya De\thanks{Supported by NSF grant CCF-1814706}\\
Northwestern University\\
{\tt anindya@eecs.northwestern.edu}
\and
Philip M. Long\\
Google\\
{\tt plong@google.com}
\and
\and Rocco A.~Servedio\thanks{Supported by NSF grants CCF-1319788 and CCF-1420349}\\
Columbia University \\
{\tt rocco@cs.columbia.edu}
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
We study density estimation for  classes of \emph{shift-invariant} distributions over $\mathbb{R}^d$.  A multidimensional distribution is ``shift-invariant'' if, roughly  speaking, it is close in total variation distance to a small shift of it in any direction.  
Shift-invariance relaxes smoothness assumptions commonly used in
non-parametric density estimation to allow jump discontinuities.  
The different classes of distributions that we consider correspond to different rates of tail decay.

For each such class we give an efficient algorithm that learns any distribution in the class from independent samples with respect to total variation distance.  As a special case of our general result, we show that $d$-dimensional shift-invariant distributions which satisfy an exponential tail bound can be learned to total variation distance error $\eps$ using $\tilde{O}_d(1/ \epsilon^{d+2})$ examples and $\tilde{O}_d(1/ \epsilon^{2d+2})$ time.  This implies that, for constant $d$, multivariate log-concave distributions can be learned in $\tilde{O}_d(1/\epsilon^{2d+2})$ time using $\tilde{O}_d(1/\epsilon^{d+2})$ samples, answering a question of  \citep{diakonikolas2016learning}. 
All of our results extend to a model of \emph{noise-tolerant} density estimation using Huber's contamination model, in which the target distribution to be learned is a $(1-\eps,\eps)$ mixture of some unknown distribution in the class with some other arbitrary and unknown distribution, and the learning algorithm must output a hypothesis distribution with total variation distance error $O(\eps)$ from the target distribution.  We show that our general results are close to best possible by proving a simple $\Omega\left(1/\epsilon^d\right)$ information-theoretic lower bound on sample complexity even for learning bounded distributions that are shift-invariant.
\end{abstract}

%\begin{keywords}
%Density estimation, unsupervised learning, log-concave distributions, non-parametrics.
%\end{keywords}


\input{intro}

\input{prelim}

\input{special-case}

\input{no-noise}

\input{with-noise}

\input{logconcave}

\input{lower}

% we already have citet's and citep's in the test
% \bibliographystyle{alpha}
\bibliographystyle{alpha}
\bibliography{allrefs}

\appendix

% Now, there is an  \input{fourier} command in prelim.tex.
%
% \input{fourier}





% Acknowledgments---Will not appear in anonymized version
%\acks{Anindya De is supported by a start-up grant from Northwestern University.
%Rocco Servedio is supported by NSF grants CCF-1420349 and CCF-1563155.}




\end{document}
