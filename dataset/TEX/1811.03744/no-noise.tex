% !TEX root =  main.tex

\section{Density estimation for densities in $\CSI(c,d,g)$}
\label{sec:no-noise}

Fix any nonincreasing tail bound function $g: \R^+ \to [0,1]$ which satisfies $\lim_{t \to +\infty} g(t) = 0$
and the condition $\min\{r \in \R: g(r) \leq 1/2\}\geq 1/10\}$ of Remark~\ref{remark:tail-weight}
and any constant $c \geq 1$.
In this section we prove the following theorem which gives a density estimation algorithm for the class of distributions
$\CSI(c,d,g)$:
\begin{theorem} \label{thm:no-noise}
For any $c,g$ as above and any $d \geq 1$,  there is an algorithm with the following property:  Let $f$ be any target density 
(unknown to the algorithm) which belongs to $\CSI(c,d,g)$. 
Given any error parameter $0 < \eps < 1/2$ and confidence parameter $\delta > 0$ and access to independent draws from $f$, the algorithm with probability $1-O(\delta)$ outputs a hypothesis  $h:[-1,1]^d \rightarrow \mathbb{R}^{\geq 0}$ such that $\int_{x \in \mathbb{R}^d} |f(x) - h(x)| \le O(\epsilon)$. 

The algorithm runs in time 

\ignore{
% OLD VERSION:
O\left(
  \left(
   (1 + g^{-1}(\epsilon))^{2d}
          \left( \frac{1}{\epsilon} \right)^{2 d + 2}
          \log^{4 d} \left( \frac{1 + g^{-1}(\epsilon)}{\epsilon} \right)
          \log \left( \frac{1 + g^{-1}(\epsilon)}{\epsilon \delta} \right) 
  + I_g \right) \log \frac{1}{\delta}
\right)
}
\[
O_{c,d}\left(
  \left(
   (g^{-1}(\epsilon))^{2d}
          \left( \frac{1}{\epsilon} \right)^{2 d + 2}
          \log^{4 d} \left( \frac{g^{-1}(\epsilon)}{\epsilon} \right)
          \log \left( \frac{g^{-1}(\epsilon)}{\epsilon \delta} \right) 
  + I_g \right) \log \frac{1}{\delta}
\right)
\] 
and uses 
\ignore{
% OLD VERSION:
O\left(
  \left(
   (1 + g^{-1}(\epsilon))^{2d}
          \left( \frac{1}{\epsilon} \right)^{2 d + 2}
          \log^{4 d} \left( \frac{1 + g^{-1}(\epsilon)}{\epsilon} \right)
          \log \left( \frac{1 + g^{-1}(\epsilon)}{\epsilon \delta} \right) 
  + I_g \right) \log \frac{1}{\delta}
\right)
}

\[
O_{c,d}\left(
 \left(
 (g^{-1}(\epsilon))^{d}
          \left( \frac{1}{\epsilon} \right)^{d+2}
          \log^{2d} \left( \frac{g^{-1}(\epsilon)}{\epsilon} \right)
          \log \left( \frac{g^{-1}(\epsilon)}{\epsilon \delta} \right)
 + I_g \right) \log \frac{1}{\delta}
 \right)
\] samples.  
\end{theorem}

\subsection{Outline of the proof}
Theorem~\ref{thm:no-noise} is proved by a reduction to Lemma~\ref{lem:finite-support}. The main ingredient in the proof of Theorem~\ref{thm:no-noise} is a ``transformation algorithm'' with the following property:  given as input access to i.i.d.\ draws from any density $f  \in\CSI(c,d,g)$, the algorithm constructs parameters which enable draws from the density $f$ to be transformed into draws from another density, which we denote $r$.  The density $r$ is obtained by
approximating $f$ after conditioning on a non-tail sample, and scaling the result so that it lies in
a ball of radius $1/2$.

Given such a transformation algorithm, the approach to learn $f$ is
clear: we first run the transformation algorithm to get access to
draws from the transformed distribution $r$.  We then use draws from
$r$ to run the algorithm of Lemma~\ref{lem:finite-support} to learn
$r$ to high accuracy.  (Intuitively, the
error relative to $f$ of the final hypothesis density is $O(\eps)$
because at most $O(\eps)$ comes from the conditioning and at most
$O(\eps)$ from the algorithm of Lemma~\ref{lem:finite-support}.) We
note that while this high-level approach is conceptually
straightforward, a number of technical complications arise; for
example, our transformation algorithm only succeeds with some
non-negligible probability, so we must run the above-described
combined procedure multiple times and perform hypothesis testing to
identify a successful final hypothesis from the resulting pool of
candidates.

The rest of this section is organized as follows: In
Section~\ref{sec:setup} we give various necessary technical
ingredients for our transformation algorithm.  We state and prove the
key results about the transformation algorithm in
Section~\ref{sec:transformation}, and we use the transformation
algorithm to prove Theorem~\ref{thm:no-noise} in
Section~\ref{sec:proof-of-no-noise}.


\subsection{Technical ingredients for the transformation algorithm} \label{sec:setup}


% \subsubsection{Conditioning approximately preserves shift-invariance}

As sketched earlier, our approach will work with a density 
obtained by conditioning $f \in \si(c,d)$ on lying in a certain ball that has mass close to 1 under $f$.\ignore{(intuitively, this corresponds to the condition used in the learning
algorithm of Section~3; recall that the densities learned by that
algorithm were assumed to lie in $B(1/2)$).}
While we know that the original density $f \in \si(c,d)$ has
good shift-invariance, we will further need the
conditioned distribution to also have good shift-invariance
in order for the \textsf{learn-bounded} algorithm of Section~\ref{sec:restricted} to work.  Thus we require the following
simple lemma, which shows that conditioning a density $f \in \si(c,d)$
on a region of large probability cannot hurt its shift invariance too much.

\begin{lemma}~\label{lem:restrict-si}
Let $f \in \si(c,d)$ and 
let $B$ be a ball such that $\Pr_{\bx \sim f} [\bx \in B] \ge 1- \delta$ where $\delta < 1/2$. 
If $f_B$ is the density of $f$ conditioned on $B$,
then,
% $f_B$ belongs to $\si(\tpsi,d)$ where 
% $\tpsi(\kappa) = \frac{4\delta}{\kappa} + 2 \psi(\kappa)$.   
for all $\kappa > 0$,
$\si(f_B,\kappa) \leq \frac{4\delta}{\kappa} + 2 c$.
\end{lemma}
\begin{proof}
Let $v$ be any unit vector in $\mathbb{R}^d$.\ignore{\footnote{spurious line?: Let $f_v$  (resp. $f_{B,v}$) be the densities obtained by projecting $f$ (resp. $f_B$) to direction $v$. }}
% It is easy to see that \begin{equation}~\label{eq:gamma-f-var} \mathsf{Var}[f_{B,v}] \le \frac{1}{1-\delta} \cdot \mathsf{Var}[f_v]. \end{equation}
Note that $f$ can be expressed as $(1-\delta)f_B$ + $\delta \cdot f_{err}$ where $f_{err}$ is some other density. As a consequence, for any $\kappa > 0$,
using the triangle inequality we have that
\begin{align*}
\int_x |f(x) - f(x + \kappa v )| dx &\ge (1-\delta) \int_x |f_B(x) - f_B(x + \kappa   v)| dx \\
& \hspace{0.5in} - \delta  \int_x |f_{err}(x) - f_{err}(x + \kappa   v)| dx.
\end{align*}
Since $f \in \calC_\si(c,d)$ the left hand side is at most  $c \kappa$, whereas the subtrahend  on the right hand side is trivially at most $2\delta$. Thus, we get
\begin{equation}~\label{eq:shift-B-bound-1}
\int_x |f_B(x) - f_B(x + \kappa  v)| dx  \leq \frac{2\delta}{1-\delta} + \frac{c \kappa}{1-\delta},
\end{equation}
completing the proof.
\end{proof}


% \subsubsection{Mean estimation}

If $f$ is an unknown target density then of course its mean 
is also unknown, and thus we will need to approximate it
using draws from $f$. 
To do this, it will be helpful to convert our condition on the tails
of $f$ to bound the variance of $|| \bx - \mu ||$, where $\bx \sim f.$
% The following
% definition will be useful for this.
% \begin{definition}
% For any function $g$ from $\R^+$ to $\R^+$
% such that $\int_0^{\infty} g(z) \; dz < \infty$, define
% $I_g = \int_0^{\infty} g(z) \; dz$.
% \end{definition}
\begin{lemma}
\label{l:var}
For any $f \in \CSI(c, d, g)$, we have
$
\E_{\bx \sim f} [ || \bx - \mu ||^2 ] \leq  I_g.
$
\end{lemma}
\begin{proof}
We have
$
\E_{\bx \sim f} [ || \bx - \mu ||^2 ]
 = \int_0^{\infty} \Pr_{\bx \sim f} [ || \bx - \mu ||^2 \geq z ] \; dz
 \leq \int_0^{\infty} g(\sqrt{z}) \; dz
 = I_g.
$
\end{proof}

The following easy proposition gives a guarantee on the quality of the empirical mean: 
\begin{lemma}~\label{lem:estimate-mean}
For any $f \in \CSI(c,d,g)$,
if $\mu \in \R^d$ is the mean of $f$ and $\hat{\bmu}$
is its empirical estimate based on $M$ samples, then 
for any $t>0$ we have
\[
\Pr\big[ || \mu-\hat{\bmu} ||^2 \ge t \big]   \le \frac{I_g}{M t}.
\]
\end{lemma}
\begin{proof} 
If $\bx_1, \ldots, \bx_M$ are independent draws from $f$, then 
\begin{eqnarray*}
\mathbf{E}[|| \mu  -\hat{\bmu} ||^2] 
 &=& \mathbf{E}\bigg[\big|\big|\mu  -\frac{\bx_1+ \ldots + \bx_M}{M}\big|\big|^2 \bigg] \\ 
 &=& \sum_{i=1}^M \frac{1}{M^2} 
  \mathbf{E}\bigg[\big|\big|\mu  - \bx_i \big|\big|^2 \bigg] 
%   \\ &=& 
  = \frac{I_g}{M},
\end{eqnarray*}
where the last inequality is by Lemma~\ref{l:var}.
Applying Markov's inequality on the left hand side, we get the stated claim. 
\end{proof}




\subsection{Transformation algorithm} \label{sec:transformation}


% The following essentially says that there is a procedure which takes
% $O(???)$ samples from a density $f \in \CSI(\psi,d,g)$ and
% succeeds with $???$ probability in producing a ball
% which captures $1-\epsilon$ of the mass of $f$.

\begin{lemma}~\label{lem:transformation}
There is an algorithm \textsf{compute-transformation} such that given
access to samples from $f \in \CSI(c,d,g)$ and an error parameter
$0 < \epsilon < 1/2$, the algorithm takes $O(I_g)$ samples from $f$ and with
probability at least $9/10$ produces a vector $\tilde{\mu} \in \mathbb{R}^d$
and a real number $t$ with the following properties:
\begin{enumerate}
\item For $B_{t} = \{x : || x-\tilde{\mu} ||  \le \sqrt{t} \}$, we have 
$\Pr_{\bx \sim f} [ \bx \in B_t ] \geq 1 - \epsilon$.
\item 
% $t = 2 ((g^{-1}(\epsilon))^2 + 1/10)$.
$t = O(g^{-1}(\epsilon)^2)$,
\item  
For all $\kappa > 0$, the density $f_{B_t}$ satisfies
$\si(f_{B_t},\kappa) \leq \frac{4\epsilon}{\kappa} + 2 c$.
% \rnote{Was ``$\si(f_{B_t},\kappa) \leq \frac{4\epsilon}{\kappa} + 2 c \red{\kappa}$''}
\end{enumerate}
\end{lemma}
\begin{proof}
For $M = 100 I_g$, the algorithm
\textsf{compute-transformation} simply works as follows: set
$\tilde{\mu}$ to be the empirical mean of the $M$ samples, and 
$t = 2 ((g^{-1}(\epsilon))^2 + 1/10)$. (Note that by Remark~\ref{remark:tail-weight} we have $t=\Theta(g^{-1}(\eps)^2).$).  Let $\mu$ denote the true mean of $f$.  First, 
by Lemma~\ref{lem:estimate-mean}, with
probability at least 0.9, the empirical
mean $\hat{\bmu}$ will be close to the true mean $\mu$ in the
following sense:
\begin{eqnarray}~\label{eq:estimate-mean}
|| \mu-\hat{\bmu} ||^2    \le \frac{1}{10}.
\end{eqnarray}
Let us assume for the rest of the proof that this happens; fix any such outcome and denote it $\tmu.$

We have 
\begin{eqnarray*}
|| x - \tmu ||^2
 \leq 2 (|| x - \mu ||^2 + || \mu - \tmu ||^2) 
%  \\
 \leq 2 (|| x - \mu ||^2 + 1/10) \\
\end{eqnarray*}
and so
\begin{align*}
& \Pr_{\bx \in f} [ || \bx - \tmu ||^2 > t ] 
%   \\
% & 
  \leq \Pr_{\bx \in f} [ 2 (|| \bx - \mu ||^2 + 1/10) > t ] 
%     \\
%    & 
 = \Pr[\|\bx - \mu\|^2 \geq g^{-1}(\eps)]
 \leq \epsilon.
\end{align*}

Applying Lemma~\ref{lem:restrict-si} completes the proof.
\end{proof}

The following proposition elaborates on the properties of the output
of the transformation algorithm.
 
\begin{lemma} \label{lem:properties}
Let $f \in \CSI(c,d,g)$, $\epsilon>0$, $\tilde{\mu} \in \mathbb{R}^d$,
and $t \in \mathbb{R}$ satisfy the properties stated in
Lemma~\ref{lem:transformation}. Consider the density
$f_{\mathrm{scond}}$ defined by
\begin{align*}
& f_{\mathrm{scaled}}(x) \eqdef 2\sqrt{t} \cdot f\big(2\sqrt{t} \cdot {( x+\tilde{\mu})}  \big) \\
& f_{\mathrm{scond}}(x) \eqdef f_{\mathrm{scaled}, B(1/2)}(x  ),
\end{align*}
where $f_{\mathrm{scaled}, B(1/2)}$ is the result of conditioning $f_{\mathrm{scaled}}$ on membership in $B(1/2)$.
Then the density $f_{\mathrm{scond}}(x)$ satisfies the following properties: 
\begin{enumerate}
\item The density $f_{\mathrm{scond}}$ is supported in the ball $B(1/2)$. 
\item For all $\epsilon < 1/2$
and $\kappa > 0$, the density $f_{\mathrm{scond}}$ 
satisfies
\[
\si(f_{\mathrm{scond}},\kappa) \leq \frac{4 \epsilon}{\kappa} + 4 c \sqrt{t}.\]
\end{enumerate}
\end{lemma}

\begin{proof}
First, it is easy to verify that function $f_{\mathrm{scond}}$ defined above is indeed a density. Item 1 
is enforced by fiat.
Now, for any direction $v$, we have
\begin{align*}
\si(f_{\mathrm{scaled}},v,\kappa) 
 & =
{\frac 1 \kappa} \cdot \sup_{\kappa' \in [0,\kappa]} \int_{\R^d} \left|f_{\mathrm{scaled}}(x+ \kappa'  v) - f_{\mathrm{scaled}}(x)\right| dx \\
 & =
{\frac{2\sqrt{t}}\kappa} \cdot \sup_{\kappa' \in [0,\kappa]} \int_{\R^d} \left|f(2 \sqrt{t} (x+ \kappa'  v)) - f(2 \sqrt{t} x)\right| dx. \\
\end{align*}
Using a change of variables, $u = 2 \sqrt{t} x$, we get
\begin{align}
\si(f_{\mathrm{scaled}},v,\kappa) 
 & = {\frac 1 \kappa} \cdot \sup_{\kappa' \in [0,\kappa]} \int_{\R^d} \left|f(u+ \kappa' 2 \sqrt{t}  v) - f(u)\right| du \nonumber \\
 & = {\frac 1 \kappa} \cdot \sup_{\kappa' \in [0,2 \sqrt{t} \kappa]} \int_{\R^d} \left|f(u+ \kappa'   v) - f(u)\right| du \nonumber \\
 & = 2 \sqrt{t} \cdot \si(f,v,2 \sqrt{t} \kappa) \le 2 c \sqrt{t}. \label{eq:a}
\end{align}
The last inequality uses that $f \in \CSI(c,d,g)$. Inequality (\ref{eq:a})  implies that $f_{\mathrm{scaled}} \in  \CSI(2c\sqrt{t},d,g)$.
Now, 
$\Pr_{\bx \sim f_{\mathrm{scaled}}}(\bx \in B(1/2)) = \Pr_{\bx \sim f}(\bx \in B_t) \geq 1 - \epsilon$, so
applying Lemma~\ref{lem:restrict-si} completes the proof.
\end{proof} 


\subsection{Proof of Theorem~\ref{thm:no-noise}} \label{sec:proof-of-no-noise}

We are now ready to prove Theorem~\ref{thm:no-noise}.  Consider the following algorithm, which we call 
\textsf{construct-candidates}:

\begin{enumerate}

\item Run the transformation algorithm \textsf{compute-transformation} $D := O (\ln(1/\delta))$ many times (with parameter $\eps$ each time).  Let $(\tilde{\mu}^{(i)},t)$ be the output that it produces on the $i$-th run, where $t=O(g^{-1}(\epsilon)^2)$. 
% (Observe that from the proof of Lemma~\ref{lem:transformation}, this is always the value of $t$ that is output by \textsf{compute-transformation}.)

\item For each $i \in [D]$,  let
$B_t^{(i)} = \{ x : || x - \tilde{\mu} || \leq \sqrt{t} \}$ and
$
f^{(i)}_{\mathrm{scond}}
$
be the density defined from $(\tilde{\mu}^{(i)},t)$ as in Lemma~\ref{lem:properties}.
\end{enumerate}

Before describing the third step of the algorithm, we observe that given the pair $(\tilde{\mu}^{(i)},t)$ it is easy to check whether any given $x \in \mathbb{R}^d$ belongs to $B_{t}^{(i)}$.\ignore{ Thus it is easy to efficiently obtain a $\pm \eps/2$-accurate estimate of $\Pr_{x \sim f}[x \in B_{t}]$ given draws from $f$; let $v^{(i)} \in [0,1]$ denote the estimate thus obtained.} We further make the following observations:

\begin{itemize}


\item If $\Pr_{\bx \sim f}[\bx \in B_{t}^{(i)}] \geq 1/2,$ then with probability at least $1/2$ a draw from $f$ can be used as a draw from $f_{B_{t}^{(i)}}$.  In this case, via rejection sampling, it is easy to very efficiently simulate draws from $f^{(i)}_{\mathrm{scond}}$ given access to samples from $f$ (the average slowdown is at most a factor of 2).  Note that if $(\tilde{\mu}^{(i)},t)$ satisfies the properties of Lemma~\ref{lem:transformation}, then $\Pr_{\bx \sim f}[\bx \in B_{t}^{(i)}] \geq 1-\eps$ and we fall into this case.

\item On the other hand, if $\Pr_{\bx \sim f}[\bx \in B_{t}^{(i)}] < 1/2,$ then it may be inefficient to simulate draws from $f^{(i)}_{\mathrm{scond}}.$  But any such $i$ will not satisfy the properties of Lemma~\ref{lem:transformation}, so if rejection sampling is inefficient to simulate draws from $f^{(i)}_{\mathrm{scond}}$ then we can ignore such an $i$ in what follows.

\end{itemize}

With this in mind, the third and fourth steps of the algorithm are as follows:
    
\begin{enumerate}

\item [3.] For each $i \in [D]$,\footnote{Actually, as described above, this and the fourth step are done only for those $i$ for which rejection sampling is not too inefficient in simulating draws from $f^{(i)}_{\mathrm{scond}}$ given draws from $f$; for the other $i$'s, the run of \textsf{learn-bounded} is terminated.} run the algorithm \textsf{learn-bounded} using $m$  samples from $f^{(i)}_{\mathrm{scond}}$, where $m=m(\eps,\delta,d)$ is the sample complexity of \textsf{learn-bounded} from Lemma~\ref{lem:finite-support}.
 Let ${h}_{\mathrm{scond}}^{(i)}$ be the resulting hypothesis that \textsf{learn-bounded} outputs.

\item [4.] Finally, for each $i \in [D]$ output the hypothesis obtained by 
inverting the mapping of Lemma~\ref{lem:properties}, i.e.
\begin{equation} \label{eq:hi}
{h}^{(i)}(x) \eqdef
{\frac 1 {2\sqrt{t}}} \cdot {h}_{\mathrm{scond}}^{(i)}\left(
{\frac 1 {2\sqrt{t}}} \cdot (x-\tilde{\mu}^{(i)})  \right).
\end{equation}
\end{enumerate} 

Thus the output of \textsf{construct-candidate} is a $D$-tuple of hypotheses $({h}^{(1)},
\dots, {h}^{(D)}).$ 

\medskip

We now analyze the \textsf{construct-candidate} algorithm.  Given Lemma~\ref{lem:transformation} and Lemma~\ref{lem:properties}, it is not difficult to show that with high probability at least one of the hypotheses that it outputs has error $O(\eps)$ with respect to $f$:

\begin{lemma} \label{lem:one-good}
With probability at least $1-O(\delta)$, at least one ${h}^{(i)}$ has $\int_x |{h}^{(i)}(x)-f(x)| dx \leq O(\eps).$
\end{lemma}
\begin{proof}
It is immediate from Lemma~\ref{lem:transformation} and the choice of $D$ that with probability $1-\delta$ at least one
triple $(\tilde{\mu}^{(i)},t)$ satisfies the properties of Lemma~\ref{lem:transformation}.  
 Fix $i'$ to be an $i$ for which this holds.

Given any $i \in [D]$, it is easy to carry out the check for whether rejection sampling is too inefficient in simulating $f^{(i)}_{\mathrm{scond}}$ in such a way that algorithm \textsf{learn-bounded} will indeed be run to completion (as opposed to being terminated) on $f^{(i')}_{\mathrm{scond}}$ with probability at least $1-\delta$, so we henceforth suppose that indeed \textsf{learn-bounded} is actually run to completion on $f^{(i')}_{\mathrm{scond}}$. 
%keen-eyed
Since $(\tilde{\mu}^{(i')},t)$ satisfies the properties of Lemma~\ref{lem:transformation}, by Lemma~\ref{lem:properties}, taking 
$\kappa = \min\{\eps{/2},\eps/(4 g^{-1}(\eps)c)\}$) the density $f^{(i')}_{\mathrm{scond}}$ satisfies  the required conditions for Lemma~\ref{lem:finite-support} to apply with that choice of $\kappa$.
The following simple proposition implies that ${h}^{(i)}$ is likewise $O(\eps)$-close to $f_{B_{t}}$:

\begin{proposition}~\label{prop:density-diff-transform}
Let $f$ and $g$ be two densities in $\mathbb{R}^d$ and let $x \mapsto A(x-z)$ be any invertible linear transformation over $\mathbb{R}^d$. Let $f_A(x) = \det(A) \cdot f(A(x-z))$ and $g_A(x) = \det(A) \cdot g(A(x-z))$ be the densities from $f$ and $g$ under this transformation.  Then $\dtv(f,g) = \dtv(f_A, g_A)$. 
\end{proposition}
\begin{proof}
\begin{align*}
\dtv(f_A,g_A) &= \int_{x} |f_A(x) - g_A(x)| dx = \int_x \det(A) |f(A(x-z)) - g(A(x-z)) | dx\\
& = \int_z |f(z) - g(z)| dz = \dtv(f,g), 
\end{align*}
where the penultimate equality follows by a linear transformation of variables. 
\end{proof}

It remains only to observe that by property 1 of Lemma~\ref{lem:transformation} the density $f_{B_{t}}$ is $\eps$-close to $f$, and then by the triangle inequality we have that ${h}^{(i)}$ is $O(\eps)$-close to $f$.  This gives Lemma~\ref{lem:one-good}.
\end{proof}

Tracing through the parameters, it is straightforward to verify that the sample and time complexities of \textsf{construct-candidates} are as claimed in the statement of Theorem~\ref{thm:no-noise}. These sample and time complexities dominate the sample and time complexities of the remaining portion of the algorithm, the hypothesis selection procedure discussed below.



All that is left is to identify a good hypothesis from the pool of $D$ candidates.   This can be carried out rather straightforwardly using well-known tools for hypothesis selection.  Many variants of the basic hypothesis selection procedure have appeared in the literature, see e.g.
\citep{Yatracos85,DaskalakisKamath14,AJOS14,DDS12stoc,DDS15}).  
The following is implicit in the proof of
Proposition 6 from \citep{DDS15}:

\begin{proposition} \label{prop:log-cover-size}
Let $\bD$ be a distribution
with support contained in a
set $W$
and let $\calD_\eps = \{ \bD_j\}_{j=1}^M$ be a collection of $M$ hypothesis distributions 
%\rnote{we have measures, not distributions}
over $W$ with the property that there exists $i \in [M]$ such that
$\dtv(\bD,\bD_i) \leq \eps$.
There is an algorithm~$\mathrm{Select}^{\bD}$ which is given
$\eps$ and a confidence parameter $\delta$, and is provided
with access to
(i) a source of i.i.d. draws from $\bD$ and from $\bD_i$, for all $i \in [M]$;
%\rnote{I guess we can only approximately sample from
%the distributions which correspond to our measures?} 
and
(ii) a $(1+\beta)$ ``approximate evaluation oracle'' $\eval_{\bD_i}(\beta)$,
for each $i \in [M]$, which, on input $w \in W$, deterministically outputs $\tilde{D}_i^{\beta}(w)$ such that 
the value $\frac{\bD_i(w)}{1+\beta} \le \tilde{D}_i^{\beta}(w) \le (1+\beta) \cdot \bD_i(w)$. 
Further, $(1+\beta)^2 \le (1+\epsilon/8)$. 
%\rnote{I guess we can only approximate this evaluation oracle?}
 The $\mathrm{Select}^{\bD}$ algorithm has the following behavior:
It makes
$m = O\left( (1/ \eps^{2}) \cdot (\log M + \log(1/\delta)) \right)
$ draws from $\bD$ and {from} each $\bD_i$, $i \in [M]$,
and $O(m)$ calls to each oracle $\eval_{\bD_i}$, $i \in [M]$.  It runs in time $\poly(m,M)$ (counting each call to an $\eval_{\bD_i}$ oracle and draw from a $\bD_i$ distribution as unit time),
and with probability $1-\delta$ it outputs an index $i^{\star} \in [M]$ that satisfies $\dtv(\bD,\bD_{i^{\star}}) \leq 6\eps.$
\end{proposition}

% \begin{remark}
% Note that Proposition~\ref{prop:log-cover-size} as stated in \citep{DDS15} is only applicable when the support of the distribution is over a finite set ${W}$.  
% In our setting the densities are supported on an uncountable domain and so per se, the above proposition is not applicable. However, the proof of Proposition~\ref{prop:log-cover-size} in \citep{DDS15} can easily be generalized to the case when the domain $W$ is uncountable assuming that we have access to a $(1+\beta)$ ``approximate evaluation oracle" which on input $w$ outputs the density function at $w$ up to a multiplicative error $(1+\beta)$.  (Indeed, note that the complexity of the algorithm does not depend on the size of the domain $W$.) We do not dwell on this issue further and use the variant of Proposition~\ref{prop:log-cover-size}, implicit in the analysis of \citet{DDS15}, that does not require the domain $W$ to be finite.\pnote{Can I suggest that, in place of
% the above lemma and this remark, we instead modify the lemma
% to remove the requirement that $W$ is finite, and then add a
% comment above saying that this is implicit in the proof of the 
% earlier proposition?  \red{Rocco:}  This is fine with me.}
% \end{remark}

As suggested above, the remaining step is to apply Proposition~\ref{prop:log-cover-size} to the list of candidate hypothesis ${h}^{(i)}$ which satisfies the guarantee of Lemma~\ref{lem:one-good}. However, to bound the sample and time complexity of running the procedure Proposition~\ref{prop:log-cover-size}, we need to bound the complexity  both of sampling from $\{{h}^{(i)}\}_{i \in [D]}$ as well as of constructing approximate evaluation oracles for these measures.\footnote{Note that while ${h}^{(i)}$ are forced to be non-negative and thus can be seen as measures, they need not integrate to $1$ and thus need not be densities.}
In fact, we will first construct densities out of the measures $\{{h}^{(i)}\}_{i \in [D]}$ and show how to both efficiently sample from these measures as well as construct approximate evaluation oracles for these densities. 

Towards this, let us now define $H_{\max}$ as follows: $H_{\max} = \max_{i \in [D]} \max_{z \in [-1,1]^n} {h}_{\mathrm{scond}}^{(i)}(z)$.  From Lemma~\ref{lem:finite-support} (recall that Lemma~\ref{lem:finite-support} was applied with $\kappa = \min\{\eps{/2},\eps/(4g^{-1}(\eps)c)\}$) we get that
\begin{equation}~\label{eq:H-max-val}H_{\max}=
%   O\left( {\frac {\log^{2d} (1/\eps)}{\eps^d}} \right).
  O_{c,d}\left( \left( {\frac {g^{-1}(\eps)}{\eps}} \right)^d \log^{2d} \frac{g^{-1}(\eps)}{\eps} \right).
\end{equation}
We will carry out the rest of our calculations in terms of $H_{\max}$.

\begin{observation}~\label{obs:h-compute}
For any $i \in [D]$, $\int_{x \in [-1,1]^d} {h}_{\mathrm{scond}}^{(i)} (x) dx$ can be 
estimated to additive accuracy $\pm \epsilon$  and confidence $1-\delta$  in time $O_d\left( \frac{H_{\max}^2}{\epsilon^2} \cdot \log (1/\delta)\right)$. 
\end{observation}
\begin{proof}
First note that it suffices to estimate
the quantity  $\mathbf{E}_{x \in [-1,1]^d} 
[{h}_{\mathrm{scond}}^{(i)} (x)]$ to additive error $\epsilon/2^d$. However, this can be estimated using the trivial random sampling algorithm. In particular,  as 
${h}_{\mathrm{scond}}^{(i)}(x) \in [0,H_{\max}]$, the variance of the simple unbiased estimator for $\mathbf{E}_{x \in [-1,1]^d} 
[{h}_{\mathrm{scond}}^{(i)} (x)]$ is also bounded by $H_{\max}^2$. This finishes the proof.  
\end{proof}

Note that, while the algorithm of Observation~\ref{obs:h-compute} does random sampling, this sampling
is not from $f$, so it adds nothing to the sample complexity of the learning algorithm.

Next, for $i \in [D]$, let us define the quantity $Z_i$ to be $Z_i = \int_{x\ignore{\in [-1,1]^d}} {h}^{(i)}(x) dx$. Since the functions ${h}^{(i)}$  and ${h}_{\mathrm{scond}}^{(i)}$ are obtained from each other by linear transformations (recall (\ref{eq:hi})),
we get that that 
\[
2 \sqrt{t} Z_i = \int_{x\ignore{\in [-1,1]^d}} {h}_{\mathrm{scond}}^{(i)}\left({\frac 1 {2\sqrt{t}}} \cdot (x-\tilde{\mu}^{(i)})\right) dx. 
\]
We now define the functions ${H}^{(i)}$ and ${H}_{\mathrm{scond}}^{(i)}$ as
\[
{H}^{(i)} (x) = \frac{{h}^{(i)}(x)}{Z_i} \ \ \textrm{and} \  \ {H}_{\mathrm{scond}}^{(i)}(x) = \frac{{h}_{\mathrm{scond}}^{(i)}(
{\frac 1 {2\sqrt{t}}} \cdot (x-\tilde{\mu}^{(i)}))}{Z_i} \cdot {\frac 1 {2\sqrt{t}}} .
\]
Observe that the functions ${H}^{(i)}$ and ${H}_{\mathrm{scond}}^{(i)}$ are densities (i.e. they are non-negative and integrate to $1$). 
First, we will show that it suffices to run the procedure $\mathrm{Select}^{\bD}$ on the densities ${H}^{(i)}$. 
To see this, note that  Lemma~\ref{lem:one-good} says that there exists $i \in [D]$ such that $h^{(i)}$ satisfies $\int_x |h^{(i)}(x) - f(x)| = O(\epsilon)$. For such an $i$, $Z_i \in [1-O(\epsilon), 1+ O(\epsilon)]$. 
\ignore{\pnote{The above change regarding what we know about $Z_i$ propagates -- the propagated changes are not marked in blue.}}
Thus, we have the following corollary. 
\begin{corollary}~\label{corr:one-good}
With probability at least $1-\delta$, at least one ${H}^{(i)}$ satisfies $\int_x |{H}^{(i)}(x) - f(x)| = O(\epsilon)$. Further, for such an $i$, $Z_i \in [1-O(\epsilon), 1+O(\epsilon)]$. 
\end{corollary}
Thus, it suffices to run the procedure $\mathrm{Select}^{\bD}$ on the candidate distributions $\{{H}^{(i)}\}_{i \in [D]}$. The next proposition shows that 
the densities $\{  {H}^{(i)}\}_{i \in [D]}$ are samplable. 



\begin{proposition}~\label{prop:samplable}
A draw from the density ${H}^{(i)}(x)$ can be sampled in time
$O(H_{\max}/Z_{i})$.
\end{proposition}
\begin{proof}
First of all, note that it suffices to sample from ${H}_{\mathrm{scond}}^{(i)}$ since $H^{(i)}$ and $H^{(i)}_{\mathrm{scond}}$ are linear transformations of each other. However, sampling from ${H}_{\mathrm{scond}}^{(i)}$  is easy using rejection sampling. More precisely, the  distribution ${H}_{\mathrm{scond}}^{(i)}$ is supported on $[-1,1]^d$. We sample from ${H}_{\mathrm{scond}}^{(i)}$ as follows: 
\begin{enumerate}
\item Let $C = [-1,1]^d \times [0, H_{\max}]$. Sample a  uniformly random point $z'= (z_1, \ldots, z_{d+1})$ from $C$. 
\item If $z_{d+1} \le {h}_{\mathrm{scond}}^{(i)}(z_1, \ldots, z_d)$, then return the point $z=(z_1, \ldots, z_d)$. 
\item Else go to Step 1 and repeat. 
\end{enumerate}
Now note that conditioned on returning a point  in step $2$, the point $z$ is returned with probability proportional to ${h}_{\mathrm{scond}}^{(i)}(z)$. Thus, the distribution sampled by this procedure is indeed ${H}_{\mathrm{scond}}^{(i)}(z)$. To bound the probability of success, note that the total volume of $C$ is $2^{d} \times H_{\max}$. On the other hand, step $2$ is successful only if $z'$ falls in a region of volume $Z_{i}$. This finishes the proof.  
\end{proof}
The next proposition says that if $Z_i \ge 1/2$, then there is an approximate evaluation oracle for the density ${H}^{(i)}$. 
\begin{proposition}~\label{prop:density-compute}
Suppose $Z_i \ge 1/2$. Then there is a $(1+O(\epsilon))$- approximate evaluation oracle for ${H}^{(i)}$ which can be computed at any point $w$ in time $O\left( \frac{H_{\max}^2 }{\epsilon^2} \right)$.
\end{proposition}
\begin{proof}
Note that we can evaluate ${h}^{(i)}$ at any point $w$ exactly and thus the only issue is to estimate the normalizing factor $Z_i$. Note that since $Z_i \ge 1/2$ , estimating $Z_i$ to within an additive $O(\epsilon)$ gives us a $(1+O(\epsilon))$ multiplicative approximation to $Z_i$ and hence to ${H}^{(i)}(w)$ at any point $w$. However, by Observation~\ref{obs:h-compute}, this takes time $O \left( \frac{H_{\max}^2 }{\epsilon^2} \right)$, concluding the proof. 
\end{proof}

We now apply Proposition~\ref{prop:log-cover-size} as follows. 
\begin{enumerate}
\item For all $i \in [D]$, estimate $Z_i$ using Observation~\ref{obs:h-compute} up to an additive error $\epsilon$. Let the estimates be $\hat{Z}_i$. 
\item Let us define $S_{\mathrm{feas}} = \{i \in [D]: \hat{L}_i \ge 1/2\}$. 
\item We run the routine \textsf{Select}$^{\mathbf{D}}$ on the densities $\{{H}^{(i)}\}_{i \in S_{\mathrm{feas}}}$. 
 To sample from a density ${H}^{(i)}$, we use Proposition~\ref{prop:samplable}. We also construct a $\beta = \epsilon/32$ approximation oracle
for each of the densities ${H}^{(i)}$ using Proposition~\ref{prop:density-compute}. Return the output of  \textsf{Select}$^{\mathbf{D}}$. 
\end{enumerate}
The correctness of the procedure follows quite easily. 
Namely, note that Corollary~\ref{corr:one-good} implies that there is one $i$ such that both $Z_i \in [1-O(\epsilon), 1+O(\epsilon)]$ and $\int_x |{H}^{(i)}(x) - f(x)| = O(\epsilon)$. Thus such an $i$ will be in $S_{\mathrm{feas}}$. Thus, by the guarantee of \textsf{Select}$^{\mathbf{D}}$, the output hypothesis is $O(\epsilon)$ close to $f$. 

We now bound the sample complexity and time complexity of this hypothesis selection portion of the algorithm. First of all, the number of samples required from $f$ for running  \textsf{Select}$^{\mathbf{D}}$ is  $O((1/\epsilon^2) \cdot (\log (1/\delta)  +d^2 \log d + \log \log (1/\delta)) = O((1/\epsilon^2) \cdot (\log (1/\delta)  +d^2 \log d )$. This is clearly dominated by the sample complexity of the previous parts. To bound the time complexity, note that the time complexity of invoking the sampling oracle for any ${H}^{(i)}$  ($ i \in S_{\mathrm{feas}}$) is dominated by the time complexity of the approximate oracle which is $2^{O(d)} \cdot H_{\max}^2/\epsilon^2$.  
The total number of calls to the sampling as well as evaluation oracle is upper bounded by $\frac{1}{\epsilon^2} (D \log D + D \log (1/\delta))$. Plugging in the value of $H_{\max}$ as well as $D$, we see that the total time complexity is dominated by the bound in the statement of Theorem~\ref{thm:no-noise}. This finishes the proof. 














