% !TEX root =  main.tex

\section{Noise-tolerant density estimation for $\CSI(c,d,g)$}
\label{sec:semi-agnostic}

Fix any nonincreasing tail bound function $g: \R^+ \to [0,1]$ which satisfies $\lim_{t \to +\infty} g(t) = 0$
and the condition of Remark~\ref{remark:tail-weight}
and any constant $c \geq 1$.
In this section we prove  our main result, Theorem~\ref{thm:main-semiagnostic}, which gives a noise tolerant density estimation algorithm for
$\CSI(c,d,g)$. We first recall the precise model of noise we consider in this paper. 
\begin{definition}~\label{def:corruption}
For two densities $f$ and $f' \in \mathbb{R}^d$, we say that $f'$ is an $\epsilon$-corruption of $f$ if $f'$ can be expressed as $f' = (1-\epsilon) \cdot f + \epsilon \cdot f_{err}$ where $f_{err}$ is a density in $\mathbb{R}^d$. 
\end{definition}
This model of noise is sometimes referred to as \emph{Huber's contamination model}~\citep{huber1967behavior}. 


\begin{theorem} [Noise-tolerant density estimation for $\CSI(c,d,g)$.] \label{thm:main-semiagnostic}
For any $c,g$ as above and any $d \geq 1$,  there is an algorithm with the following property:  Let $f$ be any density 
(unknown to the algorithm) which belongs to $\CSI(c,d,g)$ and let $f'$ be an $\epsilon$-corruption of $f$. 
Given $\eps$ and any confidence parameter $\delta > 0$ and access to independent draws from $f'$, the algorithm with probability $1-O(\delta)$ outputs a hypothesis  ${h}:[-1,1]^d \rightarrow \mathbb{R}^{\geq 0}$ such that $\int_{x \in \mathbb{R}^d} |f'(x) - {h}(x)| \le O(\epsilon)$. 

The algorithm runs in time 
\ignore{
%WAS PREVIOUSLY:
\[
O\left( 
 \exp(O(I_g)) \cdot
  \left(
   (1 + g^{-1}(\epsilon))^{2d}
          \left( \frac{1}{\epsilon} \right)^{2 d + 2}
          \log^{4 d} \left( \frac{1 + g^{-1}(\epsilon)}{\epsilon} \right)
          \log \left( \frac{1 + g^{-1}(\epsilon)}{\epsilon \delta} \right) 
  + I_g \right) \log \frac{1}{\delta}
\right)
\] 
}
\[
O_{c,d}\left( 
 \exp(O(I_g)) \cdot
  \left(
   (g^{-1}(\epsilon))^{2d}
          \left( \frac{1}{\epsilon} \right)^{2 d + 2}
          \log^{4 d} \left( \frac{ g^{-1}(\epsilon)}{\epsilon} \right)
          \log \left( \frac{ g^{-1}(\epsilon)}{\epsilon \delta} \right) 
  + I_g \right) \log \frac{1}{\delta}
\right)
\] 
and uses 
\ignore{
% WAS PREVIOUSLY:
\[
O \left(
 \exp(O(I_g)) \cdot
 \left(
 (1 + g^{-1}(\epsilon))^{d}
          \left( \frac{1}{\epsilon} \right)^{d+2}
          \log^{2d} \left( \frac{1 + g^{-1}(\epsilon)}{\epsilon} \right)
          \log \left( \frac{1 + g^{-1}(\epsilon)}{\epsilon \delta} \right)
 + I_g \right) \log \frac{1}{\delta}
  \right)
\]
}
\[
O_{c,d} \left(
 \exp(O(I_g)) \cdot
 \left(
 ( g^{-1}(\epsilon))^{d}
          \left( \frac{1}{\epsilon} \right)^{d+2}
          \log^{2d} \left( \frac{ g^{-1}(\epsilon)}{\epsilon} \right)
          \log \left( \frac{ g^{-1}(\epsilon)}{\epsilon \delta} \right)
 + I_g \right) \log \frac{1}{\delta}
  \right)
\] 
samples.  
\end{theorem}


Theorem~\ref{thm:main-semiagnostic} is identical to Theorem~\ref{thm:no-noise} except that now the target density from which draws are received is $f'$, which is an $\eps$-corruption of $f \in \CSI(c,d,g)$, rather than $f$ itself,
and the dependence on $I_g$ is exponential.   
(On the other hand, if $f$ is isotropic,
then recalling Lemma~\ref{l:var} the function $g$ can be taken to be such that $I_g = \E_{\bx \sim f} [ || \bx - \mu ||^2 ] = d$, so that 
$\exp(O(I_g)) = O(1)$ and the complexity is
the same as in the noise-free case.)
This requires essentially no changes in the algorithm and  fortunately most of the analysis from earlier can also be reused in a fairly black-box way. We briefly  explain how the analysis of Section~\ref{sec:no-noise} can be augmented to handle having access to draws from $f'$ rather than $f$. 


\begin{proofof}{Theorem~\ref{thm:main-semiagnostic}}
Without loss of generality, we can assume that $\epsilon \leq 1/10$, as otherwise any hypothesis is trivially $O(\eps)$-close to the target density. 
Recall that $f'$ can be expressed as $f' = (1-\epsilon) \cdot f + \epsilon \cdot f_{err}$ for some density $f_{err} \in \mathbb{R}^d$.  Since $\eps \le 1/10$, this means that with probability $9/10$, a random sample from $f'$ is in fact distributed exactly as a random sample from $f$. We now revisit the steps in the algorithm \textsf{construct-candidates} (Proof of Theorem~\ref{thm:no-noise}) and briefly sketch why sample access to $f'$ instead of $f$ suffices.


\textbf{Step 1:} Note that each invocation of the algorithm \textsf{compute-transformation} is supposed to draw $O(I_g)$ samples from $f$.  We have access to $f'$ rather than $f$, but since each sample from $f'$, with probability at least $9/10$, is a sample from $f$,  with probability at least $\exp(-O(I_g))$, a run of \textsf{compute-transformation} with samples from $f'$ is the same as a run with samples from $f$.   So now in Step~1, the algorithm \textsf{compute-transformation} is run $\exp(-O(I_g)) \cdot \ln(1/\delta))$ many times rather than $O(\ln(1/\delta))$ many times as in the original version (this accounts for the additional $\exp(O(I_g))$ factor in the bounds of Theorem~\ref{thm:main-semiagnostic} versus Theorem~\ref{thm:no-noise}).

\textbf{Step 2:} This goes exactly as before with no changes. In particular, for every $i$, if ${f}_{\mathrm{trans}}^{(i)}$ is the true density obtained by the $i^{th}$ transformation, then we have sample access to ${f'}_{\mathrm{trans}}^{(i)}$  which is an $\epsilon$-corruption of ${f}_{\mathrm{trans}}^{(i)}$. 

\textbf{Step 3:} In Step $3$, we run the routine \textsf{learn-bounded} as usual. In particular, let us assume that ${f}_{\mathrm{trans}}^{(i)}$ satisfies the  conditions of Lemma~\ref{lem:finite-support}. Then Corollary~\ref{corr:agnostic}, which established noise-tolerance of \textsf{learn-bounded}, implies that with sample access to ${f'}_{\mathrm{trans}}^{(i)}$, the resulting hypothesis ${h'}_{\mathrm{trans}}^{(i)}$ is $2\epsilon$-close to ${f}_{\mathrm{trans}}^{(i)}$. 

It is easy to verify that Step 4 and the subsequent steps in hypothesis testing can go on exactly as before with sample access to $f'$ instead of $f$. In particular, the hypothesis testing routine $\mathrm{Select}^D$ will output a hypothesis ${h}^{(i)}$ which is $2\epsilon$ close to $f$. 
\end{proofof}
