\begin{thebibliography}{}

\bibitem[Abadi et~al., 2016]{abadi2016tensorflow}
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M.,
  Ghemawat, S., Irving, G., Isard, M., et~al. (2016).
\newblock Tensorflow: A system for large-scale machine learning.
\newblock In {\em 12th $\{$USENIX$\}$ Symposium on Operating Systems Design and
  Implementation ($\{$OSDI$\}$ 16)}, pages 265--283.

\bibitem[Adebayo et~al., 2018]{Adebayo18}
Adebayo, J., Gilmer, J., Muelly, M., Goodfellow, I.~J., Hardt, M., and Kim, B.
  (2018).
\newblock Sanity checks for saliency maps.
\newblock In {\em NeurIPS}.

\bibitem[Alvarez-Melis and Jaakkola, 2018]{Alvarez18}
Alvarez-Melis, D. and Jaakkola, T.~S. (2018).
\newblock Towards robust interpretability with self-explaining neural networks.
\newblock In {\em NeurIPS}.

\bibitem[Ancona et~al., 2018]{Ancona18}
Ancona, M.~B., Ceolini, E., Oztireli, C., and Gross, M.~H. (2018).
\newblock Towards better understanding of gradient-based attribution methods
  for deep neural networks.
\newblock In {\em ICLR}.

\bibitem[Baehrens et~al., 2010]{Baehrens10}
Baehrens, D., Schroeter, T., Harmeling, S., Kawanabe, M., Hansen, K., and
  M{\"u}ller, K.-R. (2010).
\newblock How to explain individual classification decisions.
\newblock {\em Journal of Machine Learning Research}, 11:1803--1831.

\bibitem[Doshi-Velez and Kim, 2017]{Doshi17}
Doshi-Velez, F. and Kim, B. (2017).
\newblock Towards a rigorous science of interpretable machine learning.
\newblock {\em arXiv preprint arXiv:1702.08608}.

\bibitem[Engstrom et~al., 2019]{Engstrom19}
Engstrom, L., Ilyas, A., Santurkar, S., Tsipras, D., Tran, B., and Madry, A.
  (2019).
\newblock Learning perceptually-aligned representations via adversarial
  robustness.
\newblock {\em arXiv preprint arXiv:1906.00945}.

\bibitem[Erhan et~al., 2009]{Erhan09}
Erhan, D., Bengio, Y., Courville, A.~C., and Vincent, P. (2009).
\newblock Visualizing higher-layer features of a deep network.

\bibitem[Fong and Vedaldi, 2017]{Fong17}
Fong, R. and Vedaldi, A. (2017).
\newblock Interpretable explanations of black boxes by meaningful perturbation.

\bibitem[Ghorbani et~al., 2018]{Ghorbani18}
Ghorbani, A., Abid, A., and Zou, J.~Y. (2018).
\newblock Interpretation of neural networks is fragile.
\newblock {\em CoRR}, abs/1710.10547.

\bibitem[He et~al., 2016]{he2016deep}
He, K., Zhang, X., Ren, S., and Sun, J. (2016).
\newblock Deep residual learning for image recognition.
\newblock In {\em Proceedings of the IEEE conference on computer vision and
  pattern recognition}, pages 770--778.

\bibitem[Heo et~al., 2019]{Heo19}
Heo, J., Joo, S., and Moon, T. (2019).
\newblock Fooling neural network interpretations via adversarial model
  manipulation.
\newblock {\em arXiv preprint arXiv:1902.02041}.

\bibitem[Hooker et~al., 2018]{Hooker18}
Hooker, S., Erhan, D., Kindermans, P., and Kim, B. (2018).
\newblock Evaluating feature importance estimates.
\newblock {\em CoRR}, abs/1806.10758.

\bibitem[Jacobsen et~al., 2018]{Jacobsen18}
Jacobsen, J.-H., Behrmann, J., Zemel, R., and Bethge, M. (2018).
\newblock Excessive invariance causes adversarial vulnerability.
\newblock {\em arXiv preprint arXiv:1811.00401}.

\bibitem[Kim et~al., 2018]{Kim18}
Kim, B., Wattenberg, M., Gilmer, J., Cai, C.~J., Wexler, J., Vi{\'e}gas, F.~B.,
  and Sayres, R. (2018).
\newblock Interpretability beyond feature attribution: Quantitative testing
  with concept activation vectors (tcav).
\newblock In {\em ICML}.

\bibitem[Lage et~al., 2018]{Lage18}
Lage, I., Ross, A., Gershman, S.~J., Kim, B., and Doshi-Velez, F. (2018).
\newblock Human-in-the-loop interpretability prior.
\newblock In {\em Advances in Neural Information Processing Systems}, pages
  10159--10168.

\bibitem[Lakkaraju et~al., 2016]{Lakkaraju16}
Lakkaraju, H., Bach, S.~H., and Leskovec, J. (2016).
\newblock Interpretable decision sets: A joint framework for description and
  prediction.
\newblock In {\em Proceedings of the 22nd ACM SIGKDD international conference
  on knowledge discovery and data mining}, pages 1675--1684. ACM.

\bibitem[Lin et~al., 2014]{Lin14}
Lin, T., Maire, M., Belongie, S.~J., Bourdev, L.~D., Girshick, R.~B., Hays, J.,
  Perona, P., Ramanan, D., Doll{\'{a}}r, P., and Zitnick, C.~L. (2014).
\newblock Microsoft {COCO:} common objects in context.
\newblock {\em CoRR}, abs/1405.0312.

\bibitem[Narayanan et~al., 2018]{Narayanan18}
Narayanan, M., Chen, E., He, J., Kim, B., Gershman, S., and Doshi-Velez, F.
  (2018).
\newblock How do humans understand explanations from machine learning systems?
  an evaluation of the human-interpretability of explanation.
\newblock {\em arXiv preprint arXiv:1802.00682}.

\bibitem[Nie et~al., 2018]{Nie18}
Nie, W., Zhang, Y., and Patel, A. (2018).
\newblock A theoretical explanation for perplexing behaviors of
  backpropagation-based visualizations.
\newblock In {\em ICML}.

\bibitem[Poursabzi-Sangdeh et~al., 2018]{Poursabzi18}
Poursabzi-Sangdeh, F., Goldstein, D.~G., Hofman, J.~M., Vaughan, J.~W., and
  Wallach, H. (2018).
\newblock Manipulating and measuring model interpretability.
\newblock {\em arXiv preprint arXiv:1802.07810}.

\bibitem[Ribeiro et~al., 2016]{Ribeiro16}
Ribeiro, M.~T., Singh, S., and Guestrin, C. (2016).
\newblock Why should i trust you?: Explaining the predictions of any
  classifier.
\newblock In {\em Proceedings of the 22nd ACM SIGKDD international conference
  on knowledge discovery and data mining}, pages 1135--1144. ACM.

\bibitem[Russakovsky et~al., 2015]{Russakovsky15}
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
  Karpathy, A., Bernstein, M.~S., Fei-Fei, L., Berg, A.~C., and Khosla, A.
  (2015).
\newblock Imagenet large scale visual recognition challenge.
\newblock {\em Springer US}.

\bibitem[Samek et~al., 2017]{Samek17}
Samek, W., Binder, A., Montavon, G., Lapuschkin, S., and MÃ¼ller, K.-R. (2017).
\newblock Evaluating the visualization of what a deep neural network has
  learned.
\newblock {\em IEEE Transactions on Neural Networks and Learning Systems},
  28:2660--2673.

\bibitem[Selvaraju et~al., 2016]{Selvaraju16}
Selvaraju, R.~R., Das, A., Vedantam, R., Cogswell, M., Parikh, D., and Batra,
  D. (2016).
\newblock Grad-cam: Why did you say that?
\newblock {\em CoRR}, abs/1611.07450.

\bibitem[Simonyan et~al., 2013]{Simonyan13}
Simonyan, K., Vedaldi, A., and Zisserman, A. (2013).
\newblock Deep inside convolutional networks: Visualising image classification
  models and saliency maps.
\newblock {\em CoRR}, abs/1312.6034.

\bibitem[Smilkov et~al., 2017]{Smilkov17}
Smilkov, D., Thorat, N., Kim, B., Vi{\'e}gas, F.~B., and Wattenberg, M. (2017).
\newblock Smoothgrad: removing noise by adding noise.
\newblock {\em CoRR}, abs/1706.03825.

\bibitem[Springenberg et~al., 2014]{Springenberg14}
Springenberg, J.~T., Dosovitskiy, A., Brox, T., and Riedmiller, M. (2014).
\newblock Striving for simplicity: The all convolutional net.
\newblock {\em arXiv preprint arXiv:1412.6806}.

\bibitem[Sundararajan et~al., 2017]{Sundararajan17}
Sundararajan, M., Taly, A., and Yan, Q. (2017).
\newblock Axiomatic attribution for deep networks.
\newblock In {\em ICML}.

\bibitem[Zeiler and Fergus, 2014]{zeiler2014visualizing}
Zeiler, M.~D. and Fergus, R. (2014).
\newblock Visualizing and understanding convolutional networks.
\newblock In {\em European conference on computer vision}, pages 818--833.
  Springer.

\bibitem[Zhou et~al., 2017]{Zhou17}
Zhou, B., Lapedriza, A., Khosla, A., Oliva, A., and Torralba, A. (2017).
\newblock Places: A 10 million image database for scene recognition.
\newblock {\em IEEE Transactions on Pattern Analysis and Machine Intelligence}.

\end{thebibliography}
