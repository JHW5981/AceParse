{
    "items": [
        "\\begin{enumerate}\n  \\item $\\phi$ is satisfiable iff $\\neg \\phi$ is not valid.\n  \\item $\\phi$ is valid iff $\\neg \\phi$ is unsatisfiable.\n\\end{enumerate}",
        "\\begin{itemize}\n    \\item In $q_1$, invert the currently read symbol (e.g., from $a$ to $b$), transition to state $q_2$ and move right.\n    \\item In $q_2$, preserve the currently read symbol (e.g., $a$ stays $a$), transition to state $q_3$ and move left.\n    \\item In $q_3$, non-deterministically either:\n    \\begin{itemize}\n        \\item Preserve the currently read symbol, transition to state $q_1$ and move right.\n        \\item Invert the currently read symbol, stay in $q_3$ and move left.\n    \\end{itemize}",
        "\\begin{enumerate}[label=\\textbf{Case \\arabic*:}, leftmargin=4em]\n    \\item \\textbf{$\\mathcal{C}$ has 1 literal:} let $\\mathcal{C}=a$ where $a$ is a literal. We introduce two new variables $z_1$ and $z_2$ to replace $\\mathcal{C}$ by the following four 3-CNF clauses:\n    {\\centering\n        $(a\\lor z_1 \\lor z_2), (a\\lor \\neg z_1 \\lor z_2), (a\\lor z_1 \\lor \\neg z_2), (a\\lor \\neg z_1 \\lor \\neg z_2)$ \\\\\n    }\n    Note that the conjunction of all four clauses should resolve to $(a)$ since any truth assignment of $z_1$ and $z_2$ will satisfy exactly three of the four clauses leaving only one where the two disjunct $z$-literals evaluate to \\textit{false} which yields $(a)$.\n    \\item \\textbf{$\\mathcal{C}$ has 2 literals:} let $\\mathcal{C}=(a_1 \\lor a_2)$. We introduce a new variable $z$ to replace $\\mathcal{C}$ by the following two 3-CNF clauses:\n    {\\centering\n        $(a_1 \\lor a_2 \\lor z), (a_1 \\lor a_2 \\lor \\neg z)$ \\\\\n    }\n    Similarly, the conjunction of both clauses should resolve to $(a_1 \\lor a_2)$, so the resulting expression is equisatisfiable to $\\mathcal{C}$.\n    \\item \\textbf{$\\mathcal{C}$ has 3 literals:} leave $\\mathcal{C}$ intact; it is already 3-CNF.\n    \\item \\textbf{$\\mathcal{C}$ has more than 3 literals:} let $\\mathcal{C}=(a_1 \\lor a_2 \\land ... \\land a_n)$ where $n > 3$. Here, we introduce $n-3$ new variables $z_1, z_2, ..., z_{n-3}$ replacing $\\mathcal{C}$ by the following $n-3$ clauses:\n    {\\centering\n        $(a_1 \\lor a_2 \\lor z_1), (\\neg z_1 \\lor a_3 \\lor z_2), ... ,(\\neg z_i \\lor a_{i-2} \\lor z_{i+1}), (\\neg z_{i+1} \\lor a_{i-1} \\lor z_{i+2}), ..., (\\neg z_{n-3} \\lor a_{n-1} \\lor a_n)$ \\\\\n    }\n    Unlike the first two cases, the conjunction of these $\\mathcal{Z}$ clauses is not equivalent to $\\mathcal{C}$; however, it is equisatisfiable to it. We can demonstrate that equisatisfiability by cases as follows: \\\\\n    \\textbf{Case 4a:} $\\mathcal{C}$ is satisfiable: \\\\\n    The satisfiability of $\\mathcal{C}$ ensures that at least one of $a_1, a_2,..,a_n$ must be \\textit{true} in any satisfying assignment of $\\phi$. Using that fact, we can come up with a satisfying assignment for all $\\mathcal{Z}$ clauses based on the location of that \\textit{true} literal:\n    \\begin{itemize}\n        \\item If any of $a_1,a_2$ is \\textit{true}, then set all $z$-variables to \\textit{false}. The truth of $a_1 \\lor a_2$ satisfies the first clause while the \\textit{false}hood of all $z$-variables satisfies all other clauses since every $\\mathcal{Z}$ clause after the first one contains a negated $z$-variable. Thus, all $\\mathcal{Z}$ clauses are satisfiable.\n        \\item If $a_i$ is \\textit{true} where $3 \\leq i \\leq n$, then set $z_1$ to \\textit{true} and $z_2,...,z_{n-3}$ to \\textit{false}. Similarly, the truth of $z_1$ satisfies the first clause while the \\textit{false}hood of all other $z$-variables satisfies all other clauses since every other $\\mathcal{Z}$ clause after the first one contains a negated $z$-variable. Thus, all $\\mathcal{Z}$ clauses are satisfiable.\n    \\end{itemize}\n    \\textbf{Case 4b:} $\\mathcal{C}$ is unsatisfiable: \\\\\n    The unsatisfiability of $\\mathcal{C}$ ensures that none of $a$-literals can be \\textit{true} without falsifying some other clause in $\\phi$. That means that we need to assume that all $a$-literals must be \\textit{false}. Having all $a$-literals be \\textit{false} reduces our $\\mathcal{Z}$ clauses into:\n    {\\centering\n        $z_1, (\\neg z_1 \\lor z_2), ... ,(\\neg z_i \\lor z_{i+1}), (\\neg z_{i+1} \\lor  z_{i+2}), ..., \\neg z_{n-3}$ \\\\\n    }\n    Clearly, to have all $\\mathcal{Z}$ clauses \\textit{true}, then $z_1$ must be \\textit{true} and $z_{n-3}$ must be \\textit{false} since they form two unit clauses. Notice how if we  keep doing this substitution for the first and last literals (the ones forming unit clauses) accordingly, the truth of the first variable will falsify the first literal in the next intermediate clause. Similarly, the \\textit{false}hood of the last literal removes the second literal in the previous clause. For example, consider the following reduced $\\mathcal{Z}$ clauses:\n    \\begin{center}\n        $z_1, (\\neg z_1 \\lor z_2),(\\neg z_2 \\lor z_3),(\\neg z_3 \\lor z_4), \\neg z_4$ \\\\\n        $\\downarrow$ \\\\\n        $z_2,(\\neg z_2 \\lor z_3),\\neg z_3$ \\\\\n        $\\downarrow$ \\\\\n        $z_2,\\neg z_2$ \\\\\n        $\\downarrow$ \\\\\n        (contradiction)\n    \\end{center}\n    As such, the conjunction of all $\\mathcal{Z}$ clauses ultimately result in a contradiction, thereby making them unsatisfiable.\n\\end{enumerate}",
        "\\begin{enumerate}[label=\\textbf{Property \\arabic*:}, leftmargin=8em]\n    \\item Each variable gets a different color from its negation.\n    \\item T, F, and B vertices get different colors.\n    \\item Any 3-coloring of the graph will map to a valid truth assignment since no literal can take the neutral base color, so they will either be green (i.e., True) or red (i.e., False).\n\\end{enumerate}",
        "\\begin{enumerate}[label=\\textbf{Property \\arabic*:}, leftmargin=6em]\n    \\item No sub-path vertex is connected to more than one clause vertex.\n    \\item There are two ways to visit $C_j$ in a Hamiltonian path, namely: $x_{i,2j-1} \\rightarrow C_j \\rightarrow x_{i,2j}$ and $x_{i,2j} \\rightarrow C_j \\rightarrow x_{i,2j-1}$.\n\\end{enumerate}",
        "\\begin{itemize}\n    \\item Start at vertex $s$.\n    \\item For every variable $x_i$, traverse its sub-path in the direction that matches its truth value while visiting any reachable unvisited clause vertices along the way.\n    \\item Go to vertex $t$ and return to $s$.\n\\end{itemize}",
        "\\begin{itemize}\n    \\item Infinite memory tape\n    \\item Machine state\n    \\item Finite control over state transitions\n    \\item Read-write tape heads that can read/write from/to the tape\n\\end{itemize}",
        "\\begin{enumerate}\n    \\item For each character $c_i \\in w$ on the left side of \\#, cross it and slide tape head to the right until \\# is encountered. If a \\# symbol is found, slide to the first uncrossed $c_j$ symbol after the last x location. If $c_i=c_j \\Rightarrow$ cross off $c_j$ with the 'x' symbol, else $\\Rightarrow$ \\emph{reject}. If no \\# is found, also \\emph{reject}.\n    \\item Check if there are any remaining symbols aside from \\# and x. If there are, \\emph{reject}, otherwise, \\emph{accept}.\n\\end{enumerate}",
        "\\begin{enumerate}\n    \\item $Q$ is the set of possible TM states.\n    \\item $\\Sigma$ is the input alphabet excluding the blank symbol $\\sqcup$.\n    \\item $\\Gamma$ is the tape alphabet, where $\\sqcup \\in \\Gamma$ and $\\Sigma \\subseteq \\Gamma$.\n\\end{enumerate}",
        "\\begin{enumerate}\n    \\item $\\delta: Q\\times\\Gamma \\rightarrow Q\\times\\Gamma\\times\\{L,R\\}$\n    \\item $q_0 \\in Q$ is the initial state.\n    \\item $q_{accept} \\in Q$ is the accepting state.\n    \\item $q_{reject} \\in Q$ is the rejecting state.\n\\end{enumerate}",
        "\\begin{enumerate}[label=\\textbf{Case \\arabic*:}, leftmargin=4em]\n    \\item H outputs \\textbf{Yes}: that immediately implies that $\\text{H}^*$ halts on $\\text{H}^*$, but that would mean that $\\text{H}^*$ loops forever since it definitionally does that when its internal H outputs Yes. This is a contradiction since it means that $\\text{H}^*$ both halts and does not halt.\n    \\item H outputs \\textbf{No}: that immediately implies that $\\text{H}^*$ does not halt on $\\text{H}^*$, but that would mean that $\\text{H}^*$ halts with a No since it propagates the output of its internal H when it outputs No. This is a contradiction since it means that $\\text{H}^*$ both halts and does not halt.\n\\end{enumerate}",
        "\\begin{itemize}\n    \\item \\textbf{Tape 1 (input)}: stores the input tape and is never modified.\n    \\item \\textbf{Tape 2 (simulation)}: keeps a copy of $N$'s tape on some configuration node.\n    \\item \\textbf{Tape 3 (address)}: tracks $D$'s location on $N$'s computation tree which is needed for the BFS traversal.\n\\end{itemize}",
        "\\begin{enumerate}[label=\\textbf{Step \\arabic*:}, leftmargin=4em]\n    \\item Tape 1 contains input $w$ while tapes 2 \\& 3 are empty.\n    \\item Copy tape 1 to tape 2.\n    \\item Simulate the current computation node using tape 2 after consulting the next symbol on tape 3 at each step to make the next allowable transition. If the transition pointed to by the symbol is invalid (i.e., does not correspond to a valid $\\delta$ transition) or if no symbols are left on the tape, then abort that simulation step and go to step 4. If an accepting configuration is found, \\emph{accept} the input. If a rejecting configuration is instead found, go to step 4.\n    \\item Modify the string on tape 3 to have the lexographically next string, then simulate the corresponding branch by going to step 2.\n\\end{enumerate}",
        "\\begin{enumerate}\n    \\item $n_0 \\in \\mathbb{Z}^+$\n    \\item $c \\in \\mathbb{R}^+$\n\\end{enumerate}",
        "\\begin{enumerate}\n    \\item $n_0 \\in \\mathbb{Z}^+$\n    \\item $c \\in \\mathbb{R}^+$\n\\end{enumerate}",
        "\\begin{enumerate}\n    \\item Use $\\mathcal{F}$ to convert $x$ to $x_B$, an input for language $B$.\n    \\item Query the $B$-oracle for string $x_B$: if it accepts, \\emph{accept} $x$, otherwise, \\emph{reject} $x$.\n\\end{enumerate}",
        "\\begin{enumerate}\n    \\item $L \\in$ NP.\n    \\item Every language in NP must be polynomially reducible to $L$.\n\\end{enumerate}",
        "\\begin{enumerate}\n    \\item generates all possible subsets of vertices in graph $G$.\n    \\item checks every generated subset for whether it forms a clique of size at least $k$.\n    \\item if such subset is found, it \\emph{accepts}.\n    \\item otherwise, goes to step 1.\n\\end{enumerate}"
    ],
    "algorithms": [
        "\\begin{algorithm}[H]\n\\SetAlgoLined\n\\SetKwInOut{Input}{Input}\n\\Input{Boolean formula $\\phi$}\n\\KwResult{YES if $\\phi$ is satisfiable, NO otherwise}\nConvert $\\phi$ to implicative normal form $\\gamma$. \\\\\nConstruct graph $G$ with all literals in $\\gamma$. \\\\\n\\ForEach{$\\alpha = (x \\Rightarrow y) \\in \\gamma$}{\n    $E(G) := E(G) + \\{xy, \\bar{y}\\bar{x}\\}$\n}\nLet $\\mathcal{CC}:= \\text{set of maximal strongly connected components of } G$ \\\\\n\\ForEach{$\\mathcal{C} \\in \\mathcal{CC}$}{\n    \\ForEach{$u \\in V(\\mathcal{C})$}{\n        \\If{$\\bar{u} \\in V(\\mathcal{C})$}{\n            \\Return NO\n      }\n    }\n}\n\\Return YES\n \\caption{2-SAT Solver}\n\\end{algorithm}",
        "\\begin{algorithm}[H]\n\\SetAlgoLined\n\\SetKwInOut{Input}{Input}\n\\SetKwRepeat{Do}{do}{while}\n\\Input{Boolean formula $\\phi$}\n\\KwResult{$\\mathcal{U}(\\phi): \\text{the reduced formula of }\\phi$}\nLet $\\mathcal{S}_u := \\text{set of all unit clauses of } \\phi$ \\\\\n\\While{$\\mathcal{S}_u \\neq \\emptyset$} {\n\\ForEach{literal $x \\in \\mathcal{S}_u$}{\n    Eliminate the unit clause of $x$ from $\\phi$ \\\\\n    \\ForEach{clause $\\gamma\\ \\in \\phi $}{\n        \\uIf{$x \\in \\gamma$}{\n            Remove $\\gamma$ from $\\phi$\n        }\n        \\uElseIf{$\\bar{x} \\in \\gamma$}{\n            Remove $\\bar{x}$ from $\\gamma$\n        }\n    }\n}\nUpdate $\\mathcal{S}_u$ \\\\\n}\n \\caption{Unit Propagation (UP)}\n\\end{algorithm}",
        "\\begin{algorithm}[H]\n\\SetAlgoLined\n\\SetKwInOut{Input}{Input}\n\\SetKwRepeat{Do}{do}{while}\n\\Input{Undirected graph $G$}\n\\KwResult{YES if $G$ is 2-colorable, otherwise, NO}\nLet $\\mathcal{BFS}(G) := \\text{the vertex breadth-first traversal order for } G$ \\\\\nLet $\\mathcal{C}(u) := \\text{color of vertex } u$ \\\\\nInitialize $\\mathcal{C}(u) = -1, \\forall u \\in V(G)$\n\\ForEach{vertex $u \\in \\mathcal{BFS}(G)$}{\n    \\uIf{$\\mathcal{C}(u) = -1$}{\n            $\\mathcal{C}(u) := 0$\n    }\n    \\ForEach{vertex $v \\in N(u)$}{\n        \\uIf{$\\mathcal{C}(v) = -1$}{\n            $\\mathcal{C}(v) := 1 - \\mathcal{C}(u)$\n        }\n        \\uElseIf{$\\mathcal{C}(v) \\neq \\mathcal{C}(u)$}{\n            \\Return NO\n        }\n    }\n}\n\\Return YES \\\\\n \\caption{Bipartite Check (2-COLOR)}\n\\end{algorithm}"
    ],
    "tables": [
        "\\begin{table}[h!]\n\\centering\n\\bgroup\n \\begin{tabular}{||c | c||}\n \\hline\n $Q\\times\\Gamma$ & $\\delta(Q\\times\\Gamma)$ \\\\ [0.5ex]\n \\hline\\hline\n ($q_1$, $a$) & $\\{(q_2, b, R)\\}$ \\\\ \\hline\n ($q_1$, $b$) & $\\{(q_2, a, R)\\}$ \\\\ \\hline\n ($q_2$, $a$) & $\\{(q_3, a, L)\\}$ \\\\ \\hline\n ($q_2$, $b$) & $\\{(q_3, b, L)\\}$ \\\\ \\hline\n ($q_3$, $a$) & $\\{(q_1, a, R), (q_3, b, L)\\}$ \\\\ \\hline\n ($q_3$, $b$) & $\\{(q_1, b, R), (q_3, a, L)\\}$ \\\\ [0.5ex] \\hline\n \\end{tabular}\n \\egroup\n \\caption{Example $\\delta$ Mapping}\n\\end{table}",
        "\\begin{table}[h!]\n\\centering\n\\bgroup\n\\begin{tabular}{ P{0.1\\textwidth} @{}c@{} P{0.1\\textwidth} @{}c@{} P{0.1\\textwidth} @{}c@{} }\n(a) &\n\\begin{tabular}{|c|c|c|}\n\\hline\n$b$ & $a$ & $b$ \\\\ \\hline\n$b$ & $a$ & $b$ \\\\ \\hline\n\\end{tabular} &\n(b) &\n\\begin{tabular}{|c|c|c|}\n\\hline\n$b$ & $q_1$ & $b$ \\\\ \\hline\n$b$ & $a$ & $q_2$ \\\\ \\hline\n\\end{tabular} &\n(c) &\n\\begin{tabular}{|c|c|c|}\n\\hline\n$b$ & $q_3$ & $a$ \\\\ \\hline\n$a$ & $a$ & $q_1$ \\\\ \\hline\n\\end{tabular} \\\\ [5ex]\n(d) &\n\\begin{tabular}{|c|c|c|}\n\\hline\n$b$ & $a$ & $a$ \\\\ \\hline\n$a$ & $a$ & $a$ \\\\ \\hline\n\\end{tabular} &\n(e) &\n\\begin{tabular}{|c|c|c|}\n\\hline\n$\\#$ & $a$ & $q_1$ \\\\ \\hline\n$\\#$ & $a$ & $b$ \\\\ \\hline\n\\end{tabular} &\n(f) &\n\\begin{tabular}{|c|c|c|}\n\\hline\n$b$ & $q_3$ & $a$ \\\\ \\hline\n$q_3$ & $b$ & $a$ \\\\ \\hline\n\\end{tabular} \\\\\n\\end{tabular}\n\\egroup\n\\caption{Examples of Legal Windows}\n\\end{table}",
        "\\begin{table}[h!]\n\\centering\n\\bgroup\n\\begin{tabular}{ P{0.1\\textwidth} @{}c@{} P{0.1\\textwidth} @{}c@{} P{0.1\\textwidth} @{}c@{} }\n(g) &\n\\begin{tabular}{|c|c|c|}\n\\hline\n$b$ & $a$ & $b$ \\\\ \\hline\n$b$ & $b$ & $b$ \\\\ \\hline\n\\end{tabular} &\n(h) &\n\\begin{tabular}{|c|c|c|}\n\\hline\n$b$ & $a$ & $b$ \\\\ \\hline\n$q_1$ & $a$ & $q_2$ \\\\ \\hline\n\\end{tabular} &\n(i) &\n\\begin{tabular}{|c|c|c|}\n\\hline\n$q_1$ & $a$ & $b$ \\\\ \\hline\n$a$ & $q_2$ & $a$ \\\\ \\hline\n\\end{tabular} \\\\\n\\end{tabular}\n\\egroup\n\\caption{Examples of Illegal Windows}\n\\end{table}",
        "\\begin{table}[h!]\n\\centering\n \\begin{tabular}{||c | c||}\n \\hline\n $Q$ & \\textbf{Meaning}  \\\\ [0.5ex]\n \\hline\\hline\n $A$ & $\\text{capture first uncrossed input symbol to the left of the \\#}$  \\\\ \\hline\n $F_{s}$ & $\\text{search forwards for the \\# symbol after capturing symbol }s$  \\\\ \\hline\n $B$ & $\\text{search backwards for the \\# symbol}$  \\\\ \\hline\n $C_s$ & $\\text{match first uncrossed input symbol to the right of the \\# with symbol }s$  \\\\ \\hline\n $D$ & $\\text{find first x symbol to the left of the \\#}$  \\\\ \\hline\n $\\emph{accept}$ & $\\text{accepting state}$  \\\\ \\hline\n $\\emph{reject}$ & $\\text{rejecting state}$  \\\\ [1ex]\n \\hline\n \\end{tabular}\n \\caption{Example [85] TM States Interpretations}\n\\end{table}",
        "\\begin{table}[h!]\n\\centering\n \\begin{tabular}{||c | c||}\n \\hline\n $Q\\times\\Gamma$ & $\\delta(Q\\times\\Gamma)$ \\\\ [0.5ex]\n \\hline\\hline\n ($A$, 1) & ($F_{1}$, x, R) \\\\ \\hline\n ($F_{1}$, 0) & ($F_{1}$, 0, R) \\\\ \\hline\n ($F_{1}$, 1) & ($F_{1}$, 1, R) \\\\ \\hline\n ($F_{1}$, \\#) & ($C_1$, \\#, R) \\\\ \\hline\n ($F_{1}$, $\\sqcup$) & ($\\emph{reject}$, $\\sqcup$, R) \\\\ \\hline\n ($C_1$, 1) & ($B$, x, L) \\\\ \\hline\n ($C_1$, 0) & ($\\emph{reject}$, 0, L) \\\\ \\hline\n ($B$, x) & ($B$, x, L) \\\\ \\hline\n ($B$, \\#) & ($D$, \\#, L) \\\\ \\hline\n ($D$, 1) & ($D$, 1, L) \\\\ \\hline\n ($D$, x) & ($A$, x, R) \\\\ \\hline\n ($A$, \\#) & ($C_{\\#}$, \\#, R) \\\\ \\hline\n ($C_{\\#}$, $\\sqcup$) & ($\\emph{accept}$, $\\sqcup$, L) \\\\ \\hline\n ($C_{\\#}$, $\\#$) & ($\\emph{reject}$, $\\#$, L) \\\\ \\hline\n ($C_{\\#}$, 0) & ($\\emph{reject}$, 0, L) \\\\ \\hline\n ($C_{\\#}$, 1) & ($\\emph{reject}$, 1, L) \\\\ [1ex] \\hline\n \\end{tabular}\n \\caption{Example (example:TM1) Partial $\\delta$ Mapping}\n\\end{table}"
    ],
    "sentences": [
        "The expression above translates to the property that every cell in the tableau needs to first have a symbol from the given list of allowed symbol (i.e.",
        "$\\mathcal{C}$) AND not have two different symbols simultaneously.",
        "Next, we define $\\Psi_{initial}$ as:.",
        "Looking at the expression for $\\Psi_{initial}$, one can see the clear resemblance between it and the first row indicated in Figure (fig:tableau) as it is describing exactly the same format for the initial configuration row (i.e.",
        "the 1st one).",
        "Moving on to defining $\\Psi_{accept}$ which is arguably the most straightforward:.",
        "Finally, we set the stage for defining the $\\Psi_{move}$ sub-formula to ensure the legality of all transitions.",
        "First, we can see that a transition is represented by two consecutive rows since every row (except for the first one) follows from the previous one.",
        "However, it would be infeasible to come up with a closed form for a valid transition between two complete rows.",
        "Here, windows along with the \\# delimiters come into play.",
        "Using windows, we would only need to formally describe the validity of any $2\\times 3$ sub-matrices which is considerably easier to accomplish.",
        "A window is \\textbf{legal} if it can follow from a valid transition function $\\delta$ for $N_L$.",
        "Note that this local notion of \\textbf{legality} does in fact guarantee that no two windows contradict each other over the entire tableau due to the non-determinism of $N_L$'s transition function allowing it to take multiple different actions from the same configuration.",
        "To illustrate, consider an example of a non-deterministic $\\delta$ function that recognizes two symbols $\\{a,b\\}$does the following based on three states $q_1, q_2,q_3$:.",
        "Formally, we can express this $\\delta$ function using the following table:.",
        "Now let's see a few examples and non-examples of \\textbf{legal} windows in light of that $\\delta$ function:.",
        "In Table [32], (a) is legal because we do not know the location of the tape head, so it might not be pointing at any cells in the first row.",
        "Window (d) is legal because the tape head might be pointing at the leftmost cell (but we would not be able to see it) while in state $q_3$ in which case, it can invert the $b$ to $a$ and move to the left.",
        "Windows (b) \\& (e) are legal because they follow the correct $\\delta$ rules.",
        "Note that in (c), the tape head takes a different decision from the one in (f) even though they are both in the same state $q_3$ and reading the same symbol $a$.",
        "That is still legal because the non-deterministic $\\delta$ is allowed to make either of them.",
        "Table [21] shows some examples that cannot result from the given $\\delta$ function.",
        "In fact, the first two can never follow from any $\\delta$ function whatsoever.",
        "Window (g) is illegal because the top middle symbol cell (containing $a$) was modified while it was not pointed to by the tape head.",
        "Window (h) is also illegal because it contains two state cells which cannot happen in this setting where we have only one tape head.",
        "On the other hand, window (i) is illegal because it simply violates the chosen $\\delta$ rules as it preserve the symbol while in state $q_1$.",
        "Note that window (i), unlike other (g) and (h), is not absolutely illegal, that is, it can be a legal window for another $\\delta$ function while (g) and (h) cannot.",
        "Having shown how window legality guarantees the legality of configuration row transitions, we can now define $\\Psi_{move}$ as follows:.",
        "Note that we have not defined $\\mathcal{L}(W_{i,j})$ in rigorous detail as it would take us far afield to precisely formulate it.",
        "However, such rigorous definition is not essential for this proof as long as $\\mathcal{L}(W)$ can evidently be defined only in terms of $X_{i,j}$ on all of its six content cells.",
        "This is for two reasons.",
        "Firstly, only the cells of a window completely determine whether it is legal or not with respect to the given $\\delta$ function.",
        "Secondly, the total number of distinct windows is finite since each window has a finite number of cells (six cells) and each cell can only take a finite number of values for any computation tableau on a given language$^{65}$$ is finite since $Q$ and $\\Gamma$ are finite.}.",
        "Last but not least, it remains to show that this reduction from $L$ to SAT is polynomial-time by examining the size of $\\Psi$ in terms of $p(n)$.",
        "Recall that $\\Psi$ is entirely composed of variables in the form $X_{i,j,s}$ which amount to $\\mathcal{O}(p(n)^2\\times |\\mathcal{C}|)$ variables.",
        "But since $|\\mathcal{C}|$ is a constant defined by the machine $N_L$ (not input size $n$), then $\\Psi$ has $\\mathcal{O}(p(n)^2)$ variables.",
        "Next, we need to examine the size of each sub-formula of $\\Psi$.",
        "Looking at $\\Psi_{cell}$, it is clear that comprises $\\mathcal{O}(p(n)^2\\times |\\mathcal{C}|^2) = \\mathcal{O}(p(n)^2)$ literals.",
        "$\\Psi_{cell}$ is a linear scan on the first row, so it involves $\\mathcal{O}(p(n))$ literals.",
        "$\\Psi_{accept}$ checks all tableau cells, so it has a size of $\\mathcal{O}(p(n)^2)$.",
        "$\\Psi_{move}$ iterates over all windows of which there are $\\mathcal{O}(p(n)^2)$.",
        "The predicate $\\mathcal{L}$, on any window $W_{i,j}$, has a fixed size irrespective of the input size $n$, so $\\Psi_{move}$ is of size $\\mathcal{O}(p(n)^2)$.",
        "Therefore, $\\Psi$ has a polynomial size of $\\mathcal{O}(p(n)^2)$.",
        "Note that this reduction merely writes out $\\Psi$, so if $\\Psi$ is of polynomial size, then the reduction would be of polynomial-time.",
        "As such, we have concluded the proof for the Cook-Levin theorem, showing that SAT is indeed NP-complete.",
        "One of the most common feasible SATs is \\textbf{2-SAT} which is a special case of CNF-SAT where each clause contains at most 2 literals.",
        "This variant can be efficiently solved by first writing each clause in its \\textbf{implicative normal form} that is as simple as writing each disjunction as an equivalent implication.",
        "For instance, $x \\lor y$ can be written as $\\bar{x} \\Rightarrow y$ or $\\bar{y} \\Rightarrow x$.",
        "Now, since all clauses are in conjunction, we can just chain all implications together and see if we obtain an implicative contradiction in form of $x \\Rightarrow ..",
        "\\Rightarrow \\bar{x}$ AND $\\bar{x} \\Rightarrow ..",
        "\\Rightarrow x$, in which case the formula would be unsatisfiable.",
        "Performing the latter check has a linear-time solution by [39] through constructing a implication graph where implications between literals correspond to edges between vertices.",
        "The algorithms is based on graph theory concepts such as strongly connected components and topological sorting (for finding an assignment) which are defined as follows.",
        "As shown, the algorithm constructs the implication graph of $\\phi$ and checks whether a literal and its negation exist on the same strongly connected component.",
        "For instance, consider the following formula:.",
        "As shown in Figure [33], the graph contains no contradicting literals on the same SCC, hence $\\phi$ is statisfiable.",
        "We can now use the 2-SAT solver to find a satisfying interpretation.",
        "One straightforward way to do so would be to iterate over each variable $x$ of the formula $\\phi$, set it to \\textit{true} in $\\phi$ then test if the resulting formula is still satisfiable.",
        "If it is, then $x$ can be \\textit{true} in the desired interpretation.",
        "Otherwise, if the resulting formula becomes unsatisfiable, then $x$ must be \\textit{false}.",
        "Note that at every step, previous assignments need to be assumed in the substitution.",
        "Another more efficient way to find a satisfying assignment is through an extension to Algorithm [55] which sorts strongly connected components topologically where $\\textit{comp}[u] \\leq \\textit{comp}[v]$ if there is a path from $u$ to $v$ where $\\textit{comp}[u]$ denotes the topological order index of the SCC to which vertex $u$ belongs.",
        "Then, we assign variable $x$ as \\textit{false} if $\\textit{comp}[x] < \\textit{comp}[\\neg x]$, otherwise, we assign it as \\textit{true}.",
        "We are not going to go into the proof of correctness for such method, but it to give some perspective, we can apply it to Example (example:2sat).",
        "From the marked implication graph, we can note the following relations: $\\textit{comp}[a] < \\textit{comp}[\\neg a]; \\textit{comp}[b] < \\textit{comp}[\\neg b]; \\textit{comp}[c] < \\textit{comp}[\\neg c]$.",
        "As such, the corresponding truth assignment is $a=\\textit{false},b=\\textit{false},c=\\textit{false}$ which indeed satisfies $\\phi$.",
        "Horn satisfiability is a variant of CNF-SAT where each clause is a Horn clause where at most one literal is positive (i.e.",
        "unnegated)",
        "Note that HORN-SAT places no restriction on the number of literals in each clause which might give the impression that it is intractable$^{92}$.",
        "However, despite the looks of it, HORN-SAT can be solved in polynomial time -- in linear time even.",
        "First, we introduce the method of \\textbf{Unit propagation (UP)} or \\textbf{Boolean Constraint propagation (BCP)}.",
        "It is a procedure by which CNF formulae can be simplified via exploiting unit clauses (i.e.",
        "clauses containing one literal).",
        "Despite it being listed as an SAT variant, \\textbf{Maximum Satisfiability} is in fact more general than general SAT as it asks about the maximum number of clauses in a formula that are satisfiable with one variable assignment.",
        "The decision version of that asks whether given a Boolean formula $\\phi$ in CNF, there exists an interpretation that satisfies at least $k$ clauses of $\\phi$.",
        "Now it seems clear why this goal is more general than general SAT because one can obtain an answer to the latter by merely replacing $k$ with the total number of clauses in $\\phi$.",
        "As such, SAT easily reduces to MAX-SAT, making it NP-hard.",
        "Moreover, SAT and MAX-SAT share the same type of certificates being variable assignments, so verifying MAX-SAT solutions is as simple as substituting these assignments and counting the clauses they make \\textit{true}.",
        "That means that the decision problem of MAX-SAT is indeed in NP, hence NP-complete.",
        "\\noindent Unsatisfied clauses in $\\phi$ can be viewed as a conflict with other satisfied ones.",
        "In that sense, maximum satisfiability aims to minimize conflicting clauses in a given formula.",
        "That is why MAX-SAT has many useful applications in the field of software.",
        "One example of that is software package upgradeability where we have a set of packages each requiring other packages while also being in conflict with others.",
        "In that scenario, we want to find the maximum number of packages that we can install without causing any conflicts, which is a standard application of MAX-SAT since packages can be easily modelled as Boolean variables.",
        "A more complex application would be error localization in C code where we have a faulty program and we aim to find the smallest segment of code that is causing that fault.",
        "This is quite challenging since errors in some lines can propagate to other lines, giving the illusion that those other errors are erroneously written while they might in fact be correct.",
        "Luckily, there are numerous heuristic approximations around MAX-SAT that are very good at solving real-life instances of the problem, albeit not any general arbitrary case.",
        "Graph coloring is one of most important concepts of graph theory.",
        "It essentially involves assigning each vertex in a graph with some color to satisfy certain properties or constraints.",
        "For $k$-coloring problems, we ask whether we can color a graph with at most $k$ colors such that no two adjacent vertices have the same color.",
        "As such, 2-COLOR $\\in P$.",
        "Sadly, much like the case of 2-SAT and 3-SAT, 3-COLOR is NP-complete despite it seeming like a slightly harder version of 2-COLOR.",
        "Firstly, it is quite easy to see why 3-COLOR $\\in NP$ since its certificates are vertex color assignments which are the same size of the graph and can be checked in polynomial time by traversing edges, checking if two end-vertices have the same color.",
        "Secondly, we shall now show that 3-SAT reduces to 3-COLOR thereby proving it intractable.",
        "Cliques are complete subgraphs, that is, a fully connected subset of vertices in some graph.",
        "The problem of finding cliques with some properties (e.g., size) is a very relevant one in graph theory.",
        "It is essentially a sibling problem of the Independent Set problem where we look for a pair-wise disjoint subset of vertices on some graph.",
        "CLIQUE is in NP since a proof certificate for it would consist of a set $\\{v_1,...,v_k\\}$ of vertices claimed to form a clique which can be checked in linear time.",
        "Next, we show that CLIQUE is intractable by reducing 3-SAT to it.",
        "The Hamiltonian Cycle problem (HAM-CYCLE) is one of the original twenty-one NP-complete problems by Karp in 1971.",
        "A Hamiltonian cycle is a cycle that visits all vertices in a graph.",
        "In essence, HAM-CYCLE is a special case of the Travelling Salesman Problem (TSP) where we ask whether a graph contains a path that visits every vertex exactly once and returns to the starting vertex under a certain distance.",
        "Such path is also called a cycle and its length is the sum of the weights of all edges it traverses.",
        "As such, HAM-CYCLE is an instance of TSP where all edges have a weight equal to one, and the maximum path length is $|G|$ (number of vertices).",
        "When the program starts, the tape only contains the input symbols on its prefix locations while the rest contains the empty symbol $\\sqcup$.",
        "For any intermediate information the machine needs to store, it can write it somewhere on the tape and read it again later.",
        "Note that the head movement is not necessarily unidirectional throughout the program but can rather apply multiple passes on the same tape locations.",
        "To demonstrate how useful such model can be in describing computation, we shall apply it to language membership problem, that is, verifying whether some input string follows a certain language pattern.",
        "More formally, let language $L = \\{s\\#s \\mid s \\in \\{0,1\\}^*\\}$ and string $w$ be an input string, and we wish to decide whether $w \\in L$.",
        "That is, we want to know whether string $w$ is composed of two identical binary strings separated by a \\# character.",
        "A straightforward approach to do such processing would be to compare each character with the one at the congruent location across the \\# character and check if they match or not and that one \\# exists.",
        "Each processed character aside from \\# gets crossed off by the x symbol.",
        "Thus, we can describe a Turing machine $M$ to perform this task as follows:.",
        "To formalize the control transition from one state to another, we define a transition function $\\delta$ that given the current state and currently read symbol on the tape, determines the next move.",
        "To achieve this, we define the following sets.",
        "As such, we can define the $\\delta$ function in the form: $Q\\times\\Gamma \\rightarrow Q\\times\\Gamma\\times\\{L,R\\}$.",
        "To elaborate, $\\delta(q,a)=(r,b,L)$ means that when the machine is at state $q$ and the head reads symbol $a$ from the tape, the next move would be to transition to state $r$, write symbol $b$ on the tape (replacing $a$), and move the head one location to the left.",
        "Now that we have all the formal objects described, we can formally define a Turing machine.",
        "We then specify the TM's initial state as $q_0 = A$.",
        "Now let's partially define the delta function mappings for some of the machine configurations of interest.",
        "On that basis, any language that cannot be decided with an ordinary Turing machine is called \\emph{Turing-undecidable} or \\emph{undecidable}.",
        "One of the most famous undecidable problems, and the first to be proven so, is the notorious \\textbf{Halting Problem} formally denoted as $\\text{HALT}$:.",
        "Alan Turing, in 1936, proved by contradiction that $\\text{HALT}$ is undecidable.",
        "This undecidability of HALT was one of the most important theorems for theoretical computer science and mathematics because it definitively proved that some parts of mathematics cannot be decided.",
        "That is, some mathematical questions may never be answered by a consistent system of thought.",
        "For the sheer simplicity of Turing's proof for such a seemingly difficult and crucial question, we shall present it here.",
        "Typically, when we say \\emph{Turing machine}, it is meant as something somewhat more specific that is \\emph{deterministic single-tape Turing machine}.",
        "Turing machine determinism will be deferred to the next subsection.",
        "As the name suggests, a \\emph{multitape Turing machine} can have more one than one tape each with their own read-write heads.",
        "As such, to have a more general definition that covers a variation of $k$ tapes, we need to modify the control function $\\delta$ as:.",
        "In this definition, a $k$-tuple is used to represent the symbols read on all $k$ tapes and another $k$-tuple to represent the symbols to be written to each tape in the next step.",
        "The last $k$-tuple represents the movement of each of the $k$ tapes.",
        "Despite the fact that the term \\emph{nondeterminism} typically conveys uncertainty, in the context of computation, nondeterminism is a stronger notion than deterministic computation.",
        "In essence, a deterministic machine is a nondeterministic one but with only one possible state transition from any given state and input.",
        "Both models are similar in their construction with the only difference being the transition $\\delta$ function.",
        "For a nondeterministic Turing machine (NTM), the transition function in defined as:.",
        "where $\\mathcal{P}(S)$ is the power set of $S$, that is, the set of all subsets of $S$.",
        "As such, a nondeterministic Turing machine branches in a tree-like manner, simultaneously considering more than one possibility at a given state.",
        "NTM computation accepts when any of its branches accepts and rejects when all of its branches reject; otherwise, computation never halts.",
        "What is clear from such description of NTMs is that an NTM is more computationally powerful or efficient than a deterministic TM.",
        "What might not be very clear, however, is that there is an equivalence between the two.",
        "\\textbf{But how is SAT different from any other difficult problem?} One answer to what makes SAT truly interesting is its completeness relationship over a bigger class of problems called NP problems.",
        "Simply speaking, for a problem $A$ to be NP-complete, it means that any problem $B$ belonging to the NP class (i.e.",
        "an NP problem) is efficiently reducible or convertible to $A$, which means that a solution to $A$ can be transformed into a solution to $B$.",
        "Problems that are at least as hard as NP-complete problems are informally called \\textit{intractable}.",
        "The formal and rigorous nature of the NP class of problems as well as its relationship to the P problem class shall be later explained in more rigor.",
        "However, to give a rough idea, it suffices to say that an NP problem is one whose solutions can be verified easily.",
        "For example, a solution to CLIQUE$^{7}$ can be easily checked by verifying whether the claimed set of vertices really form a clique (and of course, are more than the given integer).",
        "Unfortunately, solving the problem might not be nearly as easy as verifying its solutions, and that is the essence of the P vs.",
        "NP question.",
        "On the other hand, a P problem is one that can be solved easily, hence also verified easily (by just solving it and comparing the results).",
        "To reduce problem $A$ to problem $B$ is to convert input of $A$ to an input of $B$ such that an answer to the $B$ problem instance automatically gives us an answer to the $A$ instance.",
        "The fact that $A$ is reducible to $B$ is formally expressed as $A \\leq_{\\mathcal{P}} B$.",
        "To say that $A \\leq_{\\mathcal{P}} B$ is to say that $A$ is at most as hard as $B$, which is intuitive, for $A$ cannot be harder than $B$ as long as answering $B$ promptly grants us an answer to $A$.",
        "In that context, $\\mathcal{F}$ is a transformer algorithm or a machine that translates an instance of $A$ to an instance of $B$.",
        "While this definition of reduction is for decision problems, the general idea for any two problems is similar, but this paper mainly focuses on decision problems.",
        "One simple example of a problem reduction is reducing the problem of finding the minimum element in a sequence $X$ of integers (problem $A$) to the problem of sorting a sequence $X$ of integers ascendingly (problem $B$).",
        "The reduction here is trivial because we can pass on the same sequence $X$ as input to problem $B$, then we take the very first element in the output of $B$, which would be the answer to $A$.",
        "Note that none of these two problems are decision problems because their answer is not a binary Yes or No.",
        "It would be somewhat superfluous to tackle the proofs of both corollaries as they can be semantically inferred from the conceptual meaning of reducibility, worst-case, and best-case complexities.",
        "To explain, for corollary [1], if $A$ is at most as hard as $B$, and $A$ has a lower bound complexity $f(n)$, then $B$ must have that lower bound as well$^{48}$.",
        "That is because otherwise, $B$ would have a lower bound than $\\Omega(f(n))$, in which case there must exist at least one input word $w$ such that it is easier to decide if $w \\in B$ than deciding if $w_A$$^{21}$ $\\in A$, which contradicts that $A$ is not harder than $B$.",
        "The same method can be used for corollary (red and comp2) by showing that $A \\neq \\mathcal{O}(f(n))$ would imply that $A$ is sometimes harder than $B$.",
        "In that sense, an oracle for language $A$ is a magical box that takes in a word $w$ and immediately decides if $w \\in A$, and the way it determines that is irrelevant since oracles are purely abstract analytical tools used to establish other more useful concepts.",
        "Furthermore, when an oracle is attached to a Turing machine, the resultant is called an \\textbf{\\emph{oracle Turing machine}}.",
        "To give an example of decision proofs, in a general SAT problem, a proof string $y$ would be an interpretation (i.e., variable assignment) that is claimed to satisfy the given formula.",
        "\\newline",
        "Let $\\phi = (x_1 \\land x_2 \\land \\neg x_3)$, then $y = 110$ is a valid proof string that $\\phi \\in$ SAT since that assignment satisfies $\\phi$.",
        "A proof strings is also sometimes called a \\emph{certificate} or \\textit{witness}.",
        "This concept of polynomially verifiable proof strings is often invoked when analyzing NP problems in the deterministic sense (i.e., using DTMs).",
        "One common misconception about the NP class is that NP stands for \"\\textbf{N}on-\\textbf{P}olynomial\" while, in fact, it stands for \\say{\\textbf{N}on-deterministic \\textbf{P}olynomial.} That naming is based on another definition that is equivalent to the previous one, but relies on NTMs instead.",
        "We call a problem \\textit{natural} when it arises from a genuine objective we need to accomplish in real-life practice as opposed to a problem that was conceptualized merely for the purpose of proving a point.",
        "An example of the former would be sorting sequences while an example of the latter would be producing an arbitrary string of length $n^{1000}$.",
        "The latter problem indeed has a polynomial-time algorithm despite being totally infeasible, but we do not care about it since it does not arise in a practically useful context.",
        "As such, we can generally define P problems as feasibly solvable and NP problems as feasibly verifiable.",
        "Accordingly, as mentioned before, $\\text{P} \\subseteq \\text{NP}$ since it is easy to verify that which is easily solvable.",
        "We have introduced the complexity class NP followed by the concept of polynomial-time reductions$^{33}$.}.",
        "In this section, we present NP-completeness as a property of languages (i.e.",
        "decision problems) that combines both concepts.",
        "Note that CLIQUE is decision problem whose proof certificate would be a set of vertices in $G$ on which the induced subgraph is claimed to be a clique$^{66}$ of size at least k.",
        "Verifying that claim should be as easy as verifying that all vertices are valid, pairwise-adjacent in $G$ and that their count is at least k.",
        "All of that can be done in polynomial time making CLIQUE an NP problem.",
        "In fact, it was also proven to be NP-complete [15]."
    ],
    "elses": [
        "\\begin{definition}\nA Boolean formula $\\phi$ is satisfiable if there exists an interpretation (i.e., variable assignment) that makes it hold true.\n\\end{definition}",
        "\\begin{center}\n    $\\text{SAT} = \\{\\phi \\;|\\: \\phi \\text{ is satisfiable}\\}$\n\\end{center}",
        "\\begin{definition}\nA Boolean formula $\\phi$ is valid if it holds true for every possible interpretation (i.e., variable assignment).\n\\end{definition}",
        "\\begin{lemma}\nThere is a duality between validity and satisfiability:\n\\end{lemma}",
        "\\begin{picture}(0,0)\n        \\put(-15,-8){\\makebox(0,0){\\scalebox{4}{\n          \\textcolor{quotemark}{\\textquotedblright}}}}\n      \\end{picture}",
        "\\begin{genericthm}}\n  {\\end{genericthm}",
        "\\begin{proposition}\nFor any two consecutive configuration rows $\\mathcal{R}_i$ and $\\mathcal{R}_{i+1}$, if all windows sliding over both rows are legal, then $\\mathcal{R}_{i+1}$ legally follows from $\\mathcal{R}_i$. That is, $\\mathcal{R}_i \\text{ is legal} \\Rightarrow \\mathcal{R}_{i+1} \\text{ is legal}$.\n\\end{proposition}",
        "\\begin{proof}\nWe can prove that proposition as follows. If $\\mathcal{R}_i$ is illegal, then the proposition trivially follows. If $\\mathcal{R}_i$ is legal, we consider two types of cells. Cells in $\\mathcal{R}_i$ that are neither a state nor adjacent to a state cell will appear in the top-middle cell in some window$^{27}$. Since all windows on those two rows are legal, then that cell should appear unchanged in $\\mathcal{R}_{i+1}$ in the same relative location where it is supposed to be in a legal transition.\nOn the other hand, state cells in $\\mathcal{R}_i$ must appear in the top-middle cell for some window, so the bottom three cells will be updated based on the $\\delta$ function. That ensures that state cells along with state-adjacent cells are both correctly placed in $\\mathcal{R}_{i+1}$. Therefore, if all windows of both rows are legal, then $\\mathcal{R}_{i+1}$ must be legal, which concludes the proof of Proposition (prop:r r).\n\\end{proof}",
        "\\begin{definition}\nA directed graph $G$ is said to be \\textbf{strongly connected} if there exists a path between any two of its vertices.\n\\end{definition}",
        "\\begin{remark}\nA strongly connected component is a subgraph that is strongly connected.\n\\end{remark}",
        "\\begin{definition}\nFor a directed acyclic graph $G$, \\textbf{topological sort} is a linear ordering of $V(G)$ in which vertex $u$ comes before vertex $v$ if there is directed edge $uv$ from $u$ to $v$.\n\\end{definition}",
        "\\begin{example}\nLet $\\phi = (a\\lor \\neg b)\\land(\\neg a\\lor b)\\land(\\neg a\\lor \\neg b)\\land(a \\lor\\neg c)$. Writing $\\phi$ in implicative normal form yields the following respective implications:\n{\\centering\n    $\\neg a\\Rightarrow \\neg b, a\\Rightarrow b, a\\Rightarrow \\neg b,\\neg a \\Rightarrow\\neg c$ \\\\\n    and equivalently, $b \\Rightarrow a, \\neg b\\Rightarrow \\neg a, b\\Rightarrow \\neg a,c \\Rightarrow a$ \\\\\n}\n\\noindent Constructing the implication graph accordingly gives us the following graph:\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=.45\\textwidth]{img/imp_graph.jpg}\n    \\caption{Example Implication Graph}\n\\end{figure}\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=.45\\textwidth]{img/imp_graph_marked.JPG}\n    \\caption{Implication Graph Strongly Connected Components}\n\\end{figure}\n\\end{example}",
        "\\begin{proposition}\nIf $\\Gamma$ is a Horn formula, then $\\mathcal{U}(\\Gamma)$ is also Horn.\n\\end{proposition}",
        "\\begin{proof}\nIt is easy to infer this from the UP procedure by noting that it either eliminates literals or entire clauses. It does not introduce new positive literals into clauses, so the Horn property should be preserved.\n\\end{proof}",
        "\\begin{lemma}\nIf $\\Gamma$ is an unsatisfiable formula, then $\\Gamma$ contains at least one positive clause and one negative clause.\n\\end{lemma}",
        "\\begin{proof}\nFor the sake of contradiction, assume that $\\Gamma$ has no positive clauses, then every clause of $\\Gamma$ contains at least one negative literal. Thus, we can form a satisfying interpretation by assigning all variables to \\textit{false}. Likewise, we can reach a contradiction if we assume $\\Gamma$ has no negative clauses.\n\\end{proof}",
        "\\begin{theorem}\nA Horn formula $\\Gamma$ is satisfiable $\\iff$ $\\emptyset \\notin \\mathcal{U}(\\Gamma).$ where $\\emptyset$ is the empty clause.\n\\end{theorem}",
        "\\begin{proof}\n$(\\Rightarrow):$ if $\\emptyset \\in \\mathcal{U}(\\Gamma).$, then $\\Gamma$ cannot be satisfiable because that implies that while eliminating some literal $x$, UP encountered a clause that only contains $\\bar{x}$. That means that $x$ and $\\bar{x}$ were in conjunction, so $\\Gamma$ contains a contradiction.\n$(\\Leftarrow):$ If $\\Gamma$ is Horn, then $\\mathcal{U}(\\Gamma).$ is also Horn. Now suppose $\\mathcal{U}(\\Gamma).$ is unsatisfiable. By Lemma (lemma:horn 1 1), $\\mathcal{U}(\\Gamma).$ contains at least one positive clause and one negative clause. A positive Horn clause must either be a unit clause or an empty clause. However, $\\mathcal{U}(\\Gamma).$ cannot contain a unit clause as it would have been eliminated during UP. Thus, the positive Horn clause must be the empty clause $\\emptyset$.\n\\end{proof}",
        "\\begin{theorem}\nSAT $\\leq_{\\mathcal{P}}$ 3-SAT.\n\\end{theorem}",
        "\\begin{proof}\nTo prove this, we consider an arbitrary CNF-SAT formula $\\phi$, and construct an equisatisfiable 3-SAT formula $\\Psi$. We do this transformation for any clause $\\mathcal{C} \\in \\phi$ by replacing it with a set $\\mathcal{Z}$ of 3-CNF clauses. For any $\\phi$ clause, there are four cases:\nTherefore, $\\mathcal{C} \\in \\text{SAT} \\iff \\left(\\bigwedge\\limits_{\\gamma \\in \\mathcal{Z}} \\gamma \\right) \\in \\text{SAT}$, i.e. they are equisatisfiable.\nFinally, this reduction is evidently polynomial since we only add either a constant number of new clauses/variables (cases 1, 2, \\& 3) or a polynomial number thereof (case 4 with $n-3$ clauses).\n\\end{proof}",
        "\\begin{definition}\n    $\\text{MAX-SAT} = \\{\\langle \\phi,k \\rangle \\;|\\: \\text{formula $\\phi$ has at least $k$ jointly satisfiable clauses}\\}$\n\\end{definition}",
        "\\begin{example}\nConsider the following Boolean formula:\n    $\\phi = (a \\lor b)\\land(a \\lor \\neg b)\\land(\\neg a \\lor b)\\land(\\neg a \\lor \\neg b)$.\n$\\phi$ is evidently unsatisfiable on all possible truth assignments of $a$ and $b$. However, any truth assignment will satisfy three out of all four clauses, so $\\langle \\phi,3 \\rangle \\in \\text{MAX-SAT}$.\n\\end{example}",
        "\\begin{definition}\nA $k$-coloring of an undirected graph $G$ is a function $f: V(G) \\rightarrow \\{1, ..., k\\}$ such that $\\{u,v\\}\\in E(G) \\rightarrow f(u) \\neq f(v)$.\n\\begin{remark}\nWe say graph $G$ is $k$-colorable if such function exists for it.\n\\end{remark}\n\\end{definition}",
        "\\begin{theorem}\n3-SAT $\\leq_{\\mathcal{P}}$ 3-COLOR.\n\\end{theorem}",
        "\\begin{proof}\nTo establish the reduction, for any 3-SAT Boolean formula $\\Psi$ in variables $\\{x_1,...,x_n\\}$, we construct a graph $G$ such that $G$ is 3-colorable if and only if $\\Psi$ is satisfiable. We start by adding two vertices for every variable in $\\Psi$: one for $x_i$, and another for $\\neg x_i$. We now need to ensure a coloring on the vertices of $G$ corresponds to a valid assignment. To achieve this, we add a triangle subgraph with three vertices T (True), F (False), and B (Base), and connect every variable and its negation with B as shown below.\n\\begin{figure}[!h]\n\\centering\n\\begin{tikzpicture}[main/.style = {draw, circle, minimum size=3.3em}, scale=0.7, every node/.append style={transform shape},\n        B/.style={fill=cyan, text=white},\n\t\tT/.style={fill=darkgreen, text=white},\n\t\tF/.style={fill=red, text=white}]\n\\node[main,B] (1) at (0,0) {\\Large \\textbf{B}};\n\\node[main,F] (2) at (1.5,-1.5) {\\Large \\textbf{F}};\n\\node[main,T] (3) at (-1.5,-1.5) {\\Large \\textbf{T}};\n\\node[main] (4) at (-4.9,1) {\\Large $x_1$};\n\\node[main] (5) at (-3.9,2.4) {\\Large $\\neg x_1$};\n\\node[main] (6) at (-0.9,3.5) {\\Large $x_2$};\n\\node[main] (7) at (0.9,3.5) {\\Large $\\neg x_2$};\n\\node[main] (8) at (4.9,1) {\\Large $x_3$};\n\\node[main] (9) at (3.9,2.4) {\\Large $\\neg x_3$};\n\\draw[-] (1) -- (2);\n\\draw[-] (2) -- (3);\n\\draw[-] (3) -- (1);\n\\draw[-] (4) -- (5);\n\\draw[-] (4) -- (1);\n\\draw[-] (5) -- (1);\n\\draw[-] (6) -- (7);\n\\draw[-] (6) -- (1);\n\\draw[-] (7) -- (1);\n\\draw[-] (8) -- (9);\n\\draw[-] (8) -- (1);\n\\draw[-] (9) -- (1);\n\\end{tikzpicture}\n\\caption{Subgraph of $G$}\n\\end{figure}\nThese connections ensure the following properties:\nNext, we need to represent clauses in $G$. To emulate a clause structure (i.e., disjunction), we need to add a few extra vertices and connections between these vertices and the original literal vertices we already have. This extra gadget is shown in Figure (fig:3color 1). Note that $t_1,t_2,$ and $t_3$ are literal vertices (i.e., could either be $x_i$ or $\\neg x_i$) of one clause, so this gadget should be added for every clause in $\\Psi$.\n\\begin{figure}[!h]\n\\centering\n\\begin{tikzpicture}[main/.style = {draw, circle, minimum size=2.8em}, scale=0.6, every node/.append style={transform shape},\n        B/.style={fill=cyan, text=white},\n\t\tT/.style={fill=darkgreen, text=white},\n\t\tF/.style={fill=red, text=white},\n\t\tD/.style={double, double distance=2pt, outer sep=2pt}]\n\\node[main] (1) at (0,0) {};\n\\node[main] (2) at (4,2) {};\n\\node[main] (3) at (-4,2) {};\n\\node[main] (4) at (4,-2) {};\n\\node[main] (5) at (-4,-2) {};\n\\node[main] (6) at (0,3.5) {};\n\\node[main,T] (7) at (0,-3.5) {\\Large \\textbf{T}};\n\\node[main,F] (8) at (7,2) {\\Large \\textbf{F}};\n\\node[main,D] (9) at (-4,-5) {\\Large $t_1$};\n\\node[main,D] (10) at (4,-5) {\\Large $t_2$};\n\\node[main,D] (11) at (2,-5) {\\Large $t_3$};\n\\draw[-] (1) -- (6);\n\\draw[-] (1) -- (7);\n\\draw[-] (1) -- (11);\n\\draw[-] (7) -- (4);\n\\draw[-] (7) -- (5);\n\\draw[-] (7) -- (3);\n\\draw[-] (4) -- (2);\n\\draw[-] (4) -- (10);\n\\draw[-] (8) -- (2);\n\\draw[-] (9) -- (5);\n\\draw[-] (3) -- (6);\n\\draw[-] (3) -- (5);\n\\draw[-] (2) -- (6);\n\\end{tikzpicture}\n\\caption{Gadget subgraph $\\mathcal{H}$ of clause $(t_1\\lor t_2\\lor t_3)$}\n\\end{figure}\nNote that the property we need to ensure is that $\\mathcal{H}$ is 3-colorable if and only if clause $(t_1\\lor t_2\\lor t_3)$ is True, that is, at least one of $t_1,t_2,$ and $t_3$ is True. We can demonstrate that this is indeed the case by first showing the graph coloring that results from having all literals set to False:\n\\begin{figure}[!h]\n\\centering\n\\begin{tikzpicture}[main/.style = {draw, circle, minimum size=2.8em}, scale=0.6, every node/.append style={transform shape},\n        B/.style={fill=cyan, text=white},\n\t\tT/.style={fill=darkgreen, text=white},\n\t\tF/.style={fill=red, text=white},\n\t\tD/.style={double, double distance=2pt, outer sep=2pt}]\n\\node[main,B] (1) at (0,0) {};\n\\node[main,T] (2) at (4,2) {};\n\\node[main,F] (3) at (-4,2) {};\n\\node[main,B] (4) at (4,-2) {};\n\\node[main,B] (5) at (-4,-2) {};\n\\node[main] (6) at (0,3.5) {???};\n\\node[main,T] (7) at (0,-3.5) {\\Large \\textbf{T}};\n\\node[main,F] (8) at (7,2) {\\Large \\textbf{F}};\n\\node[main,D,F] (9) at (-4,-5) {\\Large $t_1$};\n\\node[main,D,F] (10) at (4,-5) {\\Large $t_2$};\n\\node[main,D,F] (11) at (2,-5) {\\Large $t_3$};\n\\draw[-] (1) -- (6);\n\\draw[-] (1) -- (7);\n\\draw[-] (1) -- (11);\n\\draw[-] (7) -- (4);\n\\draw[-] (7) -- (5);\n\\draw[-] (7) -- (3);\n\\draw[-] (4) -- (2);\n\\draw[-] (4) -- (10);\n\\draw[-] (8) -- (2);\n\\draw[-] (9) -- (5);\n\\draw[-] (3) -- (6);\n\\draw[-] (3) -- (5);\n\\draw[-] (2) -- (6);\n\\end{tikzpicture}\n\\caption{Gadget $\\mathcal{H}$ Coloring When All Clause Variables Are False}\n\\end{figure}\nWe can see in Figure (fig:3color 2) that the top vertex cannot take any of the three colors, making gadget $\\mathcal{H}$ not 3-colorable when its clause is False. This essentially proves the forward direction. To prove the reverse direction, we let one of the three literals be True and show that $\\mathcal{H}$ would be 3-colorable. Note that it makes no difference which literal we set to True since disjunction is commutative.\n\\begin{figure}[!h]\n\\centering\n\\begin{tikzpicture}[main/.style = {draw, circle, minimum size=2.8em}, scale=0.6, every node/.append style={transform shape},\n        B/.style={fill=cyan, text=white},\n\t\tT/.style={fill=darkgreen, text=white},\n\t\tF/.style={fill=red, text=white},\n\t\tD/.style={double, double distance=2pt, outer sep=2pt}]\n\\node[main,B] (1) at (0,0) {};\n\\node[main,T] (2) at (4,2) {};\n\\node[main,B] (3) at (-4,2) {};\n\\node[main,B] (4) at (4,-2) {};\n\\node[main,F] (5) at (-4,-2) {};\n\\node[main,F] (6) at (0,3.5) {};\n\\node[main,T] (7) at (0,-3.5) {\\Large \\textbf{T}};\n\\node[main,F] (8) at (7,2) {\\Large \\textbf{F}};\n\\node[main,D,T] (9) at (-4,-5) {\\Large $t_1$};\n\\node[main,D,F] (10) at (4,-5) {\\Large $t_2$};\n\\node[main,D,F] (11) at (2,-5) {\\Large $t_3$};\n\\draw[-] (1) -- (6);\n\\draw[-] (1) -- (7);\n\\draw[-] (1) -- (11);\n\\draw[-] (7) -- (4);\n\\draw[-] (7) -- (5);\n\\draw[-] (7) -- (3);\n\\draw[-] (4) -- (2);\n\\draw[-] (4) -- (10);\n\\draw[-] (8) -- (2);\n\\draw[-] (9) -- (5);\n\\draw[-] (3) -- (6);\n\\draw[-] (3) -- (5);\n\\draw[-] (2) -- (6);\n\\end{tikzpicture}\n\\caption{Gadget $\\mathcal{H}$ 3-Coloring When $t_1=$ True}\n\\end{figure}\nAs shown in Figure (fig:3color 3), $\\mathcal{H}$ is indeed 3-colorable, and the same scheme would apply if we make any of the two other literals True (i.e., green) as well. Thus, 3-colorability of a gadget subgraph corresponds to the satisfiability of its respective clause.\nThe same applies to the whole graph $G$ with all clauses mapped out to gadget subgraphs since 3-colorings of $G$ correspond to valid truth assignments (property 3) that satisfies all clauses, hence satisfying $\\Psi$. Thus, the reduction is correct.\nFinally, it remains to show that this reduction is polynomial. That is indeed the case because $|G|=2n + 6  |\\Psi|$ where $n$ is the number of variables in $\\Psi$ and $|\\Psi|$ is the number of its clauses. Therefore, constructing this graph takes polynomial-time.\n\\end{proof}",
        "\\begin{theorem}\n3-SAT $\\leq_{\\mathcal{P}}$ CLIQUE.\n\\end{theorem}",
        "\\begin{proof}\nTo establish the reduction, for any 3-SAT Boolean formula $\\Psi$ of $k$ clauses in variables $\\{x_1,...,x_n\\}$, we construct a graph $G$ such that $G$ contains a $k$-clique if and only if $\\Psi$ is satisfiable. First, we allocate $k$ groups of vertices, one for each clause, that each contain three vertices corresponding to a literal in the respective clause. More formally, $V(G)=\\{x_{i,j,h}\\;|\\: x_i \\in \\text{clause }j; \\; h \\in \\{+,-\\}\\}$. The sign in the last index represents whether the literal is positive (i.e., unnegated variable) or negative (i.e., negated variable). Now, we need to form the edge set for $G$ which we do as follows:\nEssentially, what this means is that all inter-group edges are present except for vertices of the same variable and opposite signs. This guarantees that any two connected vertices correspond to two (not necessarily different) literals that can be set to \\textit{true} simultaneously. As such, a $k$-clique in $G$ corresponds to a valid truth assignment that satisfies all $k$ clauses since it satisfies exactly one literal in each clause. Note that this assignment might not be complete (i.e., does not include all variables) in case of repeated literals on the same clique, but that makes no difference since we can assign arbitrary truth values to the remaining variables without violating that assignment. This essentially proves the forward direction of the reduction.\n\\begin{example}\nLet $\\Psi = (x_1\\lor x_2 \\lor x_3)\\land (x_1\\lor \\neg x_2 \\lor x_3)\\land (\\neg x_1\\lor x_2 \\lor \\neg x_3)$. For such formula, the reduction graph $G$ looks as follows in Figure (fig:3-clique). For simplicity, the group and sign indices are omitted and replaced with the variable names colored with a different color for each group. Note the highlighted clique maps to the assignment that sets all variables to \\textit{true} which indeed satisfies the given $\\Psi$ formula.\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=.35\\textwidth]{img/3-clique.png}\n    \\caption{Example $\\Psi$ Clique Reduction Graph}\n\\end{figure}\n\\end{example}\nTo show the reverse direction, suppose $\\Psi$ is satisfiable. That means that there exists a truth assignment $f: \\{x_1,...,x_n\\} \\rightarrow \\{\\text{\\textit{true}, \\textit{false}}\\}$ that satisfies at least one literal in every clause. Hence, their corresponding vertices must be fully connected (i.e., form a clique) since they belong to different clauses and they can evidently be set \\textit{true} simultaneously. Thus, $G$ must contain a $k$-clique. That concludes the proof of reduction correctness.\nFinally, we need to show that this reduction is polynomial-time. This is \\textit{true} because $|G|= 3|\\Psi|=3k$ where $|\\Psi|$ is the number of clauses in $\\Psi$. Note that this implies the size of $G$ (i.e., number of edges) is always $\\mathcal{O}(\\frac{n(n-1)}{2})=\\mathcal{O}(n^2)$ where $n$ is the graph order, so $|E(G)|$ would also be polynomial in formula size $|\\Psi|$. Therefore, this reduction indeed takes polynomial-time since constructing this graph takes polynomial-time.\n\\end{proof}",
        "\\begin{theorem}\n3-SAT $\\leq_{\\mathcal{P}}$ HAM-CYCLE.\n\\end{theorem}",
        "\\begin{proof}\nTo establish the reduction, for any 3-SAT Boolean formula $\\Psi$ of $k$ clauses in variables $\\{x_1,...,x_n\\}$, we construct a graph $G$ such that $G$ contains a Hamiltonian cycle if and only if $\\Psi$ is satisfiable.\nTo achieve this, we need to encode truth assignments in the graph $G$ such that every truth assignment encodes a different path, and satisfying assignments encode a Hamiltonian one.\nTo encode assignments, we construct $n$ paths of length $2k$ where each path corresponds to a truth assignment on one variable. Unlike other graph reductions in this chapter, this reduction requires $G$ to be a directed graph for reasons that will follow shortly. Note that arrow-less edges indicate a bidirectional edge, that is, two opposite directed edges.\n\\begin{example}\nConsider $\\Psi = (x_1 \\lor x_2 \\lor x_3)\\land(\\neg x_2 \\lor x_3 \\lor \\neg x_4)\\land(\\neg x_1 \\lor x_2 \\lor x_4)$. For simplicity, vertices of each variable path are numbered from $1$ to $2k$, colored differently.\n\\end{example}\n\\begin{figure}[!h]\n\\centering\n\\begin{tikzpicture}[main/.style = {draw, circle, minimum size=2.5em}, scale=0.6, every node/.append style={transform shape},\n        x1/.style={fill=dred, text=white},\n\t\tx2/.style={fill=dblue, text=white},\n\t\tx3/.style={fill=dgreen, text=white},\n\t\tx4/.style={fill=dorange, text=white}]\n\\pgfmathsetmacro{\\xsp}{1.8}\n\\pgfmathsetmacro{\\ysp}{2.2}\n\\foreach \\i in {1,...,4}\n{\n    \\pgfmathsetmacro{\\y}{-1*(\\i - 1)*\\ysp};\n    \\node[] (\\i * 7) at (0,\\y) {\\large $\\boldsymbol{x_{\\i}}$};\n    \\foreach \\j in {1,...,6}\n    {\n        \\pgfmathtruncatemacro{\\label}{\\i*7 + \\j};\n        \\pgfmathtruncatemacro{\\prev}{(\\label - 1)};\n        \\pgfmathsetmacro{\\x}{\\j * \\xsp};\n        \\node[main,x\\i] (\\label) at (\\x,\\y) {\\textbf{\\j}};\n        \\ifthenelse{\\j > 1}\n        {\\draw[-] (\\label) -- (\\prev);}\n    }\n}\n\\end{tikzpicture}\n\\caption{Example $\\Psi$ Assignment Graph}\n\\end{figure}\nUnder this scheme, traversing path $x_i$ from left to right (from $1\\rightarrow 2k$) corresponds to assigning variable $x_i$ \\textit{true} while traversing it from right to left (from $2k\\rightarrow 1$) corresponds to an assignment of \\textit{false}. This encodes a way to assign variables, but not one to make complete truth assignments on all variables using paths because the paths are not connected. To solve this, we connect them end-to-end in sequence as shown in Figure [25] so that a Hamiltonian path encodes a full truth assignment. Note that any Hamiltonian path must traverse every sub-path $x_i$ in only one direction, otherwise it would revisit vertices. That ensures that every Hamiltonian path in $G$ encode a \\textbf{valid} truth assignment.\n\\begin{figure}[!h]\n\\centering\n\\begin{tikzpicture}[main/.style = {draw, circle, minimum size=2.5em}, scale=0.6, every node/.append style={transform shape},\n        x1/.style={fill=dred, text=white},\n\t\tx2/.style={fill=dblue, text=white},\n\t\tx3/.style={fill=dgreen, text=white},\n\t\tx4/.style={fill=dorange, text=white}]\n\\pgfmathsetmacro{\\xsp}{1.8}\n\\pgfmathsetmacro{\\ysp}{2.2}\n\\foreach \\i in {1,...,4}\n{\n    \\pgfmathsetmacro{\\y}{-1*(\\i - 1)*\\ysp};\n    \\node[] (\\i * 7) at (0,\\y) {\\large $\\boldsymbol{x_{\\i}}$};\n    \\foreach \\j in {1,...,6}\n    {\n        \\pgfmathtruncatemacro{\\label}{\\i*7 + \\j};\n        \\pgfmathtruncatemacro{\\prev}{(\\label - 1)};\n        \\pgfmathsetmacro{\\x}{\\j * \\xsp};\n        \\node[main,x\\i] (\\label) at (\\x,\\y) {\\textbf{\\j}};\n        \\ifthenelse{\\j > 1}\n        {\\draw[-] (\\label) -- (\\prev);}\n    }\n    \\pgfmathtruncatemacro{\\ff}{(\\i-1)*7 + 1};\n    \\pgfmathtruncatemacro{\\lf}{(\\i-1)*7 + 6};\n    \\pgfmathtruncatemacro{\\fs}{\\i*7 + 1};\n    \\pgfmathtruncatemacro{\\ls}{\\i*7 + 6};\n    \\ifthenelse{\\i > 1}\n    {\n        \\draw[->,>=stealth] (\\ff.330) -- (\\ls.150);\n        \\draw[->,>=stealth] (\\lf.210) -- (\\fs.30);\n        \\draw[->,>=stealth] (\\ff.270) -- (\\fs.90);\n        \\draw[->,>=stealth] (\\lf.270) -- (\\ls.90);\n    }\n}\n\\end{tikzpicture}\n\\caption{Example $\\Psi$ Connected Assignment Graph}\n\\end{figure}\nNext, we need to ensure that Hamiltonian paths in $G$ encode \\textbf{satisfying} truth assignments. In other words, we need to represent clauses in $G$. We do that by adding $k$ clause vertices corresponding to $k$ clauses. Each clause vertex $C_j$ should be connected to two vertices each sub-path belonging to its constituent variables, namely, $x_{i,2j-1}$ and $x_{i,2j}$. This results in the following properties:\nThe two edges connecting $C_j$ to sub-path $x_i$ take the first direction (left to right) if clause $C_j$ contains $x_i$ unnegated, otherwise the edges are directed from right to left. In doing so, a variable assignment satisfies clause $C_j$ if its corresponding Hamiltonian path visits the $C_j$ vertex. The reason this is the case is that variable sub-paths can only be traversed in one direction in a Hamiltonian path, so if vertex $C_j$ is not oriented according to that direction, the truth assignment on that variable will not satisfy the clause. For example, as shown in Figure (fig:ham-cycle2), $C_1$ contains $x_1$ in positive form, so the edges connecting it to the $x_1$ sub-path are directed from left to right. Consequently, any Hamiltonian path that traverses $x_1$ from right to left (i.e., $x_1=$\\textit{false}) will not be able to visit $C_1$ from $x_1$, and vice versa.\n\\begin{figure}[!h]\n\\centering\n\\begin{tikzpicture}[main/.style = {draw, circle, minimum size=2.5em}, scale=0.6, every node/.append style={transform shape},\n        x1/.style={fill=dred, text=white},\n\t\tx2/.style={fill=dblue, text=white},\n\t\tx3/.style={fill=dgreen, text=white},\n\t\tx4/.style={fill=dorange, text=white},\n\t\tC1/.style={fill=teal, text=white, minimum size=3.3em},\n\t\tC2/.style={fill=teal, text=white, minimum size=3.3em},\n\t\tC3/.style={fill=teal, text=white, minimum size=3.3em}]\n\\pgfmathsetmacro{\\xsp}{1.8}\n\\pgfmathsetmacro{\\ysp}{2.2}\n\\foreach \\i in {1,...,4}\n{\n    \\pgfmathsetmacro{\\y}{-1*(\\i - 1)*\\ysp};\n    \\node[] (\\i * 7) at (0,\\y) {\\large $\\boldsymbol{x_{\\i}}$};\n    \\foreach \\j in {1,...,6}\n    {\n        \\pgfmathtruncatemacro{\\label}{(\\i-1)*7 + \\j};\n        \\pgfmathtruncatemacro{\\prev}{(\\label - 1)};\n        \\pgfmathsetmacro{\\x}{\\j * \\xsp};\n        \\node[main,x\\i] (\\label) at (\\x,\\y) {\\textbf{\\j}};\n        \\ifthenelse{\\j > 1}\n        {\\draw[-] (\\label) -- (\\prev);}\n    }\n    \\pgfmathtruncatemacro{\\ff}{(\\i-2)*7 + 1};\n    \\pgfmathtruncatemacro{\\lf}{(\\i-2)*7 + 6};\n    \\pgfmathtruncatemacro{\\fs}{(\\i-1)*7 + 1};\n    \\pgfmathtruncatemacro{\\ls}{(\\i-1)*7 + 6};\n    \\ifthenelse{\\i > 1}\n    {\n        \\draw[->,>=stealth] (\\ff.330) -- (\\ls.150);\n        \\draw[->,>=stealth] (\\lf.210) -- (\\fs.30);\n        \\draw[->,>=stealth] (\\ff.270) -- (\\fs.90);\n        \\draw[->,>=stealth] (\\lf.270) -- (\\ls.90);\n    }\n}\n\\edef\\tonode{{{1,8,15}, {10,18,24}, {5,13,27}}}\n\\edef\\fromnode{{{2, 9, 16}, {11, 17, 25}, {6, 12, 26}}}\n\\edef\\angst{{150, 110, 130}}\n\\node[main,C1] (C1) at (\\xsp*7,2) {\\large $\\boldsymbol{C_1}$};\n\\node[main,C2] (C2) at (\\xsp*7,-1*\\ysp*4) {\\large $\\boldsymbol{C_2}$};\n\\node[main,C3] (C3) at (\\xsp*8,-1*\\ysp*1.5) {\\large $\\boldsymbol{C_3}$};\n\\foreach \\i in {1,...,3}\n{\n    \\foreach \\z in {0,...,2}\n    {\n        \\pgfmathsetmacro{\\angt}{\\angst[\\i-1] + \\z*40}\n        \\pgfmathsetmacro{\\tox}{\\tonode[\\i-1][\\z]}\n        \\pgfmathsetmacro{\\fox}{\\fromnode[\\i-1][\\z]}\n        \\draw[dotted,line width=0.5pt,->,>=stealth,magenta] (C\\i.\\angt) -- (\\tox);\n        \\draw[dotted,line width=0.5pt,->,>=stealth, blue] (\\fox) -- (C\\i.\\angt);\n    }\n};\n\\end{tikzpicture}\n\\caption{Example $\\Psi$ Clause-Aware Assignment Graph}\n\\end{figure}\nNow, in order to turn Hamiltonian paths into cycles, we add two vertices: a source vertex $s$, and a target vertex $t$ in the way shown in Figure [48]. That simplifies the structure of assignment Hamiltonian cycles by stipulating that they start and end at vertex $s$, without loss of generality. Note that it is very easy to find a cycle that visits all non-clause vertices as there are $2^n$ such cycles even if $\\Psi$ itself is not satisfiable. The goal, however, is to be able to visit clause vertices as well, that is, our assignment path must satisfy (i.e., visit) all clause vertices.\n\\begin{figure}[!h]\n\\centering\n\\begin{tikzpicture}[main/.style = {draw, circle, minimum size=2.5em}, scale=0.6, every node/.append style={transform shape},\n        s/.style={double=blue, fill=white, text=black},\n        t/.style={double=red, fill=black, text=white},\n        x1/.style={fill=dred, text=white},\n\t\tx2/.style={fill=dblue, text=white},\n\t\tx3/.style={fill=dgreen, text=white},\n\t\tx4/.style={fill=dorange, text=white},\n\t\tC1/.style={fill=teal, text=white, minimum size=3.3em},\n\t\tC2/.style={fill=teal, text=white, minimum size=3.3em},\n\t\tC3/.style={fill=teal, text=white, minimum size=3.3em}]\n\\pgfmathsetmacro{\\xsp}{1.8}\n\\pgfmathsetmacro{\\ysp}{2.2}\n\\foreach \\i in {1,...,4}\n{\n    \\pgfmathsetmacro{\\y}{-1*(\\i - 1)*\\ysp};\n    \\node[] (\\i * 7) at (0,\\y) {\\large $\\boldsymbol{x_{\\i}}$};\n    \\foreach \\j in {1,...,6}\n    {\n        \\pgfmathtruncatemacro{\\label}{(\\i-1)*7 + \\j};\n        \\pgfmathtruncatemacro{\\prev}{(\\label - 1)};\n        \\pgfmathsetmacro{\\x}{\\j * \\xsp};\n        \\node[main,x\\i] (\\label) at (\\x,\\y) {\\textbf{\\j}};\n        \\ifthenelse{\\j > 1}\n        {\\draw[-] (\\label) -- (\\prev);}\n    }\n    \\pgfmathtruncatemacro{\\ff}{(\\i-2)*7 + 1};\n    \\pgfmathtruncatemacro{\\lf}{(\\i-2)*7 + 6};\n    \\pgfmathtruncatemacro{\\fs}{(\\i-1)*7 + 1};\n    \\pgfmathtruncatemacro{\\ls}{(\\i-1)*7 + 6};\n    \\ifthenelse{\\i > 1}\n    {\n        \\draw[->,>=stealth] (\\ff.330) -- (\\ls.150);\n        \\draw[->,>=stealth] (\\lf.210) -- (\\fs.30);\n        \\draw[->,>=stealth] (\\ff.270) -- (\\fs.90);\n        \\draw[->,>=stealth] (\\lf.270) -- (\\ls.90);\n    }\n}\n\\edef\\tonode{{{1,8,15}, {10,18,24}, {5,13,27}}}\n\\edef\\fromnode{{{2, 9, 16}, {11, 17, 25}, {6, 12, 26}}}\n\\edef\\angst{{150, 110, 130}}\n\\node[main,C1] (C1) at (\\xsp*7,2) {\\large $\\boldsymbol{C_1}$};\n\\node[main,C2] (C2) at (\\xsp*7,-1*\\ysp*4) {\\large $\\boldsymbol{C_2}$};\n\\node[main,C3] (C3) at (\\xsp*8,-1*\\ysp*1.5) {\\large $\\boldsymbol{C_3}$};\n\\foreach \\i in {1,...,3}\n{\n    \\foreach \\z in {0,...,2}\n    {\n        \\pgfmathsetmacro{\\angt}{\\angst[\\i-1] + \\z*40}\n        \\pgfmathsetmacro{\\tox}{\\tonode[\\i-1][\\z]}\n        \\pgfmathsetmacro{\\fox}{\\fromnode[\\i-1][\\z]}\n        \\draw[dotted,line width=0.5pt,->,>=stealth,magenta] (C\\i.\\angt) -- (\\tox);\n        \\draw[dotted,line width=0.5pt,->,>=stealth, blue] (\\fox) -- (C\\i.\\angt);\n    }\n};\n\\node[main,s] (s) at (\\xsp*3.5,2) {\\Large $\\boldsymbol{s}$};\n\\draw[->,>=stealth] (s) -- (1.30);\n\\draw[->,>=stealth] (s) -- (6.150);\n\\node[main,t] (t) at (\\xsp*3.5,-1*\\ysp*4) {\\Large $\\boldsymbol{t}$};\n\\draw[->,>=stealth] (22.-30) -- (t);\n\\draw[->,>=stealth] (27.-150) -- (t);\n\\draw[->,>=stealth] (t) -- (-1, -1*\\ysp*4) -- (-1,2) -- (s);\n\\end{tikzpicture}\n\\caption{Example $\\Psi$ Final Assignment Graph}\n\\end{figure}\nTo finalize the correctness of this reduction, we verify both directions of the correspondence between satisfying assignments on $\\Psi$ and Hamiltonian cycles on $G$:\n$(\\Rightarrow):$ If $\\Psi$ is satisfiable, then there must exist a satisfying assignment $f: \\{x_1,...,x_n\\} \\rightarrow \\{\\text{\\textit{true}, \\textit{false}}\\}$. We can then construct a Hamiltonian cycle on $G$ as follows:\n$(\\Leftarrow):$ If $G$ contains a Hamiltonian cycle $\\mathcal{H}$, then we can construct a satisfying assignment $f: \\{x_1,...,x_n\\} \\rightarrow \\{\\text{\\textit{true}, False}\\}$ for $\\Psi$ as follows.\nThis assignment is guaranteed to satisfy all clauses since $\\mathcal{H}$ must visit all clause vertices. Thus, this reduction is indeed correct.\nFinally, it remains to show that this reduction is polynomial-time. This is verily the case since $|G|=2n|\\Psi| + |\\Psi|+2= (2n+1)|\\Psi| + 2 = (2n+1)k + 2$, hence the size of $G$ must also be a polynomial. Therefore, constructing this graph takes polynomial-time.\n\\end{proof}",
        "\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=.6\\textwidth]{img/TM.png}\n    \\caption{Turing Machine}\n\\end{figure}",
        "\\begin{example}\n$M:= \\text{ On input string }w:$\nFigure (fig:lang mem tm) is one showcase of $M$ on a sample input $w=101\\#101$. Note that it does not show all intermediate steps.\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=.5\\textwidth]{img/lang_mem_tm.png}\n    \\caption{Language Membership Turing Machine}\n\\end{figure}\n\\end{example}",
        "\\begin{tcolorbox}[title=Turing Machine Set Denotations]\nFor a Turing machine, we denote three finite sets $Q, \\Sigma, \\Gamma$ as:\n\\end{tcolorbox}",
        "\\begin{definition}\nA \\textbf{Turing machine} is a 7-tuple ($Q, \\Sigma, \\Gamma, \\delta, q_0, q_{accept}, q_{reject}$) where $Q, \\Sigma, \\Gamma$ are finite sets such that:\n\\end{definition}",
        "\\begin{center}\n$\\Sigma = \\{0,1\\}$ \\\\\n$\\Gamma = \\{0, 1, \\text{x}, \\sqcup\\}$ \\\\\n\\end{center}",
        "\\begin{center}\n$Q = \\{A, B, D, \\text{accept}, \\text{reject}\\}\\bigcup \\;\\{F_s\\;|\\:s \\in \\Gamma\\}\\bigcup \\;\\{C_s\\;|\\:s \\in \\Gamma\\} \\text{ where:}$\n\\end{center}",
        "\\begin{definition}\nA language $L$ is Turing-decidable if there exists a Turing machine $M$ that \\emph{accepts} on all $\\{w\\mid w \\in L\\}$ and \\emph{rejects} on all $\\{w\\;|\\:w \\notin L\\}$.\n\\end{definition}",
        "\\begin{remark}\nIn order for $M$ to decide $L$, $M$ needs to halt on all inputs.\n\\end{remark}",
        "\\begin{center}\n    $\\text{HALT}=\\{\\langle M, w \\rangle \\;|\\: M \\text{ is a TM that halts on input }w\\}$\n\\end{center}",
        "\\begin{theorem}\nHALT is Turing-undecidable.\n\\end{theorem}",
        "\\begin{proof}\nFor the sake of contradiction, assume there is a Turing machine \\textbf{H} that can decide the halting problem. That is, given a TM's description \\textbf{A} along with its input \\textbf{w}, machine H determines whether or not A halts on input w as shown in Figure (fig:halt).\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=.3\\textwidth]{img/HALT1.png}\n    \\caption{A TM That Decides HALT}\n\\end{figure}\nNow we construct a new Turing machine $\\text{H}^*$ by slightly modifying H to make it go into an infinite loop when input machine A halts on input w (i.e., when H gives a Yes outcome) as illustrated in Figure (fig:halt2).\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=.4\\textwidth]{img/HALT2.png}\n    \\caption{Modified Encapsulating Machine $\\text{H}^*$}\n\\end{figure}\nAs such, upon receiving a halting input, $\\text{H}^*$ can either reject (i.e., output No) or loop forever (i.e., not halt) since it goes into an infinite back-and-forth transition between two states $\\text{Q}_1$ and $\\text{Q}_2$. Now what if we insert the $\\text{H}^*$ machine description as both input machine A and input string w to $\\text{H}^*$? Here, we are interested in the output of the internal H machine for which there can be only two cases: either Yes or No.\nTherefore, there cannot exist a Turing machine that decides HALT since its very existence is contradictory. Hence, HALT is undecidable.\n\\end{proof}",
        "\\begin{definition}\nA Turing machine $M$ recognizes a language $L$ if  $M$ \\emph{accepts} on all $\\{w\\;|\\:w \\in L\\}$, but does not necessarily halt or \\emph{reject} on all $\\{w\\;|\\:w \\notin L\\}$.\n\\end{definition}",
        "\\begin{remark}\nIf $M$ decides $L$, then $M$ recognizes $L$.\n\\end{remark}",
        "\\begin{center}\n$\\delta: Q\\times\\Gamma^k \\rightarrow Q\\times\\Gamma^k\\times\\{L,R\\}^k$\n\\end{center}",
        "\\begin{theorem}\nEvery multitape Turing machine has an equivalent single-tape Turing machine.\n\\end{theorem}",
        "\\begin{proof}\nWe present a way to simulate multiple tapes using a single tape. Let $M$ be a multi-tape TM with $k$ tapes, and $S$ be a single-tape TM. We can then simulate $M$ using $S$ by constructing its tape as the concatenation of all $k$ tapes' contents delimited by the \\# symbol. The heads locations would be simulated by dotting the symbols to which they point, that is, replacing each symbol $a$ pointed to by some tape head with symbol $\\dot{a}$. Note that \\# $\\notin \\Sigma$ and $\\{\\dot{a}\\;|\\: a\\in\\Sigma\\}\\cap\\Sigma=\\phi$. In other words, the special characters are not part of the input alphabet to avoid confusion with input symbols.\n\\end{proof}",
        "\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=.6\\textwidth]{img/multitape_conv.png}\n    \\caption{Multitape to Single-Tape Conversion}\n\\end{figure}",
        "\\begin{center}\n$\\delta: Q\\times\\Gamma^k \\rightarrow \\mathcal{P}( Q\\times\\Gamma^k\\times\\{L,R\\}^k)$\n\\end{center}",
        "\\begin{theorem}\nEvery nondeterministic Turing machine has an equivalent deterministic Turing machine.\n\\end{theorem}",
        "\\begin{tcolorbox}[title=Proof Idea]\nThe idea is to simulate a nondeterministic Turing machine $N$ by searching its computation tree using a deterministic Turing machine $D$, and if $D$ finds an accepting state, it accepts, and if it exhausts the entire search tree without finding such state, it rejects. Otherwise, $D$ never halts. In $N$'s computation tree, every node corresponds to some machine configuration$^{81}$ which can either be accepting, rejecting, or intermediate. Conducting this search in a breadth-first manner is more prudent than depth-first as some branch might not halt while some other branch reaches an \\textit{accepting} state, in which case, $D$ might fail to find the accepting node as it gets stuck trying to cover the depth of the infinite branch.\n\\end{tcolorbox}",
        "\\begin{proof}\nWe construct $D$ as a 3-tape TM which, based on theorem [81], has an equivalent single-tape TM. The three tapes of $D$ shall be used as follows:\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=.6\\textwidth]{img/ntm-to-tm.png}\n    \\caption{Deterministic TM $D$}\n\\end{figure}\nOne pithy way to represent the track location on tape 3 would be as a sequence representing the child order relative to its parent at each level starting from root. For instance, node 421 represents the node we reach by going from root onto its $4^{th} child \\rightarrow 2^{nd} child \\rightarrow 1^{st} child$. However, as with any tape, it needs to have a finite alphabet, so we should have a limited set of possible integers in that sequence. In other words, a child's order should be bounded; therefore, we define tape 3's alphabet as $\\Sigma=\\{1,2,...,b\\}$ where $b$ is the maximum branching factor for $N$'s computation tree. It is worth noting that the address on tape 3 could be invalid if it corresponds to a nonexistent child. With that settled, we can now start describing $D$ as follows:\n\\noindent Note that $D$ repeats steps it already performed before because it traverses (i.e., simulates) the computational tree from the root till the last child on the node address for every new node address. In essence, $D$ performs BFS by a DFS dynamic, that is, it dives through the depth of the tree to reach a certain node and then returns to the root (step 2) before going for the next node in the BFS order. Note that this is not a precise description of the simulator Turing machine since rigorously describing it would be cumbersome and does not serve the core purpose of this proof. However, moderate understanding of how Turing machines work suffices to see that it is possible to construct such machine as $D$.\n\\end{proof}",
        "\\begin{definition}\nA Boolean clause is a conjunction or disjunction of literals where a literal is a Boolean variable or its negation. The default usage of \\say{clause} is to mean disjunctive clause.\n\\end{definition}",
        "\\begin{definition}\nAn alphabet $\\Sigma$ is a set of symbols used to encode information such as inputs and proofs.\n\\begin{remark}\nThe most common encoding is binary encoding on the alphabet $\\{0,1\\}$.\n\\end{remark}\n\\end{definition}",
        "\\begin{definition}\nA language is a set $L \\subseteq \\Sigma^*$ where $\\Sigma^*$ is the set of all finite strings on the alphabet $\\Sigma$.\n\\end{definition}",
        "\\begin{definition}\nA decision problem is some language $L \\subseteq \\Sigma^*$ which represents the set of all inputs that satisfy the underlying predicate of the problem.\n\\begin{remark}\nAnother way to look at these inputs is that they are the only ones on which a Turing machine that solves $L$ \\textit{accepts}.\n\\end{remark}\n\\end{definition}",
        "\\begin{convention}\nThe outcome (i.e., decision) of a Turing machine $M$ on some input $x$ is denoted as $M\\langle x \\rangle$. We write $M\\langle x \\rangle = 1$, Yes, or \\textit{accept} to mean that $M$ \\textit{accepts} on input $x$. Conversely, we write $M\\langle x \\rangle = 0$, No, or \\textit{reject} to mean that $M$ \\textit{rejects} on input $x$.\n\\end{convention}",
        "\\begin{convention}\nWhen a Turing machine $M$ is expected to have an output string aside from its halting decision on input $x$, we denote that output as $M(x)$. Structurally, that output can be considered as the tape contents at the halting state.\n\\end{convention}",
        "\\begin{convention}\nThe terms \\emph{decision problem} and \\emph{language} are often used interchangeably with \\emph{language} being more frequent in abstract contexts.\n\\end{convention}",
        "\\begin{definition}\nA Turing machine $M$ is PTIME if there exists a polynomial $p$ such that $M$ takes $p(|x|)$ steps (i.e. tape head moves) to compute $M\\langle x \\rangle$ where $x\\in\\Sigma^*$.\n\\end{definition}",
        "\\begin{definition}[Big-O Notation]\nFor two functions $f$ and $g$, we write $f(n) = \\mathcal{O}(g(n))$ \\emph{iff} there exists:\nsuch that $\\forall n \\ge n_0 (f(n) \\le cg(n))$. This expresses an upper bound.\n\\end{definition}",
        "\\begin{definition}[Big-Omega Notation]\nFor two functions $f$ and $g$, we write $f(n) = \\Omega(g(n))$ \\emph{iff} there exists:\nsuch that $\\forall n \\ge n_0 (f(n) \\ge cg(n))$. This expresses a lower bound.\n\\end{definition}",
        "\\begin{definition}[Big-Theta Notation]\nFunction $f(n) = \\Theta(g(n))$ \\emph{iff} $f(n) = \\Omega(g(n))$ and $f(n) = \\mathcal{O}(g(n))$. That is, $g(n)$ is a tight bound for $f(n)$.\n\\end{definition}",
        "\\begin{convention}\nFor a problem $L$, a bound function $\\mathcal{B} \\in \\{\\mathcal{O}, \\Omega, \\Theta\\}$, and a function $f$, we write $L=\\mathcal{B}(f(n))$ if there exists a TM $A$ with run-time function $T$ such that $T(n)=\\mathcal{B}(f(n))$.\n\\end{convention}",
        "\\begin{definition}\nFor two decision problems $A,B$, we write $A \\leq_{\\mathcal{P}} B$ if there exists a PTIME TM $\\mathcal{F}$ such that for any input $x$ to problem A, $\\mathcal{F}(x) \\in B$ $\\iff$ $x \\in A$.\n\\end{definition}",
        "\\begin{lemma}\nIf $A \\leq_{\\mathcal{P}} B$ and $B \\leq_{\\mathcal{P}} C$, then $A \\leq_{\\mathcal{P}} C$.\n\\end{lemma}",
        "\\begin{proof}\nLet $\\mathcal{F}$ be an $A$-to-$B$ reducer, and $\\mathcal{H}$ be a $B$-to-$C$ reducer. Then, $x \\in A \\iff \\mathcal{F}(x) \\in B$, and  $y \\in B \\iff \\mathcal{H}(y) \\in C$. Therefore, it follows that $\\mathcal{H}(\\mathcal{F}(x)) \\in C$ \\emph{iff} $x \\in A$; that is, $\\mathcal{H} \\circ \\mathcal{F}$ is an $A$-to-$C$ reducer. As such, it remains to show that $\\mathcal{H} \\circ \\mathcal{F}$ is PTIME, but since polynomials are closed under composition$^{44}$, then $\\mathcal{H} \\circ \\mathcal{F}$ is PTIME as well. Hence, $A \\leq_{\\mathcal{P}} C$.\n\\end{proof}",
        "\\begin{corollary}\nIf $A \\leq_{\\mathcal{P}} B$ and $A = \\Omega(f(n))$, then $B = \\Omega(f(n))$.\n\\end{corollary}",
        "\\begin{corollary}\nIf $A \\leq_{\\mathcal{P}} B$ and $B = \\mathcal{O}(f(n))$, then $A = \\mathcal{O}(f(n))$.\n\\end{corollary}",
        "\\begin{definition}\nAn \\textbf{\\emph{oracle}} is a black box device capable of deciding a language in a single step. That is, for any word $w$, the oracle for language $A$ can instantly decide whether $w \\in A$.\n\\end{definition}",
        "\\begin{definition}\nAn \\textbf{\\emph{oracle Turing machine}} $M^A$ is a modified Turing machine that can consult an oracle for language $A$.\n\\end{definition}",
        "\\begin{definition}\nFor two languages $A, B$, we write $A \\leq_T B$ to denote that $A$ is decidable relative to $B$. That is, there exists an oracle Turing machine $M^B$ that can decide $A$.\n\\end{definition}",
        "\\begin{theorem}\nIf $A \\leq_T B$ and $B$ is decidable, then $A$ is decidable.\n\\end{theorem}",
        "\\begin{proof}\nLet $M^B$ be an oracle TM that decides $A$, and $D$ be an ordinary TM that decides B. Then, we can replace the $B$-oracle in $M^B$ by $D$, which will result in a compound \\emph{ordinary} TM that decides $A$. Thus, $A$ is decidable.\n\\end{proof}",
        "\\begin{lemma}\nIf $A \\leq_{\\mathcal{P}} B$, then $A \\leq_T B$.\n\\end{lemma}",
        "\\begin{proof}\nLet $\\mathcal{F}$ be an $A$-to-$B$ reducer TM. We can build an oracle Turing machine $\\mathcal{F}^B$ that decides $A$ as follows:\n\\noindent$\\mathcal{F}^B:= \\text{ On input string }x$:\nTherefore, there exists an oracle Turing machine $\\mathcal{F}^B$ that decides $A$.\n\\end{proof}",
        "\\begin{definition}\nA decision problem $L \\subseteq \\Sigma^*$ is in P if there is a PTIME algorithm (i.e., Turing machine) M such that $x \\in L$ $\\iff$ $M\\langle x \\rangle  = 1$.\n\\begin{remark}\nIn that sense, $M$ is a solver algorithm as it determines the outcome of the decision problem (Yes or No).\n\\end{remark}\n\\end{definition}",
        "\\begin{definition}\nA decision problem $L \\subseteq \\Sigma^*$ is in NP if there exist a polynomial $p$ and a PTIME algorithm (i.e., Turing machine) $V$ such that $x \\in L$ $\\iff$ $\\exists~ y \\in \\Sigma^{\\le p(|x|)}(V\\langle x,y \\rangle = 1)$.\n\\begin{remark}\nHere, $V$ is a verifying algorithm as it takes $y$ as a proof string that $x \\in L$ and checks if it is a valid proof. $\\Sigma^{\\le p(|x|)} = \\{w: |w| \\le p(|x|)\\}$ where $|w|$ is the length of string $w$, which ensures the proof length is polynomially bounded in size.\n\\end{remark}\n\\end{definition}",
        "\\begin{theorem}\nA decision problem $L \\subseteq \\Sigma^*$ is in NP if and only if it can be solved by a nondeterministic Turing machine (NTM) in polynomial time.\n\\end{theorem}",
        "\\begin{proof}\n$(\\Rightarrow):$ if $L \\in \\text{NP}$, then the size of its proof certificate is polynomially bounded, so all of its possibilities can be exhausted by an NTM in polynomial time through exhaustive branching. And since each branch, corresponding to one possibility of a certificate, can be deterministically verified (i.e., through a DTM) in PTIME, then the entire certificate search process can be done by an NTM in PTIME. If a valid proof certificate is found, then the NTM \\textit{accepts}, otherwise, it \\textit{rejects}.\n$(\\Leftarrow):$ if $L$ can be solved by an NTM $N_L$ in PTIME, then certificates can be set up as the set of nondeterministic choices that $N_L$ makes until it reaches an accepting state (if any). Note that if $N_L$ \\textit{accepts}, there must be at least one such polynomially bounded certificate since certificate length would be determined by the depth of the accepting branch in $N_L$. We can then devise a deterministic TM $V_L$ that can verify such certificates by simulating the $M_L$ computation tree on the choices described by the certificate and checking whether they result in an accepting node (i.e., configuration). For instance, the witness $421$ represents the node we reach by going from root onto its $4^{th} child \\rightarrow 2^{nd} child \\rightarrow 1^{st} child$, which may or may not be an accepting node.\n\\end{proof}",
        "\\begin{namedthm}{Feasibility Thesis}\nA natural problem has a feasible algorithm iff it has a polynomial-time algorithm.\n\\end{namedthm}",
        "\\begin{lemma}\n$\\text{P} \\subseteq \\text{NP}$\n\\end{lemma}",
        "\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=.3\\textwidth]{img/comp_hier.png}\n    \\caption{General Complexity Class Hierarchy}\n\\end{figure}",
        "\\begin{proof}\nLet a problem $L \\in P$. Then, there exists a PTIME algorithm M such that $x \\in L$ \\emph{iff} $M(x) = 1$. We can then construct a PTIME verifier machine $V$ as: $V\\langle x,y \\rangle = M\\langle x \\rangle$. In other words, we can ignore the proof $y$ and just solve $L$ using $M$ in PTIME directly. Thus, $L \\in NP$\n\\end{proof}",
        "\\begin{definition}\nA language $L$ is NP-complete if:\n\\end{definition}",
        "\\begin{center}\n    $\\text{CLIQUE} = \\{\\langle G,k \\rangle \\;|\\: \\text{G is a graph containing a clique of size $\\ge$ k}\\}$\n\\end{center}",
        "\\begin{theorem}\n    CLIQUE $\\leq_{\\mathcal{P}}$ HALT.\n\\end{theorem}",
        "\\begin{proof}\nLet $M\\langle G,k \\rangle$ be a TM that on input graph $G$ and integer $k$:\n\\begin{remark}\nIf no satisfying subset exists, the machine $M$ effectively loops forever. As such, $M$ halts only when a satisfying clique exists.\n\\end{remark}\nNow we can establish the reduction from CLIQUE to HALT as:\n\\begin{center}\n    HALT$\\langle M, \\langle G,k \\rangle\\rangle$ $\\iff$ $\\langle G,k \\rangle \\in \\text{CLIQUE}$\n\\end{center}\nNote that $M$ is an EXPTIME Turing machine, but the reduction does not involve actually running it, therefore, CLIQUE is polynomially reducible to HALT.\n\\end{proof}"
    ],
    "formulas": [
        "\\[.7cm]\n\t\\includegraphics[width=25mm]{img/auc_logo.jpg}\\\\[.5cm]\n\t\\textsc{Department of Mathematics and Actuarial Science}\\\\[0.5cm]\n\t\\rule\\{\\linewidth\\}\\{0\\.5mm\\} \\\\[0.4cm]\n\t{ \\huge \\bfseries On\\ Theoretical\\ Complexity}\\\\[0.1cm]\n\t\\textsc{And\\ Boolean\\ Satisfiability}\\\\\n\t\\rule\\{\\linewidth\\}\\{0\\.5mm\\} \\\\[.5cm]\n\t\\textsc{\\large Thesis BS.c. Mathematics}\\\\[2.5cm]\n\t\\begin{minipage}{0.4\\textwidth}\n\t\\begin{flushleft} \\large\n\t\\emph{Author:}\\\\\n\tMohamed\\space \\textsc{Ghanem}\n\t\\end{flushleft}\n\t\\end{minipage}\n\t~\n\t\\begin{minipage}{0.4\\textwidth}\n\t\\begin{flushright} \\large\n\t\\emph{Supervisor:} \\\\\n\tDr\\.\\ Daoud\\space \\textsc{Siniora} \\\\[1em]\n\t\\end{flushright}\n\t\\end{minipage}\\\\[4cm]\n\t\\vfill\n\t{\\large May\\ 2021}\\\\\n    \\clearpage\n\\end{titlepage}\n\\let\\cleardoublepage=\\clearpage\n\\input{abstract}\n\\input{acknowledgement}\n\\tableofcontents\n\\input{introduction}\n\\input{problem_hardness}\n\\input{SAT}\n\\input{graph_reds}\n\\bibliographystyle{plain}\n\\bibliography{ref.bib}\n\\end{document}\n\\begin{fquote}[Alan Turing, 1939][The Father of Computer Science]\nWe shall not go any further into the nature of this oracle apart from saying that it cannot be a machine.\n\\end{fquote}\nPrior to the conception of theoretical complexity, mathematicians had no rigorous basis to determine the difficulty level of a given problem; instead, they merely followed a rough sense of how hard they struggled with it. Fortunately, the relatively recent branch of theoretical complexity takes it upon itself to try to set formal grounds and boundaries to computational problem hardness relative to other problems. It bases this hardness comparison between two problems on the number of steps$^{73}$ it takes a Turing machine to transform one of them to the other. For simplicity, we can treat a TM as a program or an algorithm designed to solve a specific problem without paying much attention to its internals. This leads us to the concept of complexity classes.\n\\input{problem_hardness/comp_hier}\n\\input{problem_hardness/problem_reduction}\n\\input{problem_hardness/np_completeness}\n\\textbf{General SAT}\nThe language of SAT is classified under multiple categories depending on the form of its Boolean formulas. Some of these variations are easier than others, and many others are, in fact, equivalent despite their apparent dissimilarities.\nThere are two major normal forms in which any Boolean formula can be expressed: Conjunctive Normal Form (CNF) and Disjunctive Normal Form (DNF). As the name suggests, CNF is a conjunction of disjunctions (i.e., disjunctive clauses) while DNF is a disjunction of conjunctions (i.e., conjunctive clauses). It should be easy to see that formulas in DNF are much easier to solve as one only needs to show that one member conjunctive clause is satisfiable for the whole disjunctive system to be so. Deciding whether a conjunction is satisfiable is as easy as checking that it does not contain a contradiction; that is, it does not simultaneously contain a variable and its negation. This check can be done efficiently in linear time. In Boolean algebra, any formula can expressed in CNF as well as DNF; however, the conversion process might be exponentially long in terms of the time it takes and the length of the resulting formula. For instance, consider the following example of a CNF formula$^{62}$:\n\\begin{center}\n    $(X_1\\lor Y_1)\\land(X_2\\lor Y_2)\\land ... \\land(X_n\\lor Y_n)$\n\\end{center}\nThis formula can be converted to DNF by applying the distributive property of Boolean algebra which will result in an expression containing $2^n$ terms. If we interchange conjunctions and disjunctions in the above expression, we get the following DNF formula:\n\\begin{center}\n    $(X_1\\land Y_1)\\lor(X_2\\land Y_2)\\lor ... \\lor(X_n\\land Y_n)$\n\\end{center}\nLikewise, if we wish to convert that formula to an equivalent one in CNF, we get the following expression:\n\\begin{center}\n    $(X_1\\lor X_2 \\lor ... \\lor X_n)\\land$  \\\\\n    $(Y_1\\lor X_2 \\lor ... \\lor X_n)\\land$    \\\\\n    $(X_1\\lor Y_2 \\lor ... \\lor X_n)\\land$    \\\\\n    $(Y_1\\lor Y_2 \\lor ... \\lor X_n)\\land...\\land$    \\\\\n    $(Y_1\\lor Y_2 \\lor ... \\lor Y_n)$\n\\end{center}\nEach disjunctive clause above contains $n$ literals, and the whole expression contains every possible configuration of such clause on $X$s and $Y$s which amounts to a total of $2^n$ clauses. However, although converting an arbitrary Boolean formula $\\phi$ into an equivalent one in CNF or DNF potentially takes exponential time, we actually do not need to find an \\emph{equivalent} one to determine whether $\\phi$ is satisfiable through solving CNF or DNF. Instead, we only need to construct an \\emph{equisatisfiable} formula to $\\phi$, that is, a formula $\\phi'$ that is satisfiable if and only if $\\phi$ is satisfiable. A simple example to distinguish between logical equivalence and equisatisfiability is the following two formulas:\n\\begin{center}\n    let $\\phi$ be a Boolean formula \\\\\n    let $\\Psi = \\phi \\land z$\n\\end{center}\nNote that if $\\phi$ is satisfiable, then $\\Psi$ must also be satisfiable since we can use the same satisfying variable assignment of $\\phi$ and append to it a \\textit{true} assignment for $z$. Conversely, if $\\phi$ is unsatisfiable, then no possible assignment can satisfy $\\Psi$ since it would always be the case that $\\Psi = \\textit{false} \\land z$ which is always \\textit{false}. Hence $\\phi$ and $\\Psi$ are indeed equisatisfiable. However, strictly speaking, they are not equivalent because they have different variables, so their satisfying variable assignments would be different.\nThe good news is there are algorithms that can transform an arbitrary Boolean formula into an equisatisfiable one in CNF in polynomial time such as the one devised by Karp [16]. However, the bad news is that no similar polynomial transformation is known for DNF, which is the much easier one to solve. In fact, the existence of such reduction would imply $\\text{P}=\\text{NP}$.  What makes efficient CNF reductions useful is that we can now transform the SAT problem into CNF-SAT without making extra assumptions about its complexity class since the conversion is PTIME anyways, so rest assured; CNF-SAT is just as hard! That is why SAT is generally used to mean CNF-SAT.\n\\newpage\n\\textbf{The Cook-Levin Theorem (CLT)}\nNow we arrive at one of the most central theorems in theoretical complexity which was arguably the last notable stride on the quest for answering the P vs. NP dilemma. The reason why it is of great significance is that it simplifies the question of finding whether the two classes of problems are equivalent to merely determining whether we can efficiently solve a single NP-complete problem. In spite of that reduction, the problem remains unsolved for nearly half a century!\nThe theorem was named after Stephen Cook and Leonid Levin who both reached it independently.\n\\begin{theorem}\nSAT is NP-complete.\n\\end{theorem}\nThe following proof of CLT is based on the works of (hartmanis1982computers, sipser2012introduction) and it uses a few concepts that were introduced earlier in this paper.\n\\begin{tcolorbox}[title=Proof Idea]\nTo prove that a problem is NP-complete, we need to first show that it is in NP, then we show that every problem in NP is polynomially reducible to it. It is easy to show that SAT is in NP which will be presented shortly. The second part is a proof by construction in which we need to come up with a polynomial many-to-one reduction to SAT that works for any problem in NP. We construct a Boolean formula $\\Psi$ that is satisfiable if and only if a TM for some NP problem $L$ accepts on some input $I$. More formally, $I \\in L \\iff \\phi \\in \\text{SAT}$, which is the essence of reduction.\n\\end{tcolorbox}\n\\begin{proof}\nTo show that SAT is in NP, it suffices to see that it can be solved in polynomial time by an NDTM that guesses all possible variable assignments at every branch and accepts when it finds a satisfying assignment.\nNow we move to constructing a polynomial many-to-one from an arbitrary problem $L$ in NP to SAT. By definition, there exists a non-deterministic Turing machine $N_L$ that decides $L$ in $p(n)$ steps where $p$ is a polynomial and $n$ is the size of input $I$. Here, we introduce an auxiliary construct to help us describe $N_L$'s computation in a tabular form called \\textbf{tableau}. As shown in Figure [23], every row contains one branch configuration on input $I$ starting with the initial configuration. The row format is set up so that every configuration is bounded by two \\# symbols whose relevance will later be shown. The configuration itself is an array of cells containing configuration state $q \\in Q$ along with tape contents $T_i \\in \\Gamma$ -- using the same Turing machine set denotations in section [82].\n\\begin{figure}[h]\n    \\centering\n    \\includegraphics[width=.65\\textwidth]{img/clt_table.png}\n    \\caption{$p(n)\\times p(n)$ configurations tableau}\n\\end{figure}\nThe state cell appears right before the symbol to which the tape head is pointing. That way, it represents both the state and the tape head location. That is why it appears before the tape sub-array in the first row since the tape head initially points to the first symbol on the tape.\nFurthermore, rows (i.e. configurations) are ordered by their occurrence in the computational branch, that is, every configuration row is the result of applying the transition function $\\delta$ on its predecessor. A \\textbf{window} is $2 \\times 3$ sub-matrix of the tableau, which will be used later to ensure the validity of configurations. It is easy to see why the tableau has $p(n)$ rows because every configuration corresponds to one step. Though, a tableau is not required to be exactly $p(n)$ columns wide, but we know that $N_L$ cannot consider more than $p(n)$ cells because that would take more than $p(n)$ contradicting the definition of $p$. Accordingly, tableau width was set as $p(n)$ containing only considered cells. In the case of a configuration requiring a smaller width to represent, its tape cells are padded with the blank symbol $\\sqcup$.\nWe say that a computation tableau is \\textit{accepting} if it contains at least one accepting configuration row. More formally, a tableau if it satisfies the condition $\\exists i X_{i,2,q_{accept}}$.\nFor convenience, we list new notations used in this proof along with their intended meaning in the following table:\n\\begin{table}[h!]\n\\centering\n\\bgroup\n \\begin{tabular}{||c | c||}\n \\hline\n \\textbf{Symbol} & \\textbf{Interpretation} \\\\ [0.5ex]\n \\hline\\hline\n $cell[i, j]$ & Symbol written on the cell of row $i$ and column $j$ \\\\ \\hline\n $X_{i,j,s}$ & $cell[i, j] = s$ \\\\ \\hline\n $\\mathcal{C}$ & $Q \\cup \\Gamma \\cup \\{\\#\\}$ \\\\ \\hline\n $W_{i,j}$ & The $2\\times 3$ window whose  top-left corner is $cell[i,j]$ \\\\ \\hline\n $\\mathcal{L}(W_{i,j})$ & Window $W_{i,j}$ is legal \\\\ [0.5ex] \\hline\n \\end{tabular}\n \\egroup\n \\caption{Used Symbols Interpretations}\n\\end{table}\nNext, we start defining a Boolean formula $\\Psi$ whose satisfiability corresponds to the existence of at least one \\emph{accepting} tableau for $N_L$ on input $I$. Remember that a tableau is \\textbf{one} computational branch of potentially many other branches on the computational tree of $N_L$. Thus, we only need to worry about the existence of an accepting branch. For convenience and generality, $\\Psi$ will be expressed in terms of variables in the form $X_{i,j,s}$.\nTo achieve that, we need to map out the exact constraints that ensure that a tableau is valid, that is, it corresponds to valid computational branch. In this proof, we group those constraints into four sub-formulas whose conjunction constitutes $\\Psi$. They are as follows:\n\\begin{table}[h!]\n\\centering\n\\bgroup\n \\begin{tabular}{||c | c||}\n \\hline\n \\textbf{Sub-formula} & \\textbf{Constraint} \\\\ [0.5ex]\n \\hline\\hline\n $\\Psi_{cell}$ & Every cell has exactly one defined symbol on it \\\\ \\hline\n $\\Psi_{initial}$ & The first row contains a valid initial configuration \\\\ \\hline\n $\\Psi_{accept}$ & The tableau contains an accepting configuration \\\\ \\hline\n $\\Psi_{move}$ & Each configuration row is a legal transition from the previous one \\\\ [0.5ex] \\hline\n \\end{tabular}\n\\egroup\n \\caption{$\\Psi$ Sub-formulas and Their Interpretation}\n\\end{table}\nOn that basis, if we manage to capture all four constraints using Boolean variables, then we guarantee a correspondence between a valid computation of $N_L$ and a variable assignment on $\\Psi$:\n{\\centering\n$\\Psi = \\Psi_{cell}\\land\\Psi_{initial}\\land\\Psi_{accept}\\land\\Psi_{move}$ \\\\\n}\nThe first three sub-formulas are relatively straightforwards while $\\Psi_{move}$ might seem relatively more involved. To begin with, we can define $\\Psi_{cell}$ as:\n\\[ \\Psi_{cell} = \\bigwedge\\limits_{1 \\leq i, j \\leq p(n)} \\left[\\left(\\bigvee\\limits_{s \\in \\mathcal{C}} X_{i,j,s}\\right) \\land \\left(\\bigwedge\\limits_{\\substack{s, r \\in \\mathcal{C} \\\\ s \\neq r}} \\neg\\left(X_{i,j,s} \\land X_{i,j,r}\\right)\\right)\\right] \\]",
        "\\[ \\Psi_{initial} = X_{1,1,\\#}\\land X_{1,2,q_0}\\land \\left(\\bigwedge\\limits_{\\substack{3 \\leq j \\leq n + 2 \\\\ s \\in I}} X_{1,j,s}\\right) \\land \\left(\\bigwedge\\limits_{n+3 \\leq j \\leq p(n)-1} X_{1,j,\\sqcup}\\right) \\land X_{1,p(n),\\#} \\]",
        "\\[ \\Psi_{accept} = \\bigvee\\limits_{1 \\leq i,j \\leq p(n)} X_{i,j, q_{accept}} \\]",
        "\\[ \\Psi_{move} = \\bigwedge\\limits_{\\substack{1 \\leq i \\leq n - 2 \\\\ 1 \\leq j \\leq n - 3}} \\mathcal{L}(W_{i,j}) \\]",
        "\\[E(G) = \\{ \\{x_{i_1,j_1,h_1}, x_{i_2,j_2,h_2}\\}\\;|\\: (j_1 \\neq j_2)\\land(i_1=i_2 \\rightarrow h_1=h_2) \\}\\]",
        "\\[ f(x_i) = \\begin{cases}\n      \\text{\\textit{true}}, & (x_{i,1},...,x_{i,2k})\\in \\mathcal{H} \\\\\n      \\text{\\textit{false}}, & (x_{i,2k},...,x_{i,1})\\in \\mathcal{H}\n   \\end{cases}\n\\]"
    ]
}